
<!DOCTYPE html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
      
        <link rel="canonical" href="https://dmesser.github.io/cns-lab-sa-summit/module-4-cluster-ops/">
      
      
      
        <link rel="shortcut icon" href="../img/favicon.ico">
      
      <meta name="generator" content="mkdocs-0.16.3, mkdocs-material-1.7.3">
    
    
      
        <title>Module 4 - Cluster Operations - Container-Native Storage Hands-on Lab - Storage SA Summit 2017</title>
      
    
    
      <script src="../assets/javascripts/modernizr-1df76c4e58.js"></script>
    
    
      <link rel="stylesheet" href="../assets/stylesheets/application-acc56dad49.css">
      
        <link rel="stylesheet" href="../assets/stylesheets/application-8817cfa535.palette.css">
      
    
    
      
        
        
        
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,400,400i,700|Roboto+Mono">
        <style>body,input{font-family:"Roboto","Helvetica Neue",Helvetica,Arial,sans-serif}code,kbd,pre{font-family:"Roboto Mono","Courier New",Courier,monospace}</style>
      
      <link rel="stylesheet" href="https://fonts.googleapis.com/icon?family=Material+Icons">
    
    
    
  </head>
  
  
  
  
    <body data-md-color-primary="red" data-md-color-accent="light-blue">
  
    <svg class="md-svg">
      <defs>
        
        
      </defs>
    </svg>
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="drawer">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="search">
    <label class="md-overlay" data-md-component="overlay" for="drawer"></label>
    
      <header class="md-header" data-md-component="header">
  <nav class="md-header-nav md-grid">
    <div class="md-flex">
      <div class="md-flex__cell md-flex__cell--shrink">
        
          <a href="https://dmesser.github.io/cns-lab-sa-summit/" title="Container-Native Storage Hands-on Lab - Storage SA Summit 2017" class="md-logo md-header-nav__button">
            <img src="../img/shadowman_rgb.png" width="24" height="24">
          </a>
        
      </div>
      <div class="md-flex__cell md-flex__cell--shrink">
        <label class="md-icon md-icon--menu md-header-nav__button" for="drawer"></label>
      </div>
      <div class="md-flex__cell md-flex__cell--stretch">
        <span class="md-flex__ellipsis md-header-nav__title">
          
            
              
                <span class="md-header-nav__parent">
                  Modules
                </span>
              
            
            Module 4 - Cluster Operations
          
        </span>
      </div>
      <div class="md-flex__cell md-flex__cell--shrink">
        
          <label class="md-icon md-icon--search md-header-nav__button" for="search"></label>
          
<div class="md-search" data-md-component="search">
  <label class="md-search__overlay" for="search"></label>
  <div class="md-search__inner">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" required placeholder="Search" accesskey="s" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="query">
      <label class="md-icon md-search__icon" for="search"></label>
      <button type="reset" class="md-icon md-search__icon" data-md-component="reset">close</button>
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" data-md-scrollfix>
        <div class="md-search-result" data-md-component="result" data-md-lang-search="">
          <div class="md-search-result__meta" data-md-lang-result-none="No matching documents" data-md-lang-result-one="1 matching document" data-md-lang-result-other="# matching documents">
            Type to start searching
          </div>
          <ol class="md-search-result__list"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
        
      </div>
      
    </div>
  </nav>
</header>
    
    <div class="md-container">
      
      
      <main class="md-main">
        <div class="md-main__inner md-grid" data-md-component="container">
          
            
              <div class="md-sidebar md-sidebar--primary" data-md-component="navigation">
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    <nav class="md-nav md-nav--primary" data-md-level="0">
  <label class="md-nav__title md-nav__title--site" for="drawer">
    
      <i class="md-logo md-nav__button">
        <img src="../img/shadowman_rgb.png">
      </i>
    
    Container-Native Storage Hands-on Lab - Storage SA Summit 2017
  </label>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
      


  <li class="md-nav__item">
    <a href=".." title="Overview" class="md-nav__link">
      Overview
    </a>
  </li>

    
      
      
      

  


  <li class="md-nav__item md-nav__item--active md-nav__item--nested">
    
      <input class="md-toggle md-nav__toggle" data-md-toggle="nav-2" type="checkbox" id="nav-2" checked>
    
    <label class="md-nav__link" for="nav-2">
      Modules
    </label>
    <nav class="md-nav" data-md-component="collapsible" data-md-level="1">
      <label class="md-nav__title" for="nav-2">
        Modules
      </label>
      <ul class="md-nav__list" data-md-scrollfix>
        
        
          
          
          


  <li class="md-nav__item">
    <a href="../module-1-ocp-in-5/" title="Module 1 - OpenShift in 5 Minutes" class="md-nav__link">
      Module 1 - OpenShift in 5 Minutes
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../module-2-deploy-cns/" title="Module 2 - Deploying Container-Native Storage" class="md-nav__link">
      Module 2 - Deploying Container-Native Storage
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../module-3-using-cns/" title="Module 3 - Using Persistent Storage" class="md-nav__link">
      Module 3 - Using Persistent Storage
    </a>
  </li>

        
          
          
          

  


  <li class="md-nav__item md-nav__item--active">
    
    <input class="md-toggle md-nav__toggle" data-md-toggle="toc" type="checkbox" id="toc">
    
    
      <label class="md-nav__link md-nav__link--active" for="toc">
        Module 4 - Cluster Operations
      </label>
    
    <a href="./" title="Module 4 - Cluster Operations" class="md-nav__link md-nav__link--active">
      Module 4 - Cluster Operations
    </a>
    
      
<nav class="md-nav md-nav--secondary">
  
  
  
    <label class="md-nav__title" for="toc">Table of contents</label>
    <ul class="md-nav__list" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#running-multiple-glusterfs-pools" title="Running multiple GlusterFS pools" class="md-nav__link">
    Running multiple GlusterFS pools
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#deleting-a-glusterfs-pool" title="Deleting a GlusterFS pool" class="md-nav__link">
    Deleting a GlusterFS pool
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#expanding-a-glusterfs-pool" title="Expanding a GlusterFS pool" class="md-nav__link">
    Expanding a GlusterFS pool
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#adding-a-device-to-a-node" title="Adding a device to a node" class="md-nav__link">
    Adding a device to a node
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#replacing-a-failed-device" title="Replacing a failed device" class="md-nav__link">
    Replacing a failed device
  </a>
  
</li>
      
      
      
    </ul>
  
</nav>
    
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../module-5-advanced/" title="Module 5 - Advanced Topics" class="md-nav__link">
      Module 5 - Advanced Topics
    </a>
  </li>

        
      </ul>
    </nav>
  </li>

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              <div class="md-sidebar md-sidebar--secondary" data-md-component="toc">
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    
<nav class="md-nav md-nav--secondary">
  
  
  
    <label class="md-nav__title" for="toc">Table of contents</label>
    <ul class="md-nav__list" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#running-multiple-glusterfs-pools" title="Running multiple GlusterFS pools" class="md-nav__link">
    Running multiple GlusterFS pools
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#deleting-a-glusterfs-pool" title="Deleting a GlusterFS pool" class="md-nav__link">
    Deleting a GlusterFS pool
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#expanding-a-glusterfs-pool" title="Expanding a GlusterFS pool" class="md-nav__link">
    Expanding a GlusterFS pool
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#adding-a-device-to-a-node" title="Adding a device to a node" class="md-nav__link">
    Adding a device to a node
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#replacing-a-failed-device" title="Replacing a failed device" class="md-nav__link">
    Replacing a failed device
  </a>
  
</li>
      
      
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          <div class="md-content">
            <article class="md-content__inner md-typeset">
              
                
                
                  <h1>Module 4 - Cluster Operations</h1>
                
                <div class="admonition summary">
<p class="admonition-title">Overview</p>
<p>In this module you be introduced to some standard operational procedures. You will learn how to run multiple GlusterFS <em>Trusted Storage Pools</em> on OpenShift and how to expand and maintain deployments.</p>
<p>Herein, we will use the term pool (GlusterFS terminology) and cluster (heketi terminology) interchangeably.</p>
</div>
<h2 id="running-multiple-glusterfs-pools">Running multiple GlusterFS pools<a class="headerlink" href="#running-multiple-glusterfs-pools" title="Permanent link">#</a></h2>
<p>In the previous modules a single GlusterFS clusters was used to supply <em>PersistentVolumes</em> to applications. CNS allows for multiple clusters to run in a single OpenShift deployment.</p>
<p>There are several use cases for this:</p>
<ol>
<li>
<p>Provide data isolation between clusters of different tenants</p>
</li>
<li>
<p>Provide multiple performance tiers of CNS, i.e. HDD-based vs. SSD-based</p>
</li>
<li>
<p>Overcome the maximum amount of 300 PVs per cluster, currently imposed in version 3.5</p>
</li>
</ol>
<p>To deploy a second GlusterFS pool follow these steps:</p>
<p>&#8680; Log in as <code>operator</code> to namespace <code>container-native-storage</code></p>
<div class="codehilite"><pre><span></span>oc login -u operator -n container-native-storage
</pre></div>


<p>Your deployment has 6 OpenShift Application Nodes in total, <code>node-1</code>, <code>node-2</code> and <code>node-3</code> currently setup running CNS. We will now set up a second cluster using <code>node-4</code>, <code>node-5</code> and <code>node-6</code>.</p>
<p>&#8680; Apply additional labels (user-defined key-value pairs) to the remaining 3 OpenShift Nodes:</p>
<div class="codehilite"><pre><span></span>oc label node/node-4.lab storagenode=glusterfs
oc label node/node-5.lab storagenode=glusterfs
oc label node/node-6.lab storagenode=glusterfs
</pre></div>


<p>The label will be used to control GlusterFS pod placement and availability.</p>
<p>&#8680; Wait for all pods to show <code>1/1</code> in the <code>READY</code> column:</p>
<div class="codehilite"><pre><span></span> oc get pods -o wide -n container-native-storage
</pre></div>


<div class="admonition note">
<p class="admonition-title">Note</p>
<p>It may take up to 3 minutes for the GlusterFS pods to transition into <code>READY</code> state.</p>
</div>
<p>For bulk import of new nodes into a new cluster, the topology JSON file can be updated to include a second cluster with a separate set of nodes.</p>
<p>&#8680; Create a new file named updated-topology.json with the content below:</p>
<p><kbd>updated-topology.json:</kbd></p>
<div class="codehilite"><pre><span></span><span class="p">{</span>
    <span class="nt">&quot;clusters&quot;</span><span class="p">:</span> <span class="p">[</span>
        <span class="p">{</span>
            <span class="nt">&quot;nodes&quot;</span><span class="p">:</span> <span class="p">[</span>
                <span class="p">{</span>
                    <span class="nt">&quot;node&quot;</span><span class="p">:</span> <span class="p">{</span>
                        <span class="nt">&quot;hostnames&quot;</span><span class="p">:</span> <span class="p">{</span>
                            <span class="nt">&quot;manage&quot;</span><span class="p">:</span> <span class="p">[</span>
                                <span class="s2">&quot;node-1.lab&quot;</span>
                            <span class="p">],</span>
                            <span class="nt">&quot;storage&quot;</span><span class="p">:</span> <span class="p">[</span>
                                <span class="s2">&quot;10.0.2.101&quot;</span>
                            <span class="p">]</span>
                        <span class="p">},</span>
                        <span class="nt">&quot;zone&quot;</span><span class="p">:</span> <span class="mi">1</span>
                    <span class="p">},</span>
                    <span class="nt">&quot;devices&quot;</span><span class="p">:</span> <span class="p">[</span>
                        <span class="s2">&quot;/dev/xvdc&quot;</span>
                    <span class="p">]</span>
                <span class="p">},</span>
                <span class="p">{</span>
                    <span class="nt">&quot;node&quot;</span><span class="p">:</span> <span class="p">{</span>
                        <span class="nt">&quot;hostnames&quot;</span><span class="p">:</span> <span class="p">{</span>
                            <span class="nt">&quot;manage&quot;</span><span class="p">:</span> <span class="p">[</span>
                                <span class="s2">&quot;node-2.lab&quot;</span>
                            <span class="p">],</span>
                            <span class="nt">&quot;storage&quot;</span><span class="p">:</span> <span class="p">[</span>
                                <span class="s2">&quot;10.0.3.102&quot;</span>
                            <span class="p">]</span>
                        <span class="p">},</span>
                        <span class="nt">&quot;zone&quot;</span><span class="p">:</span> <span class="mi">2</span>
                    <span class="p">},</span>
                    <span class="nt">&quot;devices&quot;</span><span class="p">:</span> <span class="p">[</span>
                        <span class="s2">&quot;/dev/xvdc&quot;</span>
                    <span class="p">]</span>
                <span class="p">},</span>
                <span class="p">{</span>
                    <span class="nt">&quot;node&quot;</span><span class="p">:</span> <span class="p">{</span>
                        <span class="nt">&quot;hostnames&quot;</span><span class="p">:</span> <span class="p">{</span>
                            <span class="nt">&quot;manage&quot;</span><span class="p">:</span> <span class="p">[</span>
                                <span class="s2">&quot;node-3.lab&quot;</span>
                            <span class="p">],</span>
                            <span class="nt">&quot;storage&quot;</span><span class="p">:</span> <span class="p">[</span>
                                <span class="s2">&quot;10.0.4.103&quot;</span>
                            <span class="p">]</span>
                        <span class="p">},</span>
                        <span class="nt">&quot;zone&quot;</span><span class="p">:</span> <span class="mi">3</span>
                    <span class="p">},</span>
                    <span class="nt">&quot;devices&quot;</span><span class="p">:</span> <span class="p">[</span>
                        <span class="s2">&quot;/dev/xvdc&quot;</span>
                    <span class="p">]</span>
                <span class="p">}</span>
            <span class="p">]</span>
<span class="hll">        <span class="p">},</span>
</span>        <span class="p">{</span>
            <span class="nt">&quot;nodes&quot;</span><span class="p">:</span> <span class="p">[</span>
                <span class="p">{</span>
                    <span class="nt">&quot;node&quot;</span><span class="p">:</span> <span class="p">{</span>
                        <span class="nt">&quot;hostnames&quot;</span><span class="p">:</span> <span class="p">{</span>
                            <span class="nt">&quot;manage&quot;</span><span class="p">:</span> <span class="p">[</span>
                                <span class="s2">&quot;node-4.lab&quot;</span>
                            <span class="p">],</span>
                            <span class="nt">&quot;storage&quot;</span><span class="p">:</span> <span class="p">[</span>
                                <span class="s2">&quot;10.0.2.104&quot;</span>
                            <span class="p">]</span>
                        <span class="p">},</span>
                        <span class="nt">&quot;zone&quot;</span><span class="p">:</span> <span class="mi">1</span>
                    <span class="p">},</span>
                    <span class="nt">&quot;devices&quot;</span><span class="p">:</span> <span class="p">[</span>
                        <span class="s2">&quot;/dev/xvdc&quot;</span>
                    <span class="p">]</span>
                <span class="p">},</span>
                <span class="p">{</span>
                    <span class="nt">&quot;node&quot;</span><span class="p">:</span> <span class="p">{</span>
                        <span class="nt">&quot;hostnames&quot;</span><span class="p">:</span> <span class="p">{</span>
                            <span class="nt">&quot;manage&quot;</span><span class="p">:</span> <span class="p">[</span>
                                <span class="s2">&quot;node-5.lab&quot;</span>
                            <span class="p">],</span>
                            <span class="nt">&quot;storage&quot;</span><span class="p">:</span> <span class="p">[</span>
                                <span class="s2">&quot;10.0.3.105&quot;</span>
                            <span class="p">]</span>
                        <span class="p">},</span>
                        <span class="nt">&quot;zone&quot;</span><span class="p">:</span> <span class="mi">2</span>
                    <span class="p">},</span>
                    <span class="nt">&quot;devices&quot;</span><span class="p">:</span> <span class="p">[</span>
                        <span class="s2">&quot;/dev/xvdc&quot;</span>
                    <span class="p">]</span>
                <span class="p">},</span>
                <span class="p">{</span>
                    <span class="nt">&quot;node&quot;</span><span class="p">:</span> <span class="p">{</span>
                        <span class="nt">&quot;hostnames&quot;</span><span class="p">:</span> <span class="p">{</span>
                            <span class="nt">&quot;manage&quot;</span><span class="p">:</span> <span class="p">[</span>
                                <span class="s2">&quot;node-6.lab&quot;</span>
                            <span class="p">],</span>
                            <span class="nt">&quot;storage&quot;</span><span class="p">:</span> <span class="p">[</span>
                                <span class="s2">&quot;10.0.4.106&quot;</span>
                            <span class="p">]</span>
                        <span class="p">},</span>
                        <span class="nt">&quot;zone&quot;</span><span class="p">:</span> <span class="mi">3</span>
                    <span class="p">},</span>
                    <span class="nt">&quot;devices&quot;</span><span class="p">:</span> <span class="p">[</span>
                        <span class="s2">&quot;/dev/xvdc&quot;</span>
                    <span class="p">]</span>
                <span class="p">}</span>
            <span class="p">]</span>
        <span class="p">}</span>
    <span class="p">]</span>
<span class="p">}</span>
</pre></div>


<p>The file contains the same content as <code>topology.json</code> with a second cluster specification (beginning at the highlighted line).</p>
<p>When loading this topology to heketi, it will recognize the existing cluster (leaving it unchanged) and start creating the new one, with the same bootstrapping process as seen with <code>cns-deploy</code>.</p>
<p>&#8680; Prepare the heketi CLI tool like previously in <a href="../module-2-deploy-cns/#heketi-env-setup">Module 2</a>.</p>
<div class="codehilite"><pre><span></span>export HEKETI_CLI_SERVER=http://heketi-container-native-storage.cloudapps.34.252.58.209.nip.io
export HEKETI_CLI_USER=admin
export HEKETI_CLI_KEY=myS3cr3tpassw0rd
</pre></div>


<p>&#8680; Verify there is currently only a single cluster known to heketi</p>
<div class="codehilite"><pre><span></span>heketi-cli cluster list
</pre></div>


<p>Example output:</p>
<div class="codehilite"><pre><span></span>Clusters:
fb67f97166c58f161b85201e1fd9b8ed
</pre></div>


<p><strong>Note this cluster&rsquo;s id</strong> for later reference.</p>
<p>&#8680; Load the new topology with the heketi client</p>
<div class="codehilite"><pre><span></span>heketi-cli topology load --json=updated-topology.json
</pre></div>


<p>You should see output similar to the following:</p>
<div class="codehilite"><pre><span></span>Found node node-1.lab on cluster fb67f97166c58f161b85201e1fd9b8ed
Found device /dev/xvdc
Found node node-2.lab on cluster fb67f97166c58f161b85201e1fd9b8ed
Found device /dev/xvdc
Found node node-3.lab on cluster fb67f97166c58f161b85201e1fd9b8ed
Found device /dev/xvdc
Creating cluster ... ID: 46b205a4298c625c4bca2206b7a82dd3
Creating node node-4.lab ... ID: 604d2eb15a5ca510ff3fc5ecf912d3c0
Adding device /dev/xvdc ... OK
Creating node node-5.lab ... ID: 538b860406870288af23af0fbc2cd27f
Adding device /dev/xvdc ... OK
Creating node node-6.lab ... ID: 7736bd0cb6a84540860303a6479cacb2
Adding device /dev/xvdc ... OK
</pre></div>


<p>As indicated from above output a new cluster got created.</p>
<p>&#8680; List all clusters:</p>
<div class="codehilite"><pre><span></span>heketi-cli cluster list
</pre></div>


<p>You should see a second cluster in the list:</p>
<div class="codehilite"><pre><span></span>Clusters:
46b205a4298c625c4bca2206b7a82dd3
fb67f97166c58f161b85201e1fd9b8ed
</pre></div>


<p>The second cluster with the ID <code>46b205a4298c625c4bca2206b7a82dd3</code> is an entirely independent GlusterFS deployment. <strong>Note this second cluster&rsquo;s ID</strong> as well for later reference.</p>
<p>Now we have two independent GlusterFS clusters managed by the same heketi instance:</p>
<table>
<thead>
<tr>
<th></th>
<th>Nodes</th>
<th>Cluster UUID</th>
</tr>
</thead>
<tbody>
<tr>
<td>First Cluster</td>
<td>node-1, node-2, node-3</td>
<td>fb67f97166c58f161b85201e1fd9b8ed</td>
</tr>
<tr>
<td>Second Cluster</td>
<td>node-4, node-5, node-6</td>
<td>46b205a4298c625c4bca2206b7a82dd3</td>
</tr>
</tbody>
</table>
<p>&#8680; Query the updated topology:</p>
<div class="codehilite"><pre><span></span>heketi-cli topology info
</pre></div>


<p>Abbreviated output:</p>
<div class="codehilite"><pre><span></span>Cluster Id: 46b205a4298c625c4bca2206b7a82dd3

    Volumes:

    Nodes:

      Node Id: 538b860406870288af23af0fbc2cd27f
      State: online
      Cluster Id: 46b205a4298c625c4bca2206b7a82dd3
      Zone: 2
      Management Hostname: node-5.lab
      Storage Hostname: 10.0.3.105
      Devices:
        Id:e481d022cea9bfb11e8a86c0dd8d3499   Name:/dev/xvdc           State:online    Size (GiB):499     Used (GiB):0       Free (GiB):499
            Bricks:

      Node Id: 604d2eb15a5ca510ff3fc5ecf912d3c0
      State: online
      Cluster Id: 46b205a4298c625c4bca2206b7a82dd3
      Zone: 1
      Management Hostname: node-4.lab
      Storage Hostname: 10.0.2.104
      Devices:
        Id:09a25a114c53d7669235b368efd2f8d1   Name:/dev/xvdc           State:online    Size (GiB):499     Used (GiB):0       Free (GiB):499
            Bricks:

      Node Id: 7736bd0cb6a84540860303a6479cacb2
      State: online
      Cluster Id: 46b205a4298c625c4bca2206b7a82dd3
      Zone: 3
      Management Hostname: node-6.lab
      Storage Hostname: 10.0.4.106
      Devices:
        Id:cccadb2b54dccd99f698d2ae137a22ff   Name:/dev/xvdc           State:online    Size (GiB):499     Used (GiB):0       Free (GiB):499
            Bricks:

Cluster Id: fb67f97166c58f161b85201e1fd9b8ed

[...output omitted for brevity...]
</pre></div>


<p>heketi formed an new, independent 3-node GlusterFS cluster on those nodes.</p>
<p>&#8680; Check running GlusterFS pods</p>
<div class="codehilite"><pre><span></span>oc get pods -o wide
</pre></div>


<p>From the output you can spot the pod names running on the new cluster&rsquo;s nodes:</p>
<div class="codehilite"><pre><span></span>NAME              READY     STATUS    RESTARTS   AGE       IP           NODE
<span class="hll">glusterfs-1nvtj   1/1       Running   0          23m       10.0.4.106   node-6.lab
</span><span class="hll">glusterfs-5gvw8   1/1       Running   0          24m       10.0.2.104   node-4.lab
</span>glusterfs-5rc2g   1/1       Running   0          4h        10.0.2.101   node-1.lab
<span class="hll">glusterfs-b4wg1   1/1       Running   0          24m       10.0.3.105   node-5.lab
</span>glusterfs-jbvdk   1/1       Running   0          4h        10.0.3.102   node-2.lab
glusterfs-rchtr   1/1       Running   0          4h        10.0.4.103   node-3.lab
heketi-1-tn0s9    1/1       Running   0          4h        10.130.2.3   node-6.lab
</pre></div>


<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Again note that the pod names are dynamically generated and will be different. Use the FQDN of your hosts to determine one of new cluster&rsquo;s pods.</p>
</div>
<p>&#8680; Log on to one of the new cluster&rsquo;s pods:</p>
<div class="codehilite"><pre><span></span>oc rsh glusterfs-1nvtj
</pre></div>


<p>&#8680; Query this GlusterFS node for it&rsquo;s peers:</p>
<div class="codehilite"><pre><span></span>gluster peer status
</pre></div>


<p>As expected this node only has 2 peers, evidence that it&rsquo;s running in it&rsquo;s own GlusterFS pool separate from the first cluster in deployed in Module 2.</p>
<div class="codehilite"><pre><span></span>Number of Peers: 2

Hostname: node-5.lab
Uuid: 0db9b5d0-7fa8-4d2f-8b9e-6664faf34606
State: Peer in Cluster (Connected)
Other names:
10.0.3.105

Hostname: node-4.lab
Uuid: 695b661d-2a55-4f94-b22e-40a9db79c01a
State: Peer in Cluster (Connected)
</pre></div>


<p>Before you can use the second cluster two tasks have to be accomplished:</p>
<ol>
<li>
<p>The <em>StorageClass</em> for the first cluster has to be updated to point the first cluster&rsquo;s UUID,</p>
</li>
<li>
<p>A second <em>StorageClass</em> for the second cluster has to be created, pointing to the same heketi</p>
</li>
</ol>
<div class="admonition tip">
<p class="admonition-title">Why do we need to update the first <em>StorageClass</em>?</p>
<p>By default, when no cluster UUID is specified, heketi serves volume creation requests from any cluster currently registered to it.<br />
In order to request a volume from specific cluster you have to supply the cluster&rsquo;s UUID to heketi. The first <em>StorageClass</em> has no UUID specified so far.</p>
</div>
<p>&#8680; Edit the existing <em>StorageClass</em> definition by updating the <code>cns-storageclass.yml</code> file from <a href="../module-3-using-cns/#storageclass-setup">Module 3</a> like below:</p>
<p><kbd>cns-storageclass.yml:</kbd></p>
<div class="codehilite"><pre><span></span><span class="l l-Scalar l-Scalar-Plain">apiVersion</span><span class="p p-Indicator">:</span> <span class="l l-Scalar l-Scalar-Plain">storage.k8s.io/v1beta1</span>
<span class="l l-Scalar l-Scalar-Plain">kind</span><span class="p p-Indicator">:</span> <span class="l l-Scalar l-Scalar-Plain">StorageClass</span>
<span class="l l-Scalar l-Scalar-Plain">metadata</span><span class="p p-Indicator">:</span>
  <span class="l l-Scalar l-Scalar-Plain">name</span><span class="p p-Indicator">:</span> <span class="l l-Scalar l-Scalar-Plain">container-native-storage</span>
  <span class="l l-Scalar l-Scalar-Plain">annotations</span><span class="p p-Indicator">:</span>
    <span class="l l-Scalar l-Scalar-Plain">storageclass.beta.kubernetes.io/is-default-class</span><span class="p p-Indicator">:</span> <span class="s">&quot;true&quot;</span>
<span class="l l-Scalar l-Scalar-Plain">provisioner</span><span class="p p-Indicator">:</span> <span class="l l-Scalar l-Scalar-Plain">kubernetes.io/glusterfs</span>
<span class="l l-Scalar l-Scalar-Plain">parameters</span><span class="p p-Indicator">:</span>
  <span class="l l-Scalar l-Scalar-Plain">resturl</span><span class="p p-Indicator">:</span> <span class="s">&quot;http://heketi-container-native-storage.cloudapps.34.252.58.209.nip.io&quot;</span>
  <span class="l l-Scalar l-Scalar-Plain">restauthenabled</span><span class="p p-Indicator">:</span> <span class="s">&quot;true&quot;</span>
  <span class="l l-Scalar l-Scalar-Plain">restuser</span><span class="p p-Indicator">:</span> <span class="s">&quot;admin&quot;</span>
  <span class="l l-Scalar l-Scalar-Plain">volumetype</span><span class="p p-Indicator">:</span> <span class="s">&quot;replicate:3&quot;</span>
  <span class="l l-Scalar l-Scalar-Plain">secretNamespace</span><span class="p p-Indicator">:</span> <span class="s">&quot;default&quot;</span>
  <span class="l l-Scalar l-Scalar-Plain">secretName</span><span class="p p-Indicator">:</span> <span class="s">&quot;cns-secret&quot;</span>
<span class="hll">  <span class="l l-Scalar l-Scalar-Plain">clusterid</span><span class="p p-Indicator">:</span> <span class="s">&quot;fb67f97166c58f161b85201e1fd9b8ed&quot;</span>
</span></pre></div>


<p>Note the additional <code>clusterid</code> parameter highlighted. It&rsquo;s the cluster&rsquo;s UUID as known by heketi. Don&rsquo;t change anything else.</p>
<p>&#8680; Delete the existing <em>StorageClass</em> definition in OpenShift</p>
<div class="codehilite"><pre><span></span>oc delete storageclass/container-native-storage
</pre></div>


<p>&#8680; Add the <em>StorageClass</em> again:</p>
<div class="codehilite"><pre><span></span>oc create -f cns-storageclass.yml
</pre></div>


<p>&#8680; Create a new file called cns-storageclass-slow.yml with the following contents:</p>
<p><kbd>cns-storageclass-slow.yml:</kbd></p>
<div class="codehilite"><pre><span></span><span class="l l-Scalar l-Scalar-Plain">apiVersion</span><span class="p p-Indicator">:</span> <span class="l l-Scalar l-Scalar-Plain">storage.k8s.io/v1beta1</span>
<span class="l l-Scalar l-Scalar-Plain">kind</span><span class="p p-Indicator">:</span> <span class="l l-Scalar l-Scalar-Plain">StorageClass</span>
<span class="l l-Scalar l-Scalar-Plain">metadata</span><span class="p p-Indicator">:</span>
  <span class="l l-Scalar l-Scalar-Plain">name</span><span class="p p-Indicator">:</span> <span class="l l-Scalar l-Scalar-Plain">container-native-storage-slow</span>
<span class="l l-Scalar l-Scalar-Plain">provisioner</span><span class="p p-Indicator">:</span> <span class="l l-Scalar l-Scalar-Plain">kubernetes.io/glusterfs</span>
<span class="l l-Scalar l-Scalar-Plain">parameters</span><span class="p p-Indicator">:</span>
  <span class="l l-Scalar l-Scalar-Plain">resturl</span><span class="p p-Indicator">:</span> <span class="s">&quot;http://heketi-container-native-storage.cloudapps.34.252.58.209.nip.io&quot;</span>
  <span class="l l-Scalar l-Scalar-Plain">restauthenabled</span><span class="p p-Indicator">:</span> <span class="s">&quot;true&quot;</span>
  <span class="l l-Scalar l-Scalar-Plain">restuser</span><span class="p p-Indicator">:</span> <span class="s">&quot;admin&quot;</span>
  <span class="l l-Scalar l-Scalar-Plain">volumetype</span><span class="p p-Indicator">:</span> <span class="s">&quot;replicate:3&quot;</span>
  <span class="l l-Scalar l-Scalar-Plain">secretNamespace</span><span class="p p-Indicator">:</span> <span class="s">&quot;default&quot;</span>
  <span class="l l-Scalar l-Scalar-Plain">secretName</span><span class="p p-Indicator">:</span> <span class="s">&quot;cns-secret&quot;</span>
<span class="hll">  <span class="l l-Scalar l-Scalar-Plain">clusterid</span><span class="p p-Indicator">:</span> <span class="s">&quot;46b205a4298c625c4bca2206b7a82dd3&quot;</span>
</span></pre></div>


<p>Again note the <code>clusterid</code> in the <code>parameters</code> section referencing the second cluster&rsquo;s UUID.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Again, keep in mind that also the <code>resturl</code> parameter is specific to your environment as it contains the route to heketi using the public URL.</p>
</div>
<p>&#8680; Add the new <em>StorageClass</em>:</p>
<div class="codehilite"><pre><span></span>oc create -f cns-storageclass-slow.yml
</pre></div>


<p>Let&rsquo;s verify both <em>StorageClasses</em> are working as expected:</p>
<p>&#8680; Create the following two files containing PVCs issued against either of both GlusterFS pools via their respective <em>StorageClass</em>:</p>
<p><kbd>cns-pvc-fast.yml:</kbd></p>
<div class="codehilite"><pre><span></span><span class="l l-Scalar l-Scalar-Plain">kind</span><span class="p p-Indicator">:</span> <span class="l l-Scalar l-Scalar-Plain">PersistentVolumeClaim</span>
<span class="l l-Scalar l-Scalar-Plain">apiVersion</span><span class="p p-Indicator">:</span> <span class="l l-Scalar l-Scalar-Plain">v1</span>
<span class="l l-Scalar l-Scalar-Plain">metadata</span><span class="p p-Indicator">:</span>
  <span class="l l-Scalar l-Scalar-Plain">name</span><span class="p p-Indicator">:</span> <span class="l l-Scalar l-Scalar-Plain">my-container-storage-fast</span>
  <span class="l l-Scalar l-Scalar-Plain">annotations</span><span class="p p-Indicator">:</span>
    <span class="l l-Scalar l-Scalar-Plain">volume.beta.kubernetes.io/storage-class</span><span class="p p-Indicator">:</span> <span class="l l-Scalar l-Scalar-Plain">container-native-storage</span>
<span class="l l-Scalar l-Scalar-Plain">spec</span><span class="p p-Indicator">:</span>
  <span class="l l-Scalar l-Scalar-Plain">accessModes</span><span class="p p-Indicator">:</span>
  <span class="p p-Indicator">-</span> <span class="l l-Scalar l-Scalar-Plain">ReadWriteOnce</span>
  <span class="l l-Scalar l-Scalar-Plain">resources</span><span class="p p-Indicator">:</span>
    <span class="l l-Scalar l-Scalar-Plain">requests</span><span class="p p-Indicator">:</span>
      <span class="l l-Scalar l-Scalar-Plain">storage</span><span class="p p-Indicator">:</span> <span class="l l-Scalar l-Scalar-Plain">5Gi</span>
</pre></div>


<p><kbd>cns-pvc-slow.yml:</kbd></p>
<div class="codehilite"><pre><span></span><span class="l l-Scalar l-Scalar-Plain">kind</span><span class="p p-Indicator">:</span> <span class="l l-Scalar l-Scalar-Plain">PersistentVolumeClaim</span>
<span class="l l-Scalar l-Scalar-Plain">apiVersion</span><span class="p p-Indicator">:</span> <span class="l l-Scalar l-Scalar-Plain">v1</span>
<span class="l l-Scalar l-Scalar-Plain">metadata</span><span class="p p-Indicator">:</span>
  <span class="l l-Scalar l-Scalar-Plain">name</span><span class="p p-Indicator">:</span> <span class="l l-Scalar l-Scalar-Plain">my-container-storage-slow</span>
  <span class="l l-Scalar l-Scalar-Plain">annotations</span><span class="p p-Indicator">:</span>
    <span class="l l-Scalar l-Scalar-Plain">volume.beta.kubernetes.io/storage-class</span><span class="p p-Indicator">:</span> <span class="l l-Scalar l-Scalar-Plain">container-native-storage-slow</span>
<span class="l l-Scalar l-Scalar-Plain">spec</span><span class="p p-Indicator">:</span>
  <span class="l l-Scalar l-Scalar-Plain">accessModes</span><span class="p p-Indicator">:</span>
  <span class="p p-Indicator">-</span> <span class="l l-Scalar l-Scalar-Plain">ReadWriteOnce</span>
  <span class="l l-Scalar l-Scalar-Plain">resources</span><span class="p p-Indicator">:</span>
    <span class="l l-Scalar l-Scalar-Plain">requests</span><span class="p p-Indicator">:</span>
      <span class="l l-Scalar l-Scalar-Plain">storage</span><span class="p p-Indicator">:</span> <span class="l l-Scalar l-Scalar-Plain">7Gi</span>
</pre></div>


<p>&#8680; Create both PVCs:</p>
<div class="codehilite"><pre><span></span>oc create -f cns-pvc-fast.yml
oc create -f cns-pvc-slow.yml
</pre></div>


<p>They should both be in bound state after a couple of seconds:</p>
<div class="codehilite"><pre><span></span>NAME                        STATUS    VOLUME                                     CAPACITY   ACCESSMODES   AGE
my-container-storage        Bound     pvc-3e249fdd-4ec0-11e7-970e-0a9938370404   5Gi        RWO           32s
my-container-storage-slow   Bound     pvc-405083e6-4ec0-11e7-970e-0a9938370404   7Gi        RWO           29s
</pre></div>


<p>&#8680; If you check again one of the pods of the second cluster&hellip;</p>
<div class="codehilite"><pre><span></span>oc rsh glusterfs-1nvtj
</pre></div>


<p>&#8680; &hellip;you will see a new volume has been created</p>
<div class="codehilite"><pre><span></span>sh-4.2# gluster volume list
vol_0d976f357a7979060a4c32284ca8e136
</pre></div>


<p>This is how you use multiple, parallel GlusterFS pools/clusters on a single OpenShift cluster with a single heketi instance. Whereas the first pool is created with <code>cns-deploy</code> subsequent pools/cluster are created with the <code>heketi-cli</code> client.</p>
<p>Clean up the PVCs and the second <em>StorageClass</em> in preparation for the next section.</p>
<p>&#8680; Delete both PVCs (and therefore their volume)</p>
<div class="codehilite"><pre><span></span>oc delete pvc/my-container-storage-fast
oc delete pvc/my-container-storage-slow
</pre></div>


<p>&#8680; Delete the second <em>StorageClass</em></p>
<div class="codehilite"><pre><span></span>oc delete storageclass/container-native-storage-slow
</pre></div>


<hr />
<h2 id="deleting-a-glusterfs-pool">Deleting a GlusterFS pool<a class="headerlink" href="#deleting-a-glusterfs-pool" title="Permanent link">#</a></h2>
<p>Since we want to re-use <code>node-4</code>, <code>node-5</code> and <code>node-6</code> for the next section we need to delete it the GlusterFS pools on top of them first.</p>
<p>This is a process that involves multiple steps of manipulating the heketi topology with the <code>heketi-cli</code> client.</p>
<p>&#8680; Make sure the client is still properly configured via environment variables:</p>
<div class="codehilite"><pre><span></span>export HEKETI_CLI_SERVER=http://heketi-container-native-storage.cloudapps.34.252.58.209.nip.io
export HEKETI_CLI_USER=admin
export HEKETI_CLI_KEY=myS3cr3tpassw0rd
</pre></div>


<p>&#8680; First list the topology and identify the cluster we want to delete:</p>
<div class="codehilite"><pre><span></span>heketi-cli topology info
</pre></div>


<p>The portions of interest are highlighted:</p>
<div class="codehilite"><pre><span></span><span class="hll">Cluster Id: 46b205a4298c625c4bca2206b7a82dd3
</span>
    Volumes:

    Nodes:

<span class="hll">        Node Id: 538b860406870288af23af0fbc2cd27f
</span>        State: online
        Cluster Id: 46b205a4298c625c4bca2206b7a82dd3
        Zone: 2
        Management Hostname: node-5.lab
        Storage Hostname: 10.0.3.105
        Devices:
<span class="hll">            Id:e481d022cea9bfb11e8a86c0dd8d3499   Name:/dev/xvdc           State:online    Size (GiB):499     Used (GiB):0       Free (GiB):499
</span>                Bricks:

<span class="hll">        Node Id: 604d2eb15a5ca510ff3fc5ecf912d3c0
</span>        State: online
        Cluster Id: 46b205a4298c625c4bca2206b7a82dd3
        Zone: 1
        Management Hostname: node-4.lab
        Storage Hostname: 10.0.2.104
        Devices:
<span class="hll">            Id:09a25a114c53d7669235b368efd2f8d1   Name:/dev/xvdc           State:online    Size (GiB):499     Used (GiB):0       Free (GiB):499
</span>                Bricks:

<span class="hll">        Node Id: 7736bd0cb6a84540860303a6479cacb2
</span>        State: online
        Cluster Id: 46b205a4298c625c4bca2206b7a82dd3
        Zone: 3
        Management Hostname: node-6.lab
        Storage Hostname: 10.0.4.106
        Devices:
<span class="hll">            Id:cccadb2b54dccd99f698d2ae137a22ff   Name:/dev/xvdc           State:online    Size (GiB):499     Used (GiB):0       Free (GiB):499
</span>            Bricks:
</pre></div>


<p>The hierachical dependencies in this topology works as follows: Clusters &gt; Nodes &gt; Devices.<br />
Assuming there are no volumes present these need to be deleted in reverse order.</p>
<p>Note that specific values highlighted above in your environment and carry out the following steps in strict order:</p>
<p>&#8680; Delete all devices of all nodes of the cluster you want to delete:</p>
<div class="codehilite"><pre><span></span>heketi-cli device delete e481d022cea9bfb11e8a86c0dd8d349
heketi-cli device delete 09a25a114c53d7669235b368efd2f8d1
heketi-cli device delete cccadb2b54dccd99f698d2ae137a22ff
</pre></div>


<p>&#8680; Delete all nodes of the cluster in question:</p>
<div class="codehilite"><pre><span></span>heketi-cli node delete 538b860406870288af23af0fbc2cd27f
heketi-cli node delete 604d2eb15a5ca510ff3fc5ecf912d3c0
heketi-cli node delete 7736bd0cb6a84540860303a6479cacb2
</pre></div>


<p>&#8680; Finally delete the cluster:</p>
<div class="codehilite"><pre><span></span>heketi-cli cluster delete 46b205a4298c625c4bca2206b7a82dd3
</pre></div>


<p>&#8680; Confirm the cluster is gone:</p>
<div class="codehilite"><pre><span></span>heketi-cli cluster list
</pre></div>


<p>&#8680; Verify the clusters new topology back to it&rsquo;s state from Module 3.</p>
<p>This deleted all heketi database entries about the cluster. However the GlusterFS pods are still running, since they are controlled directly by OpenShift instead of heketi:</p>
<p>&#8680; List all the pods in the <code>container-native-storage</code> namespace:</p>
<div class="codehilite"><pre><span></span>oc get pods -o wide -n container-native-storage
</pre></div>


<p>The pods formerly running the second GlusterFS pool are still there:</p>
<div class="codehilite"><pre><span></span>NAME              READY     STATUS    RESTARTS   AGE       IP           NODE
glusterfs-1nvtj   1/1       Running   0          1h       10.0.4.106   node-6.lab
glusterfs-5gvw8   1/1       Running   0          1h       10.0.2.104   node-4.lab
glusterfs-5rc2g   1/1       Running   0          4h       10.0.2.101   node-1.lab
glusterfs-b4wg1   1/1       Running   0          1h       10.0.3.105   node-5.lab
glusterfs-jbvdk   1/1       Running   0          4h       10.0.3.102   node-2.lab
glusterfs-rchtr   1/1       Running   0          4h       10.0.4.103   node-3.lab
heketi-1-tn0s9    1/1       Running   0          4h       10.130.2.3   node-6.lab
</pre></div>


<p>They can be stopped by removing the labels OpenShift uses to determine GlusterFS pod placement for CNS.</p>
<p>&#8680; Remove the labels from the last 3 OpenShift nodes like so:</p>
<div class="codehilite"><pre><span></span>oc label node/node-4.lab storagenode-
oc label node/node-5.lab storagenode-
oc label node/node-6.lab storagenode-
</pre></div>


<p>Contrary to the output of these commands the label <code>storagenode</code> is actually removed.</p>
<p>&#8680; Verify that all GlusterFS pods running on <code>node-4</code>, <code>node-5</code> and <code>node-6</code> are indeed terminated:</p>
<div class="codehilite"><pre><span></span>oc get pods -o wide -n container-native-storage
</pre></div>


<div class="admonition note">
<p class="admonition-title">Note</p>
<p>It can take up to 2 minutes for the pods to terminate.</p>
</div>
<p>You should be back down to 3 GlusterFS pods.</p>
<div class="codehilite"><pre><span></span>NAME              READY     STATUS    RESTARTS   AGE       IP           NODE
glusterfs-5rc2g   1/1       Running   0          5h        10.0.2.101   node-1.lab
glusterfs-jbvdk   1/1       Running   0          5h        10.0.3.102   node-2.lab
glusterfs-rchtr   1/1       Running   0          5h        10.0.4.103   node-3.lab
heketi-1-tn0s9    1/1       Running   0          5h        10.130.2.3   node-6.lab
</pre></div>


<hr />
<h2 id="expanding-a-glusterfs-pool">Expanding a GlusterFS pool<a class="headerlink" href="#expanding-a-glusterfs-pool" title="Permanent link">#</a></h2>
<p>Instead of creating additional GlusterFS pools in CNS on OpenShift it is also possible to expand existing pools.</p>
<p>This works similar to creating additional pools, with bulk-import via the topology file. Only this time with nodes added to the existing cluster structure in JSON.</p>
<p>Since manipulating JSON can be error-prone create a new file called <code>expanded-topology.json</code> with contents as below:</p>
<p><kbd>expanded-topology.json:</kbd></p>
<div class="codehilite"><pre><span></span><span class="p">{</span>
    <span class="nt">&quot;clusters&quot;</span><span class="p">:</span> <span class="p">[</span>
        <span class="p">{</span>
            <span class="nt">&quot;nodes&quot;</span><span class="p">:</span> <span class="p">[</span>
                <span class="p">{</span>
                    <span class="nt">&quot;node&quot;</span><span class="p">:</span> <span class="p">{</span>
                        <span class="nt">&quot;hostnames&quot;</span><span class="p">:</span> <span class="p">{</span>
                            <span class="nt">&quot;manage&quot;</span><span class="p">:</span> <span class="p">[</span>
                                <span class="s2">&quot;node-1.lab&quot;</span>
                            <span class="p">],</span>
                            <span class="nt">&quot;storage&quot;</span><span class="p">:</span> <span class="p">[</span>
                                <span class="s2">&quot;10.0.2.101&quot;</span>
                            <span class="p">]</span>
                        <span class="p">},</span>
                        <span class="nt">&quot;zone&quot;</span><span class="p">:</span> <span class="mi">1</span>
                    <span class="p">},</span>
                    <span class="nt">&quot;devices&quot;</span><span class="p">:</span> <span class="p">[</span>
                        <span class="s2">&quot;/dev/xvdc&quot;</span>
                    <span class="p">]</span>
                <span class="p">},</span>
                <span class="p">{</span>
                    <span class="nt">&quot;node&quot;</span><span class="p">:</span> <span class="p">{</span>
                        <span class="nt">&quot;hostnames&quot;</span><span class="p">:</span> <span class="p">{</span>
                            <span class="nt">&quot;manage&quot;</span><span class="p">:</span> <span class="p">[</span>
                                <span class="s2">&quot;node-2.lab&quot;</span>
                            <span class="p">],</span>
                            <span class="nt">&quot;storage&quot;</span><span class="p">:</span> <span class="p">[</span>
                                <span class="s2">&quot;10.0.3.102&quot;</span>
                            <span class="p">]</span>
                        <span class="p">},</span>
                        <span class="nt">&quot;zone&quot;</span><span class="p">:</span> <span class="mi">2</span>
                    <span class="p">},</span>
                    <span class="nt">&quot;devices&quot;</span><span class="p">:</span> <span class="p">[</span>
                        <span class="s2">&quot;/dev/xvdc&quot;</span>
                    <span class="p">]</span>
                <span class="p">},</span>
                <span class="p">{</span>
                    <span class="nt">&quot;node&quot;</span><span class="p">:</span> <span class="p">{</span>
                        <span class="nt">&quot;hostnames&quot;</span><span class="p">:</span> <span class="p">{</span>
                            <span class="nt">&quot;manage&quot;</span><span class="p">:</span> <span class="p">[</span>
                                <span class="s2">&quot;node-3.lab&quot;</span>
                            <span class="p">],</span>
                            <span class="nt">&quot;storage&quot;</span><span class="p">:</span> <span class="p">[</span>
                                <span class="s2">&quot;10.0.4.103&quot;</span>
                            <span class="p">]</span>
                        <span class="p">},</span>
                        <span class="nt">&quot;zone&quot;</span><span class="p">:</span> <span class="mi">3</span>
                    <span class="p">},</span>
                    <span class="nt">&quot;devices&quot;</span><span class="p">:</span> <span class="p">[</span>
                        <span class="s2">&quot;/dev/xvdc&quot;</span>
                    <span class="p">]</span>
                <span class="p">},</span>
                <span class="p">{</span>
                    <span class="nt">&quot;node&quot;</span><span class="p">:</span> <span class="p">{</span>
                        <span class="nt">&quot;hostnames&quot;</span><span class="p">:</span> <span class="p">{</span>
                            <span class="nt">&quot;manage&quot;</span><span class="p">:</span> <span class="p">[</span>
                                <span class="s2">&quot;node-4.lab&quot;</span>
                            <span class="p">],</span>
                            <span class="nt">&quot;storage&quot;</span><span class="p">:</span> <span class="p">[</span>
                                <span class="s2">&quot;10.0.2.104&quot;</span>
                            <span class="p">]</span>
                        <span class="p">},</span>
                        <span class="nt">&quot;zone&quot;</span><span class="p">:</span> <span class="mi">1</span>
                    <span class="p">},</span>
                    <span class="nt">&quot;devices&quot;</span><span class="p">:</span> <span class="p">[</span>
                        <span class="s2">&quot;/dev/xvdc&quot;</span>
                    <span class="p">]</span>
                <span class="p">},</span>
                <span class="p">{</span>
                    <span class="nt">&quot;node&quot;</span><span class="p">:</span> <span class="p">{</span>
                        <span class="nt">&quot;hostnames&quot;</span><span class="p">:</span> <span class="p">{</span>
                            <span class="nt">&quot;manage&quot;</span><span class="p">:</span> <span class="p">[</span>
                                <span class="s2">&quot;node-5.lab&quot;</span>
                            <span class="p">],</span>
                            <span class="nt">&quot;storage&quot;</span><span class="p">:</span> <span class="p">[</span>
                                <span class="s2">&quot;10.0.3.105&quot;</span>
                            <span class="p">]</span>
                        <span class="p">},</span>
                        <span class="nt">&quot;zone&quot;</span><span class="p">:</span> <span class="mi">2</span>
                    <span class="p">},</span>
                    <span class="nt">&quot;devices&quot;</span><span class="p">:</span> <span class="p">[</span>
                        <span class="s2">&quot;/dev/xvdc&quot;</span>
                    <span class="p">]</span>
                <span class="p">},</span>
                <span class="p">{</span>
                    <span class="nt">&quot;node&quot;</span><span class="p">:</span> <span class="p">{</span>
                        <span class="nt">&quot;hostnames&quot;</span><span class="p">:</span> <span class="p">{</span>
                            <span class="nt">&quot;manage&quot;</span><span class="p">:</span> <span class="p">[</span>
                                <span class="s2">&quot;node-6.lab&quot;</span>
                            <span class="p">],</span>
                            <span class="nt">&quot;storage&quot;</span><span class="p">:</span> <span class="p">[</span>
                                <span class="s2">&quot;10.0.4.106&quot;</span>
                            <span class="p">]</span>
                        <span class="p">},</span>
                        <span class="nt">&quot;zone&quot;</span><span class="p">:</span> <span class="mi">3</span>
                    <span class="p">},</span>
                    <span class="nt">&quot;devices&quot;</span><span class="p">:</span> <span class="p">[</span>
                        <span class="s2">&quot;/dev/xvdc&quot;</span>
                    <span class="p">]</span>
                <span class="p">}</span>
            <span class="p">]</span>
        <span class="p">}</span>
    <span class="p">]</span>
<span class="p">}</span>
</pre></div>


<p>&#8680; Again, apply the expected labels to the remaining 3 OpenShift Nodes:</p>
<div class="codehilite"><pre><span></span>oc label node/node-4.lab storagenode=glusterfs
oc label node/node-5.lab storagenode=glusterfs
oc label node/node-6.lab storagenode=glusterfs
</pre></div>


<p>&#8680; Wait for all pods to show <code>1/1</code> in the <code>READY</code> column:</p>
<div class="codehilite"><pre><span></span> oc get pods -o wide -n container-native-storage
</pre></div>


<div class="admonition note">
<p class="admonition-title">Note</p>
<p>It may take up to 3 minutes for the GlusterFS pods to transition into <code>READY</code> state.</p>
</div>
<p>This confirms all GlusterFS pods are ready to receive remote commands:</p>
<div class="codehilite"><pre><span></span>NAME              READY     STATUS    RESTARTS   AGE       IP           NODE
glusterfs-0lr75   1/1       Running   0          4m        10.0.4.106   node-6.lab
glusterfs-1dxz3   1/1       Running   0          4m        10.0.3.105   node-5.lab
glusterfs-5rc2g   1/1       Running   0          5h        10.0.2.101   node-1.lab
glusterfs-8nrn0   1/1       Running   0          4m        10.0.2.104   node-4.lab
glusterfs-jbvdk   1/1       Running   0          5h        10.0.3.102   node-2.lab
glusterfs-rchtr   1/1       Running   0          5h        10.0.4.103   node-3.lab
heketi-1-tn0s9    1/1       Running   0          5h        10.130.2.3   node-6.lab
</pre></div>


<p>&#8680; If you logged out of the session meanwhile re-instate the heketi-cli configuration with the environment variables;</p>
<div class="codehilite"><pre><span></span>export HEKETI_CLI_SERVER=http://heketi-container-native-storage.cloudapps.34.252.58.209.nip.io
export HEKETI_CLI_USER=admin
export HEKETI_CLI_KEY=myS3cr3tpassw0rd
</pre></div>


<p>&#8680; Now load the new topology:</p>
<div class="codehilite"><pre><span></span>heketi-cli topology load --json=expanded-topology.json
</pre></div>


<p>The output indicated that the existing cluster was expanded, rather than creating a new one:</p>
<div class="codehilite"><pre><span></span>Found node node-1.lab on cluster fb67f97166c58f161b85201e1fd9b8ed
    Found device /dev/xvdc
Found node node-2.lab on cluster fb67f97166c58f161b85201e1fd9b8ed
    Found device /dev/xvdc
Found node node-3.lab on cluster fb67f97166c58f161b85201e1fd9b8ed
    Found device /dev/xvdc
Creating node node-4.lab ... ID: 544158e53934a3d351b874b7d915e8d4
    Adding device /dev/xvdc ... OK
Creating node node-5.lab ... ID: 645b6edd4044cb1dd828f728d1c3eb81
    Adding device /dev/xvdc ... OK
Creating node node-6.lab ... ID: 3f39ebf3c8c82531a7ba447135742776
    Adding device /dev/xvdc ... OK
</pre></div>


<p>&#8680; Log on to one of the GlusterFS pods and confirm their new peers:</p>
<div class="codehilite"><pre><span></span>oc rsh glusterfs-0lr75
</pre></div>


<p>&#8680; Run the peer status command inside the container remote session:</p>
<div class="codehilite"><pre><span></span>gluster peer status
</pre></div>


<p>You should now have a GlusterFS consisting of 6 nodes:</p>
<div class="codehilite"><pre><span></span>Number of Peers: 5

Hostname: 10.0.3.102
Uuid: c6a6d571-fd9b-4bd8-aade-e480ec2f8eed
State: Peer in Cluster (Connected)

Hostname: 10.0.4.103
Uuid: 46044d06-a928-49c6-8427-a7ab37268fed
State: Peer in Cluster (Connected)

Hostname: 10.0.2.104
Uuid: 62abb8b9-7a68-4658-ac84-8098a1460703
State: Peer in Cluster (Connected)

Hostname: 10.0.3.105
Uuid: 5b44b6ea-6fb5-4ea9-a6f7-328179dc6dda
State: Peer in Cluster (Connected)

Hostname: 10.0.4.106
Uuid: ed39ecf7-1f5c-4934-a89d-ee1dda9a8f98
State: Peer in Cluster (Connected)
</pre></div>


<p>With this you have expanded the existing pool. New PVCs will start to use capacity from the additional nodes.</p>
<div class="admonition caution">
<p class="admonition-title">Important</p>
<p>In this lab, with this expansion, you now have a GlusterFS pool with mixed media types (both size and speed). It is recommended to have the same media type per pool.<br />
If you like to offer multiple media types for CNS in OpenShift, use separate pools and separate <code>StorageClass</code> objects as described in the <a href="#running-multiple-glusterfs-pools">previous section</a>.</p>
</div>
<hr />
<h2 id="adding-a-device-to-a-node">Adding a device to a node<a class="headerlink" href="#adding-a-device-to-a-node" title="Permanent link">#</a></h2>
<p>Instead of adding entirely new nodes you can also add new storage devices for CNS to use on existing nodes.</p>
<p>It is again possible to do this by loading an updated topology file. Alternatively to bulk-loading via JSON you are also able to do this directly with the <code>heketi-cli</code> utility. This also applies to the previous sections in this module.</p>
<p>For this purpose <code>node-6.lab</code> has an additional, so far unused block device <code>/dev/xvdd</code>.</p>
<p>&#8680; To use the heketi-cli make sure the environment variables are still set:</p>
<div class="codehilite"><pre><span></span>export HEKETI_CLI_SERVER=http://heketi-container-native-storage.cloudapps.34.252.58.209.nip.io
export HEKETI_CLI_USER=admin
export HEKETI_CLI_KEY=myS3cr3tpassw0rd
</pre></div>


<p>&#8680; Determine the UUUI heketi uses to identify <code>node-6.lab</code> in it&rsquo;s database:</p>
<div class="codehilite"><pre><span></span>heketi-cli topology info | grep -B4 node-6.lab
</pre></div>


<p><strong>Note the UUID</strong>:</p>
<div class="codehilite"><pre><span></span><span class="hll">Node Id: ae03d2c5de5cdbd44ba27ff5320d3438
</span>State: online
Cluster Id: eb909a08c8e8fd0bf80499fbbb8a8545
Zone: 3
Management Hostname: node-6.lab
</pre></div>


<p>&#8680; Query the node&rsquo;s available devices:</p>
<div class="codehilite"><pre><span></span>heketi-cli node info ae03d2c5de5cdbd44ba27ff5320d343
</pre></div>


<p>The node has one device available:</p>
<div class="codehilite"><pre><span></span>Node Id: ae03d2c5de5cdbd44ba27ff5320d3438
State: online
Cluster Id: eb909a08c8e8fd0bf80499fbbb8a8545
Zone: 3
Management Hostname: node-6.lab
Storage Hostname: 10.0.4.106
Devices:
Id:cc594d7f5ce59ab2a991c70572a0852f   Name:/dev/xvdc           State:online    Size (GiB):499     Used (GiB):0       Free (GiB):499
</pre></div>


<p>&#8680; Add the device <code>/dev/xvdd</code> to the node using the UUID noted earlier.</p>
<div class="codehilite"><pre><span></span>heketi-cli device add --node=ae03d2c5de5cdbd44ba27ff5320d3438 --name=/dev/xvdd
</pre></div>


<p>The device is registered in heketi&rsquo;s database.</p>
<div class="codehilite"><pre><span></span>Device added successfully
</pre></div>


<p>&#8680; Query the node&rsquo;s available devices again and you&rsquo;ll see a second device.</p>
<div class="codehilite"><pre><span></span>heketi-cli node info ae03d2c5de5cdbd44ba27ff5320d343

Node Id: ae03d2c5de5cdbd44ba27ff5320d3438
State: online
Cluster Id: eb909a08c8e8fd0bf80499fbbb8a8545
Zone: 3
Management Hostname: node-6.lab
Storage Hostname: 10.0.4.106
Devices:
Id:62cbae7a3f6faac38a551a614419cca3   Name:/dev/xvdd           State:online    Size (GiB):499     Used (GiB):0       Free (GiB):499
Id:cc594d7f5ce59ab2a991c70572a0852f   Name:/dev/xvdc           State:online    Size (GiB):499     Used (GiB):0       Free (GiB):499
</pre></div>


<hr />
<h2 id="replacing-a-failed-device">Replacing a failed device<a class="headerlink" href="#replacing-a-failed-device" title="Permanent link">#</a></h2>
<p>One of heketi&rsquo;s advantages is the automation of otherwise tedious manual tasks, like replacing a faulty brick in GlusterFS to repair degraded volumes.<br />
We will simulate this use case now.</p>
<p>&#8680; For convenience you can remain operator for now:</p>
<div class="codehilite"><pre><span></span>[ec2-user@master ~]$ oc whoami
operator
</pre></div>


<p>&#8680; Use any available project to submit some PVCs.</p>
<div class="codehilite"><pre><span></span>oc project playground
</pre></div>


<p>&#8680; Create the file <code>cns-large-pvc.yml</code> with content below:</p>
<p><kbd>cns-large-pvc.yml:</kbd></p>
<div class="codehilite"><pre><span></span><span class="l l-Scalar l-Scalar-Plain">kind</span><span class="p p-Indicator">:</span> <span class="l l-Scalar l-Scalar-Plain">PersistentVolumeClaim</span>
<span class="l l-Scalar l-Scalar-Plain">apiVersion</span><span class="p p-Indicator">:</span> <span class="l l-Scalar l-Scalar-Plain">v1</span>
<span class="l l-Scalar l-Scalar-Plain">metadata</span><span class="p p-Indicator">:</span>
  <span class="l l-Scalar l-Scalar-Plain">name</span><span class="p p-Indicator">:</span> <span class="l l-Scalar l-Scalar-Plain">my-large-container-store</span>
  <span class="l l-Scalar l-Scalar-Plain">annotations</span><span class="p p-Indicator">:</span>
    <span class="l l-Scalar l-Scalar-Plain">volume.beta.kubernetes.io/storage-class</span><span class="p p-Indicator">:</span> <span class="l l-Scalar l-Scalar-Plain">container-native-storage</span>
<span class="l l-Scalar l-Scalar-Plain">spec</span><span class="p p-Indicator">:</span>
  <span class="l l-Scalar l-Scalar-Plain">accessModes</span><span class="p p-Indicator">:</span>
  <span class="p p-Indicator">-</span> <span class="l l-Scalar l-Scalar-Plain">ReadWriteOnce</span>
  <span class="l l-Scalar l-Scalar-Plain">resources</span><span class="p p-Indicator">:</span>
    <span class="l l-Scalar l-Scalar-Plain">requests</span><span class="p p-Indicator">:</span>
      <span class="l l-Scalar l-Scalar-Plain">storage</span><span class="p p-Indicator">:</span> <span class="l l-Scalar l-Scalar-Plain">200Gi</span>
</pre></div>


<p>&#8680; Create this request for a large volume:</p>
<div class="codehilite"><pre><span></span>oc create -f cns-large-pvc.yml
</pre></div>


<p>The requested capacity in this PVC is larger than any single brick on nodes <code>node-1.lab</code>, <code>node-2.lab</code> and <code>node-3.lab</code> so it will be created from the bricks of the other 3 nodes which have larger bricks (500 GiB).</p>
<p>Where are now going to determine a PVCs physical backing device on CNS. This is done with the following relationships between the various entities of GlusterFS, heketi and OpenShift in mind:</p>
<p>PVC -&gt; PV -&gt; heketi volume -&gt; GlusterFS volume -&gt; GlusterFS brick -&gt; Physical Device</p>
<p>&#8680; Get the PV</p>
<div class="codehilite"><pre><span></span>oc describe pvc/my-large-container-store
</pre></div>


<p>Note the PVs name:</p>
<div class="codehilite"><pre><span></span>    Name:       my-large-container-store
    Namespace:  container-native-storage
    StorageClass:   container-native-storage
    Status:     Bound
<span class="hll">    Volume:     pvc-078a1698-4f5b-11e7-ac96-1221f6b873f8
</span>    Labels:     &lt;none&gt;
    Capacity:   200Gi
    Access Modes:   RWO
    No events.
</pre></div>


<p>&#8680; Get the GlusterFS volume name of this PV</p>
<div class="codehilite"><pre><span></span>oc describe pv/pvc-078a1698-4f5b-11e7-ac96-1221f6b873f8
</pre></div>


<p>The GlusterFS volume name as it used by GlusterFS:</p>
<div class="codehilite"><pre><span></span>    Name:       pvc-078a1698-4f5b-11e7-ac96-1221f6b873f8
    Labels:     &lt;none&gt;
    StorageClass:   container-native-storage
    Status:     Bound
    Claim:      container-native-storage/my-large-container-store
    Reclaim Policy: Delete
    Access Modes:   RWO
    Capacity:   200Gi
    Message:
    Source:
        Type:       Glusterfs (a Glusterfs mount on the host that shares a pod&#39;s lifetime)
        EndpointsName:  glusterfs-dynamic-my-large-container-store
<span class="hll">        Path:       vol_3ff9946ddafaabe9745f184e4235d4e1
</span>        ReadOnly:       false
    No events.
</pre></div>


<p>&#8680; Change to the CNS namespace</p>
<div class="codehilite"><pre><span></span>oc project container-native-storage
</pre></div>


<p>&#8680; Log on to one of the GlusterFS pods</p>
<div class="codehilite"><pre><span></span>oc rsh glusterfs-52hkc
</pre></div>


<p>&#8680; Get the volumes topology directly from GlusterFS</p>
<div class="codehilite"><pre><span></span>gluster volume info vol_3ff9946ddafaabe9745f184e4235d4e1
</pre></div>


<p>The output indicates this volume is indeed backed by, among others, <code>node-6.lab</code> (see highlighted line)</p>
<div class="codehilite"><pre><span></span>Volume Name: vol_3ff9946ddafaabe9745f184e4235d4e1
Type: Replicate
Volume ID: 774ae26f-bd3f-4c06-990b-57012cc5974b
Status: Started
Snapshot Count: 0
Number of Bricks: 1 x 3 = 3
Transport-type: tcp
Bricks:
Brick1: 10.0.3.105:/var/lib/heketi/mounts/vg_e1b93823a2906c6758aeec13930a0919/brick_b3d5867d2f86ac93fce6967128643f85/brick
Brick2: 10.0.2.104:/var/lib/heketi/mounts/vg_3c3489a5779c1c840a82a26e0117a415/brick_6323bd816f17c8347b3a68e432501e96/brick
<span class="hll">Brick3: 10.0.4.106:/var/lib/heketi/mounts/vg_62cbae7a3f6faac38a551a614419cca3/brick_a6c92b6a07983e9b8386871f5b82497f/brick
</span>Options Reconfigured:
transport.address-family: inet
performance.readdir-ahead: on
nfs.disable: on
</pre></div>


<p>&#8680; Using the full path of <code>Brick3</code> we cross-check with heketi&rsquo;s topology on which device it is based on:</p>
<div class="codehilite"><pre><span></span>heketi-cli topology info | grep -B2 &#39;/var/lib/heketi/mounts/vg_62cbae7a3f6faac38a551a614419cca3/brick_a6c92b6a07983e9b8386871f5b82497f/brick&#39;
</pre></div>


<p>Among other results grep will show the physical backing device of this brick&rsquo;s mount path:</p>
<div class="codehilite"><pre><span></span><span class="hll">Id:62cbae7a3f6faac38a551a614419cca3   Name:/dev/xvdd           State:online    Size (GiB):499     Used (GiB):201     Free (GiB):298
</span>            Bricks:
                Id:a6c92b6a07983e9b8386871f5b82497f   Size (GiB):200     Path: /var/lib/heketi/mounts/vg_62cbae7a3f6faac38a551a614419cca3/brick_a6c92b6a07983e9b8386871f5b82497f/brick
</pre></div>


<p>In this case it&rsquo;s <code>/dev/xvdd</code> of <code>node-6.lab</code>.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The device might be different for you. This is subject to heketi&rsquo;s dynamic scheduling.</p>
</div>
<p>Let&rsquo;s assume <code>/dev/xvdd</code> on <code>node-6.lab</code> has failed and needs to be replaced.</p>
<p>In such a case you&rsquo;ll take the device&rsquo;s ID and go through the following steps:</p>
<p>&#8680; First, disable the device in heketi</p>
<div class="codehilite"><pre><span></span>heketi-cli device disable 62cbae7a3f6faac38a551a614419cca3
</pre></div>


<p>This will take the device offline and exclude it from future volume creation requests.</p>
<p>&#8680; Now remove the device in heketi</p>
<div class="codehilite"><pre><span></span>heketi-cli device remove 62cbae7a3f6faac38a551a614419cca3
</pre></div>


<p>This will trigger a brick-replacement in GlusterFS. The command will block and heketi in the background will transparently create new bricks for each brick on the device to be deleted. The replacement operation will be conducted with the new bricks replacing all bricks on the device to be deleted.<br />
The new bricks, if possible, will automatically be created in zones different from the remaining bricks to maintain equal balancing and cross-zone availability.</p>
<p>&#8680; Finally, you are now able to delete the device in heketi entirely</p>
<div class="codehilite"><pre><span></span>heketi-cli device delete 62cbae7a3f6faac38a551a614419cca3
</pre></div>


<p>&#8680; Check again the volumes topology directly from GlusterFS</p>
<div class="codehilite"><pre><span></span>oc rsh glusterfs-52hkc

gluster volume info vol_3ff9946ddafaabe9745f184e4235d4e1
</pre></div>


<p>You will notice that <code>Brick3</code> is now a different mount path, but on the same node.</p>
<p>If you cross-check again the new bricks mount path with the heketi topology you will see it&rsquo;s indeed coming from a different device. The remaining device in <code>node-6.lab</code>, in this case <code>/dev/xvdc</code></p>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<p>Device removal while maintaining volume health is possible in heketi as well. Simply delete all devices of the node in question as discussed above. Then the device can be deleted from heketi with <code>heketi-cli device delete &lt;device-uuid&gt;</code></p>
</div>
<hr />
                
                  
                
              
              
                
              
            </article>
          </div>
        </div>
      </main>
      
        
<footer class="md-footer">
  
    <div class="md-footer-nav">
      <nav class="md-footer-nav__inner md-grid">
        
          <a href="../module-3-using-cns/" title="Module 3 - Using Persistent Storage" class="md-flex md-footer-nav__link md-footer-nav__link--prev" rel="prev">
            <div class="md-flex__cell md-flex__cell--shrink">
              <i class="md-icon md-icon--arrow-back md-footer-nav__button"></i>
            </div>
            <div class="md-flex__cell md-flex__cell--stretch md-footer-nav__title">
              <span class="md-flex__ellipsis">
                <span class="md-footer-nav__direction">
                  Previous
                </span>
                Module 3 - Using Persistent Storage
              </span>
            </div>
          </a>
        
        
          <a href="../module-5-advanced/" title="Module 5 - Advanced Topics" class="md-flex md-footer-nav__link md-footer-nav__link--next" rel="next">
            <div class="md-flex__cell md-flex__cell--stretch md-footer-nav__title">
              <span class="md-flex__ellipsis">
                <span class="md-footer-nav__direction">
                  Next
                </span>
                Module 5 - Advanced Topics
              </span>
            </div>
            <div class="md-flex__cell md-flex__cell--shrink">
              <i class="md-icon md-icon--arrow-forward md-footer-nav__button"></i>
            </div>
          </a>
        
      </nav>
    </div>
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-footer-copyright">
        
          <div class="md-footer-copyright__highlight">
            Copyright ©2017 Red Hat, Inc.
          </div>
        
        powered by
        <a href="http://www.mkdocs.org" title="MkDocs">MkDocs</a>
        and
        <a href="http://squidfunk.github.io/mkdocs-material/" title="Material for MkDocs">
          Material for MkDocs</a>
      </div>
      
        
      
    </div>
  </div>
</footer>
      
    </div>
    
      <script src="../assets/javascripts/application-b30566c560.js"></script>
      
      
      <script>app.initialize({url:{base:".."}})</script>
      
    
    
      
    
  </body>
</html>