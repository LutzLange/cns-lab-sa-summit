
<!DOCTYPE html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
      
      
      
        <link rel="shortcut icon" href="../assets/images/favicon.png">
      
      <meta name="generator" content="mkdocs-0.16.3, mkdocs-material-1.7.3">
    
    
      
        <title>Module 4 - Cluster Operations - Container-Native Storage Hands-on Lab - Storage SA Summit 2017</title>
      
    
    
      <script src="../assets/javascripts/modernizr-1df76c4e58.js"></script>
    
    
      <link rel="stylesheet" href="../assets/stylesheets/application-acc56dad49.css">
      
    
    
      
        
        
        
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,400,400i,700|Roboto+Mono">
        <style>body,input{font-family:"Roboto","Helvetica Neue",Helvetica,Arial,sans-serif}code,kbd,pre{font-family:"Roboto Mono","Courier New",Courier,monospace}</style>
      
      <link rel="stylesheet" href="https://fonts.googleapis.com/icon?family=Material+Icons">
    
    
    
  </head>
  
  
  
  
    <body>
  
    <svg class="md-svg">
      <defs>
        
        
      </defs>
    </svg>
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="drawer">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="search">
    <label class="md-overlay" data-md-component="overlay" for="drawer"></label>
    
      <header class="md-header" data-md-component="header">
  <nav class="md-header-nav md-grid">
    <div class="md-flex">
      <div class="md-flex__cell md-flex__cell--shrink">
        
          <a href=".." title="Container-Native Storage Hands-on Lab - Storage SA Summit 2017" class="md-logo md-header-nav__button">
            <img src="../img/RedHat.svg" width="24" height="24">
          </a>
        
      </div>
      <div class="md-flex__cell md-flex__cell--shrink">
        <label class="md-icon md-icon--menu md-header-nav__button" for="drawer"></label>
      </div>
      <div class="md-flex__cell md-flex__cell--stretch">
        <span class="md-flex__ellipsis md-header-nav__title">
          
            
              
                <span class="md-header-nav__parent">
                  Modules
                </span>
              
            
            Module 4 - Cluster Operations
          
        </span>
      </div>
      <div class="md-flex__cell md-flex__cell--shrink">
        
          <label class="md-icon md-icon--search md-header-nav__button" for="search"></label>
          
<div class="md-search" data-md-component="search">
  <label class="md-search__overlay" for="search"></label>
  <div class="md-search__inner">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" required placeholder="Search" accesskey="s" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="query">
      <label class="md-icon md-search__icon" for="search"></label>
      <button type="reset" class="md-icon md-search__icon" data-md-component="reset">close</button>
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" data-md-scrollfix>
        <div class="md-search-result" data-md-component="result" data-md-lang-search="">
          <div class="md-search-result__meta" data-md-lang-result-none="No matching documents" data-md-lang-result-one="1 matching document" data-md-lang-result-other="# matching documents">
            Type to start searching
          </div>
          <ol class="md-search-result__list"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
        
      </div>
      
    </div>
  </nav>
</header>
    
    <div class="md-container">
      
      
      <main class="md-main">
        <div class="md-main__inner md-grid" data-md-component="container">
          
            
              <div class="md-sidebar md-sidebar--primary" data-md-component="navigation">
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    <nav class="md-nav md-nav--primary" data-md-level="0">
  <label class="md-nav__title md-nav__title--site" for="drawer">
    
      <i class="md-logo md-nav__button">
        <img src="../img/RedHat.svg">
      </i>
    
    Container-Native Storage Hands-on Lab - Storage SA Summit 2017
  </label>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
      


  <li class="md-nav__item">
    <a href=".." title="Overview" class="md-nav__link">
      Overview
    </a>
  </li>

    
      
      
      

  


  <li class="md-nav__item md-nav__item--active md-nav__item--nested">
    
      <input class="md-toggle md-nav__toggle" data-md-toggle="nav-2" type="checkbox" id="nav-2" checked>
    
    <label class="md-nav__link" for="nav-2">
      Modules
    </label>
    <nav class="md-nav" data-md-component="collapsible" data-md-level="1">
      <label class="md-nav__title" for="nav-2">
        Modules
      </label>
      <ul class="md-nav__list" data-md-scrollfix>
        
        
          
          
          


  <li class="md-nav__item">
    <a href="../module-1-ocp-in-5/" title="Module 1 - OpenShift in 5 Minutes" class="md-nav__link">
      Module 1 - OpenShift in 5 Minutes
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../module-2-deploy-cns/" title="Module 2 - Deploying Container-Native Storage" class="md-nav__link">
      Module 2 - Deploying Container-Native Storage
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../module-3-using-cns/" title="Module 3 - Using Persistent Storage" class="md-nav__link">
      Module 3 - Using Persistent Storage
    </a>
  </li>

        
          
          
          

  


  <li class="md-nav__item md-nav__item--active">
    
    <input class="md-toggle md-nav__toggle" data-md-toggle="toc" type="checkbox" id="toc">
    
    
      <label class="md-nav__link md-nav__link--active" for="toc">
        Module 4 - Cluster Operations
      </label>
    
    <a href="./" title="Module 4 - Cluster Operations" class="md-nav__link md-nav__link--active">
      Module 4 - Cluster Operations
    </a>
    
      
<nav class="md-nav md-nav--secondary">
  
  
  
    <label class="md-nav__title" for="toc">Table of contents</label>
    <ul class="md-nav__list" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#running-multiple-glusterfs-pools" title="Running multiple GlusterFS pools" class="md-nav__link">
    Running multiple GlusterFS pools
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#deleting-a-glusterfs-pool" title="Deleting a GlusterFS pool" class="md-nav__link">
    Deleting a GlusterFS pool
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#expanding-a-glusterfs-pool" title="Expanding a GlusterFS pool" class="md-nav__link">
    Expanding a GlusterFS pool
  </a>
  
</li>
      
      
      
    </ul>
  
</nav>
    
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../module-5-advanced/" title="Module 5 - Advanced Topics" class="md-nav__link">
      Module 5 - Advanced Topics
    </a>
  </li>

        
      </ul>
    </nav>
  </li>

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              <div class="md-sidebar md-sidebar--secondary" data-md-component="toc">
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    
<nav class="md-nav md-nav--secondary">
  
  
  
    <label class="md-nav__title" for="toc">Table of contents</label>
    <ul class="md-nav__list" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#running-multiple-glusterfs-pools" title="Running multiple GlusterFS pools" class="md-nav__link">
    Running multiple GlusterFS pools
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#deleting-a-glusterfs-pool" title="Deleting a GlusterFS pool" class="md-nav__link">
    Deleting a GlusterFS pool
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#expanding-a-glusterfs-pool" title="Expanding a GlusterFS pool" class="md-nav__link">
    Expanding a GlusterFS pool
  </a>
  
</li>
      
      
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          <div class="md-content">
            <article class="md-content__inner md-typeset">
              
                
                
                  <h1>Module 4 - Cluster Operations</h1>
                
                <div class="admonition summary">
<p class="admonition-title">Overview</p>
<p>In this module you be introduced to some standard operational procedures. You will learn how to run multiple GlusterFS <em>Trusted Storage Pools</em> on OpenShift and how to expand and maintain deployments.</p>
<p>Herein, we will use the term pool (GlusterFS terminology) and cluster (heketi terminology) interchangeably.</p>
</div>
<h2 id="running-multiple-glusterfs-pools">Running multiple GlusterFS pools<a class="headerlink" href="#running-multiple-glusterfs-pools" title="Permanent link">#</a></h2>
<p>In the previous modules a single GlusterFS clusters was used to supply <em>PersistentVolume</em> to applications. CNS allows for multiple clusters to run in a single OpenShift deployment.</p>
<p>There are several use cases for this:</p>
<ol>
<li>
<p>Provide data isolation between clusters of different tenants</p>
</li>
<li>
<p>Provide multiple performance tiers of CNS, i.e. HDD-based vs. SSD-based</p>
</li>
<li>
<p>Overcome the maximum amount of 300 PVs per cluster, currently imposed in version 3.5</p>
</li>
</ol>
<p>To deploy a second GlusterFS pool follow these steps:</p>
<p>&#8680; Log in as <code>operator</code> to namespace <code>container-native-storage</code></p>
<div class="codehilite"><pre><span></span>oc login -u operator -n container-native-storage
</pre></div>


<p>Your deployment has 6 OpenShift Application Nodes in total, <code>node-1</code>, <code>node-2</code> and <code>node-3</code> currently setup running CNS. We will now set up a second cluster using <code>node-4</code>, <code>node-5</code> and <code>node-6</code>.</p>
<p>&#8680; Apply additional labels (user-defined key-value pairs) to the remaining 3 OpenShift Nodes:</p>
<div class="codehilite"><pre><span></span>oc label node/node-4.lab storagenode=glusterfs
oc label node/node-5.lab storagenode=glusterfs
oc label node/node-6.lab storagenode=glusterfs
</pre></div>


<p>The label will be used to control GlusterFS pod placement and availability.</p>
<p>&#8680; Wait for all pods to show <code>1/1</code> in the <code>READY</code> column:</p>
<div class="codehilite"><pre><span></span> oc get pods -o wide -n container-native-storage
</pre></div>


<p>For bulk import of new nodes into a new cluster, the topology JSON file can be updated to include a second cluster with a separate set of nodes.</p>
<p>&#8680; Create a new file named updated-topology.json with the content below:</p>
<p><kbd>updated-topology.json:</kbd></p>
<div class="codehilite"><pre><span></span><span class="p">{</span>
    <span class="nt">&quot;clusters&quot;</span><span class="p">:</span> <span class="p">[</span>
        <span class="p">{</span>
            <span class="nt">&quot;nodes&quot;</span><span class="p">:</span> <span class="p">[</span>
                <span class="p">{</span>
                    <span class="nt">&quot;node&quot;</span><span class="p">:</span> <span class="p">{</span>
                        <span class="nt">&quot;hostnames&quot;</span><span class="p">:</span> <span class="p">{</span>
                            <span class="nt">&quot;manage&quot;</span><span class="p">:</span> <span class="p">[</span>
                                <span class="s2">&quot;node-1.lab&quot;</span>
                            <span class="p">],</span>
                            <span class="nt">&quot;storage&quot;</span><span class="p">:</span> <span class="p">[</span>
                                <span class="s2">&quot;10.0.2.101&quot;</span>
                            <span class="p">]</span>
                        <span class="p">},</span>
                        <span class="nt">&quot;zone&quot;</span><span class="p">:</span> <span class="mi">1</span>
                    <span class="p">},</span>
                    <span class="nt">&quot;devices&quot;</span><span class="p">:</span> <span class="p">[</span>
                        <span class="s2">&quot;/dev/xvdc&quot;</span>
                    <span class="p">]</span>
                <span class="p">},</span>
                <span class="p">{</span>
                    <span class="nt">&quot;node&quot;</span><span class="p">:</span> <span class="p">{</span>
                        <span class="nt">&quot;hostnames&quot;</span><span class="p">:</span> <span class="p">{</span>
                            <span class="nt">&quot;manage&quot;</span><span class="p">:</span> <span class="p">[</span>
                                <span class="s2">&quot;node-2.lab&quot;</span>
                            <span class="p">],</span>
                            <span class="nt">&quot;storage&quot;</span><span class="p">:</span> <span class="p">[</span>
                                <span class="s2">&quot;10.0.3.102&quot;</span>
                            <span class="p">]</span>
                        <span class="p">},</span>
                        <span class="nt">&quot;zone&quot;</span><span class="p">:</span> <span class="mi">2</span>
                    <span class="p">},</span>
                    <span class="nt">&quot;devices&quot;</span><span class="p">:</span> <span class="p">[</span>
                        <span class="s2">&quot;/dev/xvdc&quot;</span>
                    <span class="p">]</span>
                <span class="p">},</span>
                <span class="p">{</span>
                    <span class="nt">&quot;node&quot;</span><span class="p">:</span> <span class="p">{</span>
                        <span class="nt">&quot;hostnames&quot;</span><span class="p">:</span> <span class="p">{</span>
                            <span class="nt">&quot;manage&quot;</span><span class="p">:</span> <span class="p">[</span>
                                <span class="s2">&quot;node-3.lab&quot;</span>
                            <span class="p">],</span>
                            <span class="nt">&quot;storage&quot;</span><span class="p">:</span> <span class="p">[</span>
                                <span class="s2">&quot;10.0.4.103&quot;</span>
                            <span class="p">]</span>
                        <span class="p">},</span>
                        <span class="nt">&quot;zone&quot;</span><span class="p">:</span> <span class="mi">3</span>
                    <span class="p">},</span>
                    <span class="nt">&quot;devices&quot;</span><span class="p">:</span> <span class="p">[</span>
                        <span class="s2">&quot;/dev/xvdc&quot;</span>
                    <span class="p">]</span>
                <span class="p">}</span>
            <span class="p">]</span>
<span class="hll">        <span class="p">},</span>
</span>        <span class="p">{</span>
            <span class="nt">&quot;nodes&quot;</span><span class="p">:</span> <span class="p">[</span>
                <span class="p">{</span>
                    <span class="nt">&quot;node&quot;</span><span class="p">:</span> <span class="p">{</span>
                        <span class="nt">&quot;hostnames&quot;</span><span class="p">:</span> <span class="p">{</span>
                            <span class="nt">&quot;manage&quot;</span><span class="p">:</span> <span class="p">[</span>
                                <span class="s2">&quot;node-4.lab&quot;</span>
                            <span class="p">],</span>
                            <span class="nt">&quot;storage&quot;</span><span class="p">:</span> <span class="p">[</span>
                                <span class="s2">&quot;10.0.2.104&quot;</span>
                            <span class="p">]</span>
                        <span class="p">},</span>
                        <span class="nt">&quot;zone&quot;</span><span class="p">:</span> <span class="mi">1</span>
                    <span class="p">},</span>
                    <span class="nt">&quot;devices&quot;</span><span class="p">:</span> <span class="p">[</span>
                        <span class="s2">&quot;/dev/xvdc&quot;</span>
                    <span class="p">]</span>
                <span class="p">},</span>
                <span class="p">{</span>
                    <span class="nt">&quot;node&quot;</span><span class="p">:</span> <span class="p">{</span>
                        <span class="nt">&quot;hostnames&quot;</span><span class="p">:</span> <span class="p">{</span>
                            <span class="nt">&quot;manage&quot;</span><span class="p">:</span> <span class="p">[</span>
                                <span class="s2">&quot;node-5.lab&quot;</span>
                            <span class="p">],</span>
                            <span class="nt">&quot;storage&quot;</span><span class="p">:</span> <span class="p">[</span>
                                <span class="s2">&quot;10.0.3.105&quot;</span>
                            <span class="p">]</span>
                        <span class="p">},</span>
                        <span class="nt">&quot;zone&quot;</span><span class="p">:</span> <span class="mi">2</span>
                    <span class="p">},</span>
                    <span class="nt">&quot;devices&quot;</span><span class="p">:</span> <span class="p">[</span>
                        <span class="s2">&quot;/dev/xvdc&quot;</span>
                    <span class="p">]</span>
                <span class="p">},</span>
                <span class="p">{</span>
                    <span class="nt">&quot;node&quot;</span><span class="p">:</span> <span class="p">{</span>
                        <span class="nt">&quot;hostnames&quot;</span><span class="p">:</span> <span class="p">{</span>
                            <span class="nt">&quot;manage&quot;</span><span class="p">:</span> <span class="p">[</span>
                                <span class="s2">&quot;node-6.lab&quot;</span>
                            <span class="p">],</span>
                            <span class="nt">&quot;storage&quot;</span><span class="p">:</span> <span class="p">[</span>
                                <span class="s2">&quot;10.0.4.106&quot;</span>
                            <span class="p">]</span>
                        <span class="p">},</span>
                        <span class="nt">&quot;zone&quot;</span><span class="p">:</span> <span class="mi">3</span>
                    <span class="p">},</span>
                    <span class="nt">&quot;devices&quot;</span><span class="p">:</span> <span class="p">[</span>
                        <span class="s2">&quot;/dev/xvdc&quot;</span>
                    <span class="p">]</span>
                <span class="p">}</span>
            <span class="p">]</span>
        <span class="p">}</span>
    <span class="p">]</span>
<span class="p">}</span>
</pre></div>


<p>The file contains the same content as <code>topology.json</code> with a second cluster specification (beginning at the highlighted line).</p>
<p>When loading this topology to heketi, it will recognize the existing cluster (leaving it unchanged) and start creating the new one, with the same bootstrapping process as seen with <code>cns-deploy</code>.</p>
<p>&#8680; Prepare the heketi CLI tool like previously in <a href="../module-2-deploy-cns/#heketi-env-setup">Module 2</a>.</p>
<div class="codehilite"><pre><span></span>export HEKETI_CLI_SERVER=http://heketi-container-native-storage.cloudapps.34.252.58.209.nip.io
export HEKETI_CLI_USER=admin
export HEKETI_CLI_KEY=myS3cr3tpassw0rd
</pre></div>


<p>&#8680; Verify there is currently only a single cluster known to heketi</p>
<div class="codehilite"><pre><span></span>heketi-cli cluster list
</pre></div>


<p>Example output:</p>
<div class="codehilite"><pre><span></span>Clusters:
fb67f97166c58f161b85201e1fd9b8ed
</pre></div>


<p><strong>Note this cluster&rsquo;s id</strong> for later reference.</p>
<p>&#8680; Load the new topology with the heketi client</p>
<div class="codehilite"><pre><span></span>heketi-cli topology load --json=updated-topology.jso
</pre></div>


<p>You should see output similar to the following:</p>
<div class="codehilite"><pre><span></span>Found node node-1.lab on cluster fb67f97166c58f161b85201e1fd9b8ed
Found device /dev/xvdc
Found node node-2.lab on cluster fb67f97166c58f161b85201e1fd9b8ed
Found device /dev/xvdc
Found node node-3.lab on cluster fb67f97166c58f161b85201e1fd9b8ed
Found device /dev/xvdc
Creating cluster ... ID: 46b205a4298c625c4bca2206b7a82dd3
Creating node node-4.lab ... ID: 604d2eb15a5ca510ff3fc5ecf912d3c0
Adding device /dev/xvdc ... OK
Creating node node-5.lab ... ID: 538b860406870288af23af0fbc2cd27f
Adding device /dev/xvdc ... OK
Creating node node-6.lab ... ID: 7736bd0cb6a84540860303a6479cacb2
Adding device /dev/xvdc ... OK
</pre></div>


<p>As indicated from above output a new cluster got created.</p>
<p>&#8680; List all clusters:</p>
<div class="codehilite"><pre><span></span>heketi-cli cluster list
</pre></div>


<p>You should see a second cluster in the list:</p>
<div class="codehilite"><pre><span></span>Clusters:
46b205a4298c625c4bca2206b7a82dd3
fb67f97166c58f161b85201e1fd9b8ed
</pre></div>


<p>The second cluster with the ID <code>46b205a4298c625c4bca2206b7a82dd3</code> is an entirely independent GlusterFS deployment. <strong>Note this second cluster&rsquo;s ID</strong> as well for later reference.</p>
<p>Now we have two independent GlusterFS clusters managed by the same heketi instance:</p>
<table>
<thead>
<tr>
<th></th>
<th>Nodes</th>
<th>Cluster UUID</th>
</tr>
</thead>
<tbody>
<tr>
<td>First Cluster</td>
<td>node-1, node-2, node-3</td>
<td>fb67f97166c58f161b85201e1fd9b8ed</td>
</tr>
<tr>
<td>Second Cluster</td>
<td>node-4, node-5, node-6</td>
<td>46b205a4298c625c4bca2206b7a82dd3</td>
</tr>
</tbody>
</table>
<p>&#8680; Query the updated topology:</p>
<div class="codehilite"><pre><span></span>heketi-cli topology info
</pre></div>


<p>Abbreviated output:</p>
<div class="codehilite"><pre><span></span>Cluster Id: 46b205a4298c625c4bca2206b7a82dd3

    Volumes:

    Nodes:

      Node Id: 538b860406870288af23af0fbc2cd27f
      State: online
      Cluster Id: 46b205a4298c625c4bca2206b7a82dd3
      Zone: 2
      Management Hostname: node-5.lab
      Storage Hostname: 10.0.3.105
      Devices:
        Id:e481d022cea9bfb11e8a86c0dd8d3499   Name:/dev/xvdc           State:online    Size (GiB):499     Used (GiB):0       Free (GiB):499
            Bricks:

      Node Id: 604d2eb15a5ca510ff3fc5ecf912d3c0
      State: online
      Cluster Id: 46b205a4298c625c4bca2206b7a82dd3
      Zone: 1
      Management Hostname: node-4.lab
      Storage Hostname: 10.0.2.104
      Devices:
        Id:09a25a114c53d7669235b368efd2f8d1   Name:/dev/xvdc           State:online    Size (GiB):499     Used (GiB):0       Free (GiB):499
            Bricks:

      Node Id: 7736bd0cb6a84540860303a6479cacb2
      State: online
      Cluster Id: 46b205a4298c625c4bca2206b7a82dd3
      Zone: 3
      Management Hostname: node-6.lab
      Storage Hostname: 10.0.4.106
      Devices:
        Id:cccadb2b54dccd99f698d2ae137a22ff   Name:/dev/xvdc           State:online    Size (GiB):499     Used (GiB):0       Free (GiB):499
            Bricks:

Cluster Id: fb67f97166c58f161b85201e1fd9b8ed

[...output omitted for brevity...]
</pre></div>


<p>heketi formed an new, independent 3-node GlusterFS cluster on those nodes.</p>
<p>&#8680; Check running GlusterFS pods</p>
<div class="codehilite"><pre><span></span>oc get pods -o wide
</pre></div>


<p>From the output you can spot the pod names running on the new cluster&rsquo;s nodes:</p>
<div class="codehilite"><pre><span></span>NAME              READY     STATUS    RESTARTS   AGE       IP           NODE
<span class="hll">glusterfs-1nvtj   1/1       Running   0          23m       10.0.4.106   node-6.lab
</span><span class="hll">glusterfs-5gvw8   1/1       Running   0          24m       10.0.2.104   node-4.lab
</span>glusterfs-5rc2g   1/1       Running   0          4h        10.0.2.101   node-1.lab
<span class="hll">glusterfs-b4wg1   1/1       Running   0          24m       10.0.3.105   node-5.lab
</span>glusterfs-jbvdk   1/1       Running   0          4h        10.0.3.102   node-2.lab
glusterfs-rchtr   1/1       Running   0          4h        10.0.4.103   node-3.lab
heketi-1-tn0s9    1/1       Running   0          4h        10.130.2.3   node-6.lab
</pre></div>


<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Again note that the pod names are dynamically generated and will be different. Use the FQDN of your hosts to determine one of new cluster&rsquo;s pods.</p>
</div>
<p>&#8680; Log on to one of the new cluster&rsquo;s pods:</p>
<div class="codehilite"><pre><span></span>oc rsh glusterfs-1nvtj
</pre></div>


<p>&#8680; Query this GlusterFS node for it&rsquo;s peers:</p>
<div class="codehilite"><pre><span></span>gluster peer status
</pre></div>


<p>As expected this node only has 2 peers, evidence that it&rsquo;s running in it&rsquo;s own GlusterFS pool separate from the first cluster in deployed in Module 2.</p>
<div class="codehilite"><pre><span></span>Number of Peers: 2

Hostname: node-5.lab
Uuid: 0db9b5d0-7fa8-4d2f-8b9e-6664faf34606
State: Peer in Cluster (Connected)
Other names:
10.0.3.105

Hostname: node-4.lab
Uuid: 695b661d-2a55-4f94-b22e-40a9db79c01a
State: Peer in Cluster (Connected)
</pre></div>


<p>Before you can use the second cluster two tasks have to be accomplished:</p>
<ol>
<li>
<p>The <em>StorageClass</em> for the first cluster has to be updated to point the first cluster&rsquo;s UUID,</p>
</li>
<li>
<p>A second <em>StorageClass</em> for the second cluster has to be created, pointing to the same heketi</p>
</li>
</ol>
<div class="admonition tip">
<p class="admonition-title">Why do we need to update the first <em>StorageClass</em>?</p>
<p>By default, when no cluster UUID is specified, heketi serves volume creation requests from any cluster currently registered to it.<br />
In order to request a volume from specific cluster you have to supply the cluster&rsquo;s UUID to heketi. The first <em>StorageClass</em> has no UUID specified so far.</p>
</div>
<p>&#8680; Edit the existing <em>StorageClass</em> definition by updating the <code>cns-storageclass.yml</code> file from <a href="../module-3-using-cns/#storageclass-setup">Module 3</a> like below:</p>
<p><kbd>cns-storageclass.yml:</kbd></p>
<div class="codehilite"><pre><span></span><span class="l l-Scalar l-Scalar-Plain">apiVersion</span><span class="p p-Indicator">:</span> <span class="l l-Scalar l-Scalar-Plain">storage.k8s.io/v1beta1</span>
<span class="l l-Scalar l-Scalar-Plain">kind</span><span class="p p-Indicator">:</span> <span class="l l-Scalar l-Scalar-Plain">StorageClass</span>
<span class="l l-Scalar l-Scalar-Plain">metadata</span><span class="p p-Indicator">:</span>
  <span class="l l-Scalar l-Scalar-Plain">name</span><span class="p p-Indicator">:</span> <span class="l l-Scalar l-Scalar-Plain">container-native-storage</span>
  <span class="l l-Scalar l-Scalar-Plain">annotations</span><span class="p p-Indicator">:</span>
    <span class="l l-Scalar l-Scalar-Plain">storageclass.beta.kubernetes.io/is-default-class</span><span class="p p-Indicator">:</span> <span class="s">&quot;true&quot;</span>
<span class="l l-Scalar l-Scalar-Plain">provisioner</span><span class="p p-Indicator">:</span> <span class="l l-Scalar l-Scalar-Plain">kubernetes.io/glusterfs</span>
<span class="l l-Scalar l-Scalar-Plain">parameters</span><span class="p p-Indicator">:</span>
  <span class="l l-Scalar l-Scalar-Plain">resturl</span><span class="p p-Indicator">:</span> <span class="s">&quot;http://heketi-container-native-storage.cloudapps.34.252.58.209.nip.io&quot;</span>
  <span class="l l-Scalar l-Scalar-Plain">restauthenabled</span><span class="p p-Indicator">:</span> <span class="s">&quot;true&quot;</span>
  <span class="l l-Scalar l-Scalar-Plain">restuser</span><span class="p p-Indicator">:</span> <span class="s">&quot;admin&quot;</span>
  <span class="l l-Scalar l-Scalar-Plain">volumetype</span><span class="p p-Indicator">:</span> <span class="s">&quot;replicate:3&quot;</span>
  <span class="l l-Scalar l-Scalar-Plain">secretNamespace</span><span class="p p-Indicator">:</span> <span class="s">&quot;default&quot;</span>
  <span class="l l-Scalar l-Scalar-Plain">secretName</span><span class="p p-Indicator">:</span> <span class="s">&quot;cns-secret&quot;</span>
<span class="hll">  <span class="l l-Scalar l-Scalar-Plain">clusterid</span><span class="p p-Indicator">:</span> <span class="s">&quot;fb67f97166c58f161b85201e1fd9b8ed&quot;</span>
</span></pre></div>


<p>Note the additional <code>clusterid</code> parameter highlighted. It&rsquo;s the cluster&rsquo;s UUID as known by heketi. Don&rsquo;t change anything else.</p>
<p>&#8680; Delete the existing <em>StorageClass</em> definition in OpenShift</p>
<div class="codehilite"><pre><span></span>oc delete storageclass/container-native-storage
</pre></div>


<p>&#8680; Add the <em>StorageClass</em> again:</p>
<div class="codehilite"><pre><span></span>oc create -f cns-storageclass.yml
</pre></div>


<p>&#8680; Create a new file called cns-storageclass-slow.yml with the following contents:</p>
<p><kbd>cns-storageclass-slow.yml:</kbd></p>
<div class="codehilite"><pre><span></span><span class="l l-Scalar l-Scalar-Plain">apiVersion</span><span class="p p-Indicator">:</span> <span class="l l-Scalar l-Scalar-Plain">storage.k8s.io/v1beta1</span>
<span class="l l-Scalar l-Scalar-Plain">kind</span><span class="p p-Indicator">:</span> <span class="l l-Scalar l-Scalar-Plain">StorageClass</span>
<span class="l l-Scalar l-Scalar-Plain">metadata</span><span class="p p-Indicator">:</span>
  <span class="l l-Scalar l-Scalar-Plain">name</span><span class="p p-Indicator">:</span> <span class="l l-Scalar l-Scalar-Plain">container-native-storage-slow</span>
<span class="l l-Scalar l-Scalar-Plain">provisioner</span><span class="p p-Indicator">:</span> <span class="l l-Scalar l-Scalar-Plain">kubernetes.io/glusterfs</span>
<span class="l l-Scalar l-Scalar-Plain">parameters</span><span class="p p-Indicator">:</span>
  <span class="l l-Scalar l-Scalar-Plain">resturl</span><span class="p p-Indicator">:</span> <span class="s">&quot;http://heketi-container-native-storage.cloudapps.34.252.58.209.nip.io&quot;</span>
  <span class="l l-Scalar l-Scalar-Plain">restauthenabled</span><span class="p p-Indicator">:</span> <span class="s">&quot;true&quot;</span>
  <span class="l l-Scalar l-Scalar-Plain">restuser</span><span class="p p-Indicator">:</span> <span class="s">&quot;admin&quot;</span>
  <span class="l l-Scalar l-Scalar-Plain">volumetype</span><span class="p p-Indicator">:</span> <span class="s">&quot;replicate:3&quot;</span>
  <span class="l l-Scalar l-Scalar-Plain">secretNamespace</span><span class="p p-Indicator">:</span> <span class="s">&quot;default&quot;</span>
  <span class="l l-Scalar l-Scalar-Plain">secretName</span><span class="p p-Indicator">:</span> <span class="s">&quot;cns-secret&quot;</span>
<span class="hll">  <span class="l l-Scalar l-Scalar-Plain">clusterid</span><span class="p p-Indicator">:</span> <span class="s">&quot;46b205a4298c625c4bca2206b7a82dd3&quot;</span>
</span></pre></div>


<p>Again note the <code>clusterid</code> in the <code>parameters</code> section referencing the second cluster&rsquo;s UUID</p>
<p>&#8680; Add the new <em>StorageClass</em>:</p>
<div class="codehilite"><pre><span></span>oc create -f cns-storageclass-slow.yml
</pre></div>


<p>Let&rsquo;s verify both <em>StorageClasses</em> are working as expected:</p>
<p>&#8680; Create the following two files containing PVCs issued against either of both GlusterFS pools via their respective <em>StorageClass</em>:</p>
<p><kbd>cns-pvc-fast.yml:</kbd></p>
<div class="codehilite"><pre><span></span><span class="l l-Scalar l-Scalar-Plain">kind</span><span class="p p-Indicator">:</span> <span class="l l-Scalar l-Scalar-Plain">PersistentVolumeClaim</span>
<span class="l l-Scalar l-Scalar-Plain">apiVersion</span><span class="p p-Indicator">:</span> <span class="l l-Scalar l-Scalar-Plain">v1</span>
<span class="l l-Scalar l-Scalar-Plain">metadata</span><span class="p p-Indicator">:</span>
  <span class="l l-Scalar l-Scalar-Plain">name</span><span class="p p-Indicator">:</span> <span class="l l-Scalar l-Scalar-Plain">my-container-storage-fast</span>
  <span class="l l-Scalar l-Scalar-Plain">annotations</span><span class="p p-Indicator">:</span>
    <span class="l l-Scalar l-Scalar-Plain">volume.beta.kubernetes.io/storage-class</span><span class="p p-Indicator">:</span> <span class="l l-Scalar l-Scalar-Plain">container-native-storage</span>
<span class="l l-Scalar l-Scalar-Plain">spec</span><span class="p p-Indicator">:</span>
  <span class="l l-Scalar l-Scalar-Plain">accessModes</span><span class="p p-Indicator">:</span>
  <span class="p p-Indicator">-</span> <span class="l l-Scalar l-Scalar-Plain">ReadWriteOnce</span>
  <span class="l l-Scalar l-Scalar-Plain">resources</span><span class="p p-Indicator">:</span>
    <span class="l l-Scalar l-Scalar-Plain">requests</span><span class="p p-Indicator">:</span>
      <span class="l l-Scalar l-Scalar-Plain">storage</span><span class="p p-Indicator">:</span> <span class="l l-Scalar l-Scalar-Plain">5Gi</span>
</pre></div>


<p><kbd>cns-pvc-slow.yml:</kbd></p>
<div class="codehilite"><pre><span></span><span class="l l-Scalar l-Scalar-Plain">kind</span><span class="p p-Indicator">:</span> <span class="l l-Scalar l-Scalar-Plain">PersistentVolumeClaim</span>
<span class="l l-Scalar l-Scalar-Plain">apiVersion</span><span class="p p-Indicator">:</span> <span class="l l-Scalar l-Scalar-Plain">v1</span>
<span class="l l-Scalar l-Scalar-Plain">metadata</span><span class="p p-Indicator">:</span>
  <span class="l l-Scalar l-Scalar-Plain">name</span><span class="p p-Indicator">:</span> <span class="l l-Scalar l-Scalar-Plain">my-container-storage-slow</span>
  <span class="l l-Scalar l-Scalar-Plain">annotations</span><span class="p p-Indicator">:</span>
    <span class="l l-Scalar l-Scalar-Plain">volume.beta.kubernetes.io/storage-class</span><span class="p p-Indicator">:</span> <span class="l l-Scalar l-Scalar-Plain">container-native-storage-slow</span>
<span class="l l-Scalar l-Scalar-Plain">spec</span><span class="p p-Indicator">:</span>
  <span class="l l-Scalar l-Scalar-Plain">accessModes</span><span class="p p-Indicator">:</span>
  <span class="p p-Indicator">-</span> <span class="l l-Scalar l-Scalar-Plain">ReadWriteOnce</span>
  <span class="l l-Scalar l-Scalar-Plain">resources</span><span class="p p-Indicator">:</span>
    <span class="l l-Scalar l-Scalar-Plain">requests</span><span class="p p-Indicator">:</span>
      <span class="l l-Scalar l-Scalar-Plain">storage</span><span class="p p-Indicator">:</span> <span class="l l-Scalar l-Scalar-Plain">7Gi</span>
</pre></div>


<p>&#8680; Create both PVCs:</p>
<div class="codehilite"><pre><span></span>oc create -f cns-pvc-fast.yml
oc create -f cns-pvc-slow.yml
</pre></div>


<p>They should both be in bound state after a couple of seconds:</p>
<div class="codehilite"><pre><span></span>NAME                        STATUS    VOLUME                                     CAPACITY   ACCESSMODES   AGE
my-container-storage        Bound     pvc-3e249fdd-4ec0-11e7-970e-0a9938370404   5Gi        RWO           32s
my-container-storage-slow   Bound     pvc-405083e6-4ec0-11e7-970e-0a9938370404   7Gi        RWO           29s
</pre></div>


<p>&#8680; If you check again one of the pods of the second cluster&hellip;</p>
<div class="codehilite"><pre><span></span>oc rsh glusterfs-1nvtj
</pre></div>


<p>&#8680; &hellip;you will see a new volume has been created</p>
<div class="codehilite"><pre><span></span>sh-4.2# gluster volume list
vol_0d976f357a7979060a4c32284ca8e136
</pre></div>


<p>This is how you use multiple, parallel GlusterFS pools/clusters on a single OpenShift cluster with a single heketi instance. Whereas the first pool is created with <code>cns-deploy</code> subsequent pools/cluster are created with the <code>heketi-cli</code> client.</p>
<p>Clean up the PVCs and the second <em>StorageClass</em> in preparation for the next section.</p>
<p>&#8680; Delete both PVCs (and therefore their volume)</p>
<div class="codehilite"><pre><span></span>oc delete pvc/my-container-storage-fast
oc delete pvc/my-container-storage-slow
</pre></div>


<p>&#8680; Delete the second <em>StorageClass</em></p>
<div class="codehilite"><pre><span></span>oc delete storageclass/container-native-storage-slow
</pre></div>


<hr />
<h2 id="deleting-a-glusterfs-pool">Deleting a GlusterFS pool<a class="headerlink" href="#deleting-a-glusterfs-pool" title="Permanent link">#</a></h2>
<p>Since we want to re-use <code>node-4</code>, <code>node-5</code> and <code>node-6</code> for the next section we need to delete it the GlusterFS pools on top of them first.</p>
<p>This is a process that involves multiple steps of manipulating the heketi topology with the <code>heketi-cli</code> client.</p>
<p>&#8680; Make sure the client is still properly configured via environment variables:</p>
<div class="codehilite"><pre><span></span>export HEKETI_CLI_SERVER=http://heketi-container-native-storage.cloudapps.34.252.58.209.nip.io
export HEKETI_CLI_USER=admin
export HEKETI_CLI_KEY=myS3cr3tpassw0rd
</pre></div>


<p>&#8680; First list the topology and identify the cluster we want to delete:</p>
<div class="codehilite"><pre><span></span>heketi-cli topology info
</pre></div>


<p>The portions of interest are highlighted:</p>
<div class="codehilite"><pre><span></span><span class="hll">Cluster Id: 46b205a4298c625c4bca2206b7a82dd3
</span>
    Volumes:

    Nodes:

<span class="hll">        Node Id: 538b860406870288af23af0fbc2cd27f
</span>        State: online
        Cluster Id: 46b205a4298c625c4bca2206b7a82dd3
        Zone: 2
        Management Hostname: node-5.lab
        Storage Hostname: 10.0.3.105
        Devices:
<span class="hll">            Id:e481d022cea9bfb11e8a86c0dd8d3499   Name:/dev/xvdc           State:online    Size (GiB):499     Used (GiB):0       Free (GiB):499
</span>                Bricks:

<span class="hll">        Node Id: 604d2eb15a5ca510ff3fc5ecf912d3c0
</span>        State: online
        Cluster Id: 46b205a4298c625c4bca2206b7a82dd3
        Zone: 1
        Management Hostname: node-4.lab
        Storage Hostname: 10.0.2.104
        Devices:
<span class="hll">            Id:09a25a114c53d7669235b368efd2f8d1   Name:/dev/xvdc           State:online    Size (GiB):499     Used (GiB):0       Free (GiB):499
</span>                Bricks:

<span class="hll">        Node Id: 7736bd0cb6a84540860303a6479cacb2
</span>        State: online
        Cluster Id: 46b205a4298c625c4bca2206b7a82dd3
        Zone: 3
        Management Hostname: node-6.lab
        Storage Hostname: 10.0.4.106
        Devices:
<span class="hll">            Id:cccadb2b54dccd99f698d2ae137a22ff   Name:/dev/xvdc           State:online    Size (GiB):499     Used (GiB):0       Free (GiB):499
</span>            Bricks:
</pre></div>


<p>The hierachical dependencies in this topology works as follows: Clusters &gt; Nodes &gt; Devices.<br />
Assuming there are no volumes present these need to be deleted in reverse order.</p>
<p>Note that specific values highlighted above in your environment and carry out the following steps in strict order:</p>
<p>&#8680; Delete all devices of all nodes:</p>
<div class="codehilite"><pre><span></span>heketi-cli device delete e481d022cea9bfb11e8a86c0dd8d349
heketi-cli device delete 09a25a114c53d7669235b368efd2f8d1
heketi-cli device delete cccadb2b54dccd99f698d2ae137a22ff
</pre></div>


<p>&#8680; Delete all nodes of the cluster in question:</p>
<div class="codehilite"><pre><span></span>heketi-cli node delete 538b860406870288af23af0fbc2cd27f
heketi-cli node delete 604d2eb15a5ca510ff3fc5ecf912d3c0
heketi-cli node delete 7736bd0cb6a84540860303a6479cacb2
</pre></div>


<p>&#8680; Finally delete the cluster:</p>
<div class="codehilite"><pre><span></span>heketi-cli cluster delete 46b205a4298c625c4bca2206b7a82dd3
</pre></div>


<p>&#8680; Confirm the cluster is gone:</p>
<div class="codehilite"><pre><span></span>heketi-cli cluster list
</pre></div>


<p>&#8680; Verify the clusters new topology back to it&rsquo;s state from Module 3.</p>
<p>This deleted all heketi database entries about the cluster. However the GlusterFS pods are still running, since they are controlled directly by OpenShift instead of heketi:</p>
<p>&#8680; List all the pods in the <code>container-native-storage</code> namespace:</p>
<div class="codehilite"><pre><span></span>oc get pods -o wide -n container-native-storage
</pre></div>


<p>The pods formerly running the second GlusterFS pool are still there:</p>
<div class="codehilite"><pre><span></span>NAME              READY     STATUS    RESTARTS   AGE       IP           NODE
glusterfs-1nvtj   1/1       Running   0          1h       10.0.4.106   node-6.lab
glusterfs-5gvw8   1/1       Running   0          1h       10.0.2.104   node-4.lab
glusterfs-5rc2g   1/1       Running   0          4h       10.0.2.101   node-1.lab
glusterfs-b4wg1   1/1       Running   0          1h       10.0.3.105   node-5.lab
glusterfs-jbvdk   1/1       Running   0          4h       10.0.3.102   node-2.lab
glusterfs-rchtr   1/1       Running   0          4h       10.0.4.103   node-3.lab
heketi-1-tn0s9    1/1       Running   0          4h       10.130.2.3   node-6.lab
</pre></div>


<p>They can be stopped by removing the labels OpenShift uses to determine GlusterFS pod placement for CNS.</p>
<p>&#8680; Remove the labels from the last 3 OpenShift nodes like so:</p>
<div class="codehilite"><pre><span></span>oc label node/node-4.lab storagenode-
oc label node/node-5.lab storagenode-
oc label node/node-6.lab storagenode-
</pre></div>


<p>Contrary to the output of these commands the label <code>storagenode</code> is actually removed.</p>
<p>&#8680; Verify that all GlusterFS pods running on <code>node-4</code>, <code>node-5</code> and <code>node-6</code> are indeed terminated:</p>
<div class="codehilite"><pre><span></span>oc get pods -o wide -n container-native-storage
</pre></div>


<div class="admonition note">
<p class="admonition-title">Note</p>
<p>It can take up to 2 minutes for the pods to terminate.</p>
</div>
<p>You should be back down to 3 GlusterFS pods.</p>
<div class="codehilite"><pre><span></span>NAME              READY     STATUS    RESTARTS   AGE       IP           NODE
glusterfs-5rc2g   1/1       Running   0          5h        10.0.2.101   node-1.lab
glusterfs-jbvdk   1/1       Running   0          5h        10.0.3.102   node-2.lab
glusterfs-rchtr   1/1       Running   0          5h        10.0.4.103   node-3.lab
heketi-1-tn0s9    1/1       Running   0          5h        10.130.2.3   node-6.lab
</pre></div>


<hr />
<h2 id="expanding-a-glusterfs-pool">Expanding a GlusterFS pool<a class="headerlink" href="#expanding-a-glusterfs-pool" title="Permanent link">#</a></h2>
<p>Instead of creating additional GlusterFS pools in CNS on OpenShift it is also possible to expand existing pools.</p>
<p>This works similar to creating additional pools, with bulk-import via the topology file. Only this time with nodes added to the existing cluster structure in JSON.</p>
<p>Since manipulating JSON can be error-prone create a new file called <code>expanded-topology.json</code> with contents as below:</p>
<p><kbd>expanded-topology.json:</kbd></p>
<div class="codehilite"><pre><span></span><span class="p">{</span>
    <span class="nt">&quot;clusters&quot;</span><span class="p">:</span> <span class="p">[</span>
        <span class="p">{</span>
            <span class="nt">&quot;nodes&quot;</span><span class="p">:</span> <span class="p">[</span>
                <span class="p">{</span>
                    <span class="nt">&quot;node&quot;</span><span class="p">:</span> <span class="p">{</span>
                        <span class="nt">&quot;hostnames&quot;</span><span class="p">:</span> <span class="p">{</span>
                            <span class="nt">&quot;manage&quot;</span><span class="p">:</span> <span class="p">[</span>
                                <span class="s2">&quot;node-1.lab&quot;</span>
                            <span class="p">],</span>
                            <span class="nt">&quot;storage&quot;</span><span class="p">:</span> <span class="p">[</span>
                                <span class="s2">&quot;10.0.2.101&quot;</span>
                            <span class="p">]</span>
                        <span class="p">},</span>
                        <span class="nt">&quot;zone&quot;</span><span class="p">:</span> <span class="mi">1</span>
                    <span class="p">},</span>
                    <span class="nt">&quot;devices&quot;</span><span class="p">:</span> <span class="p">[</span>
                        <span class="s2">&quot;/dev/xvdc&quot;</span>
                    <span class="p">]</span>
                <span class="p">},</span>
                <span class="p">{</span>
                    <span class="nt">&quot;node&quot;</span><span class="p">:</span> <span class="p">{</span>
                        <span class="nt">&quot;hostnames&quot;</span><span class="p">:</span> <span class="p">{</span>
                            <span class="nt">&quot;manage&quot;</span><span class="p">:</span> <span class="p">[</span>
                                <span class="s2">&quot;node-2.lab&quot;</span>
                            <span class="p">],</span>
                            <span class="nt">&quot;storage&quot;</span><span class="p">:</span> <span class="p">[</span>
                                <span class="s2">&quot;10.0.3.102&quot;</span>
                            <span class="p">]</span>
                        <span class="p">},</span>
                        <span class="nt">&quot;zone&quot;</span><span class="p">:</span> <span class="mi">2</span>
                    <span class="p">},</span>
                    <span class="nt">&quot;devices&quot;</span><span class="p">:</span> <span class="p">[</span>
                        <span class="s2">&quot;/dev/xvdc&quot;</span>
                    <span class="p">]</span>
                <span class="p">},</span>
                <span class="p">{</span>
                    <span class="nt">&quot;node&quot;</span><span class="p">:</span> <span class="p">{</span>
                        <span class="nt">&quot;hostnames&quot;</span><span class="p">:</span> <span class="p">{</span>
                            <span class="nt">&quot;manage&quot;</span><span class="p">:</span> <span class="p">[</span>
                                <span class="s2">&quot;node-3.lab&quot;</span>
                            <span class="p">],</span>
                            <span class="nt">&quot;storage&quot;</span><span class="p">:</span> <span class="p">[</span>
                                <span class="s2">&quot;10.0.4.103&quot;</span>
                            <span class="p">]</span>
                        <span class="p">},</span>
                        <span class="nt">&quot;zone&quot;</span><span class="p">:</span> <span class="mi">3</span>
                    <span class="p">},</span>
                    <span class="nt">&quot;devices&quot;</span><span class="p">:</span> <span class="p">[</span>
                        <span class="s2">&quot;/dev/xvdc&quot;</span>
                    <span class="p">]</span>
                <span class="p">},</span>
                <span class="p">{</span>
                    <span class="nt">&quot;node&quot;</span><span class="p">:</span> <span class="p">{</span>
                        <span class="nt">&quot;hostnames&quot;</span><span class="p">:</span> <span class="p">{</span>
                            <span class="nt">&quot;manage&quot;</span><span class="p">:</span> <span class="p">[</span>
                                <span class="s2">&quot;node-4.lab&quot;</span>
                            <span class="p">],</span>
                            <span class="nt">&quot;storage&quot;</span><span class="p">:</span> <span class="p">[</span>
                                <span class="s2">&quot;10.0.2.104&quot;</span>
                            <span class="p">]</span>
                        <span class="p">},</span>
                        <span class="nt">&quot;zone&quot;</span><span class="p">:</span> <span class="mi">1</span>
                    <span class="p">},</span>
                    <span class="nt">&quot;devices&quot;</span><span class="p">:</span> <span class="p">[</span>
                        <span class="s2">&quot;/dev/xvdc&quot;</span>
                    <span class="p">]</span>
                <span class="p">},</span>
                <span class="p">{</span>
                    <span class="nt">&quot;node&quot;</span><span class="p">:</span> <span class="p">{</span>
                        <span class="nt">&quot;hostnames&quot;</span><span class="p">:</span> <span class="p">{</span>
                            <span class="nt">&quot;manage&quot;</span><span class="p">:</span> <span class="p">[</span>
                                <span class="s2">&quot;node-5.lab&quot;</span>
                            <span class="p">],</span>
                            <span class="nt">&quot;storage&quot;</span><span class="p">:</span> <span class="p">[</span>
                                <span class="s2">&quot;10.0.3.105&quot;</span>
                            <span class="p">]</span>
                        <span class="p">},</span>
                        <span class="nt">&quot;zone&quot;</span><span class="p">:</span> <span class="mi">2</span>
                    <span class="p">},</span>
                    <span class="nt">&quot;devices&quot;</span><span class="p">:</span> <span class="p">[</span>
                        <span class="s2">&quot;/dev/xvdc&quot;</span>
                    <span class="p">]</span>
                <span class="p">},</span>
                <span class="p">{</span>
                    <span class="nt">&quot;node&quot;</span><span class="p">:</span> <span class="p">{</span>
                        <span class="nt">&quot;hostnames&quot;</span><span class="p">:</span> <span class="p">{</span>
                            <span class="nt">&quot;manage&quot;</span><span class="p">:</span> <span class="p">[</span>
                                <span class="s2">&quot;node-6.lab&quot;</span>
                            <span class="p">],</span>
                            <span class="nt">&quot;storage&quot;</span><span class="p">:</span> <span class="p">[</span>
                                <span class="s2">&quot;10.0.4.106&quot;</span>
                            <span class="p">]</span>
                        <span class="p">},</span>
                        <span class="nt">&quot;zone&quot;</span><span class="p">:</span> <span class="mi">3</span>
                    <span class="p">},</span>
                    <span class="nt">&quot;devices&quot;</span><span class="p">:</span> <span class="p">[</span>
                        <span class="s2">&quot;/dev/xvdc&quot;</span>
                    <span class="p">]</span>
                <span class="p">}</span>
            <span class="p">]</span>
        <span class="p">}</span>
    <span class="p">]</span>
<span class="p">}</span>
</pre></div>


<p>&#8680; Again, apply the expected labels to the remaining 3 OpenShift Nodes:</p>
<div class="codehilite"><pre><span></span>oc label node/node-4.lab storagenode=glusterfs
oc label node/node-5.lab storagenode=glusterfs
oc label node/node-6.lab storagenode=glusterfs
</pre></div>


<p>&#8680; Wait for all pods to show <code>1/1</code> in the <code>READY</code> column:</p>
<div class="codehilite"><pre><span></span> oc get pods -o wide -n container-native-storage
</pre></div>


<p>This confirms all GlusterFS pods are ready to receive remote commands:</p>
<div class="codehilite"><pre><span></span>NAME              READY     STATUS    RESTARTS   AGE       IP           NODE
glusterfs-0lr75   1/1       Running   0          4m        10.0.4.106   node-6.lab
glusterfs-1dxz3   1/1       Running   0          4m        10.0.3.105   node-5.lab
glusterfs-5rc2g   1/1       Running   0          5h        10.0.2.101   node-1.lab
glusterfs-8nrn0   1/1       Running   0          4m        10.0.2.104   node-4.lab
glusterfs-jbvdk   1/1       Running   0          5h        10.0.3.102   node-2.lab
glusterfs-rchtr   1/1       Running   0          5h        10.0.4.103   node-3.lab
heketi-1-tn0s9    1/1       Running   0          5h        10.130.2.3   node-6.lab
</pre></div>


<p>&#8680; If you logged out of the session meanwhile re-instate the heketi-cli configuration with the environment variables;</p>
<div class="codehilite"><pre><span></span>export HEKETI_CLI_SERVER=http://heketi-container-native-storage.cloudapps.34.252.58.209.nip.io
export HEKETI_CLI_USER=admin
export HEKETI_CLI_KEY=myS3cr3tpassw0rd
</pre></div>


<p>&#8680; Now load the new topology:</p>
<div class="codehilite"><pre><span></span>heketi-cli topology load --json=expanded-topology.json
</pre></div>


<p>The output indicated that the existing cluster was expanded, rather than creating a new one:</p>
<div class="codehilite"><pre><span></span>Found node node-1.lab on cluster fb67f97166c58f161b85201e1fd9b8ed
    Found device /dev/xvdc
Found node node-2.lab on cluster fb67f97166c58f161b85201e1fd9b8ed
    Found device /dev/xvdc
Found node node-3.lab on cluster fb67f97166c58f161b85201e1fd9b8ed
    Found device /dev/xvdc
Creating node node-4.lab ... ID: 544158e53934a3d351b874b7d915e8d4
    Adding device /dev/xvdc ... OK
Creating node node-5.lab ... ID: 645b6edd4044cb1dd828f728d1c3eb81
    Adding device /dev/xvdc ... OK
Creating node node-6.lab ... ID: 3f39ebf3c8c82531a7ba447135742776
    Adding device /dev/xvdc ... OK
</pre></div>


<p>&#8680; Log on to one of the GlusterFS pods and confirm their new peers:</p>
<div class="codehilite"><pre><span></span>oc rsh glusterfs-0lr75
</pre></div>


<p>&#8680; Run the peer status command inside the container remote session:</p>
<div class="codehilite"><pre><span></span>gluster peer status
</pre></div>


<p>You should now have a GlusterFS consisting of 6 nodes:</p>
<hr />
                
                  
                
              
              
                
              
            </article>
          </div>
        </div>
      </main>
      
        
<footer class="md-footer">
  
    <div class="md-footer-nav">
      <nav class="md-footer-nav__inner md-grid">
        
          <a href="../module-3-using-cns/" title="Module 3 - Using Persistent Storage" class="md-flex md-footer-nav__link md-footer-nav__link--prev" rel="prev">
            <div class="md-flex__cell md-flex__cell--shrink">
              <i class="md-icon md-icon--arrow-back md-footer-nav__button"></i>
            </div>
            <div class="md-flex__cell md-flex__cell--stretch md-footer-nav__title">
              <span class="md-flex__ellipsis">
                <span class="md-footer-nav__direction">
                  Previous
                </span>
                Module 3 - Using Persistent Storage
              </span>
            </div>
          </a>
        
        
          <a href="../module-5-advanced/" title="Module 5 - Advanced Topics" class="md-flex md-footer-nav__link md-footer-nav__link--next" rel="next">
            <div class="md-flex__cell md-flex__cell--stretch md-footer-nav__title">
              <span class="md-flex__ellipsis">
                <span class="md-footer-nav__direction">
                  Next
                </span>
                Module 5 - Advanced Topics
              </span>
            </div>
            <div class="md-flex__cell md-flex__cell--shrink">
              <i class="md-icon md-icon--arrow-forward md-footer-nav__button"></i>
            </div>
          </a>
        
      </nav>
    </div>
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-footer-copyright">
        
        powered by
        <a href="http://www.mkdocs.org" title="MkDocs">MkDocs</a>
        and
        <a href="http://squidfunk.github.io/mkdocs-material/" title="Material for MkDocs">
          Material for MkDocs</a>
      </div>
      
        
      
    </div>
  </div>
</footer>
      
    </div>
    
      <script src="../assets/javascripts/application-b30566c560.js"></script>
      
      
      <script>app.initialize({url:{base:".."}})</script>
      
    
    
      
    
  </body>
</html>