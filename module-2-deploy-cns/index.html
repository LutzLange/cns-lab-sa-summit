
<!DOCTYPE html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
      
      
      
        <link rel="shortcut icon" href="../assets/images/favicon.png">
      
      <meta name="generator" content="mkdocs-0.16.3, mkdocs-material-1.7.3">
    
    
      
        <title>Module 2 - Deploying Container-Native Storage - Container-Native Storage Hands-on Lab - Storage SA Summit 2017</title>
      
    
    
      <script src="../assets/javascripts/modernizr-1df76c4e58.js"></script>
    
    
      <link rel="stylesheet" href="../assets/stylesheets/application-acc56dad49.css">
      
    
    
      
        
        
        
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,400,400i,700|Roboto+Mono">
        <style>body,input{font-family:"Roboto","Helvetica Neue",Helvetica,Arial,sans-serif}code,kbd,pre{font-family:"Roboto Mono","Courier New",Courier,monospace}</style>
      
      <link rel="stylesheet" href="https://fonts.googleapis.com/icon?family=Material+Icons">
    
    
    
  </head>
  
  
  
  
    <body>
  
    <svg class="md-svg">
      <defs>
        
        
      </defs>
    </svg>
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="drawer">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="search">
    <label class="md-overlay" data-md-component="overlay" for="drawer"></label>
    
      <header class="md-header" data-md-component="header">
  <nav class="md-header-nav md-grid">
    <div class="md-flex">
      <div class="md-flex__cell md-flex__cell--shrink">
        
          <a href=".." title="Container-Native Storage Hands-on Lab - Storage SA Summit 2017" class="md-logo md-header-nav__button">
            <img src="../img/RedHat.svg" width="24" height="24">
          </a>
        
      </div>
      <div class="md-flex__cell md-flex__cell--shrink">
        <label class="md-icon md-icon--menu md-header-nav__button" for="drawer"></label>
      </div>
      <div class="md-flex__cell md-flex__cell--stretch">
        <span class="md-flex__ellipsis md-header-nav__title">
          
            
              
                <span class="md-header-nav__parent">
                  Modules
                </span>
              
            
            Module 2 - Deploying Container-Native Storage
          
        </span>
      </div>
      <div class="md-flex__cell md-flex__cell--shrink">
        
          <label class="md-icon md-icon--search md-header-nav__button" for="search"></label>
          
<div class="md-search" data-md-component="search">
  <label class="md-search__overlay" for="search"></label>
  <div class="md-search__inner">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" required placeholder="Search" accesskey="s" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="query">
      <label class="md-icon md-search__icon" for="search"></label>
      <button type="reset" class="md-icon md-search__icon" data-md-component="reset">close</button>
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" data-md-scrollfix>
        <div class="md-search-result" data-md-component="result" data-md-lang-search="">
          <div class="md-search-result__meta" data-md-lang-result-none="No matching documents" data-md-lang-result-one="1 matching document" data-md-lang-result-other="# matching documents">
            Type to start searching
          </div>
          <ol class="md-search-result__list"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
        
      </div>
      
    </div>
  </nav>
</header>
    
    <div class="md-container">
      
      
      <main class="md-main">
        <div class="md-main__inner md-grid" data-md-component="container">
          
            
              <div class="md-sidebar md-sidebar--primary" data-md-component="navigation">
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    <nav class="md-nav md-nav--primary" data-md-level="0">
  <label class="md-nav__title md-nav__title--site" for="drawer">
    
      <i class="md-logo md-nav__button">
        <img src="../img/RedHat.svg">
      </i>
    
    Container-Native Storage Hands-on Lab - Storage SA Summit 2017
  </label>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
      


  <li class="md-nav__item">
    <a href=".." title="Overview" class="md-nav__link">
      Overview
    </a>
  </li>

    
      
      
      

  


  <li class="md-nav__item md-nav__item--active md-nav__item--nested">
    
      <input class="md-toggle md-nav__toggle" data-md-toggle="nav-2" type="checkbox" id="nav-2" checked>
    
    <label class="md-nav__link" for="nav-2">
      Modules
    </label>
    <nav class="md-nav" data-md-component="collapsible" data-md-level="1">
      <label class="md-nav__title" for="nav-2">
        Modules
      </label>
      <ul class="md-nav__list" data-md-scrollfix>
        
        
          
          
          


  <li class="md-nav__item">
    <a href="../module-1-ocp-in-5/" title="Module 1 - OpenShift in 5 Minutes" class="md-nav__link">
      Module 1 - OpenShift in 5 Minutes
    </a>
  </li>

        
          
          
          

  


  <li class="md-nav__item md-nav__item--active">
    
    <input class="md-toggle md-nav__toggle" data-md-toggle="toc" type="checkbox" id="toc">
    
    
      <label class="md-nav__link md-nav__link--active" for="toc">
        Module 2 - Deploying Container-Native Storage
      </label>
    
    <a href="./" title="Module 2 - Deploying Container-Native Storage" class="md-nav__link md-nav__link--active">
      Module 2 - Deploying Container-Native Storage
    </a>
    
      
<nav class="md-nav md-nav--secondary">
  
  
  
    <label class="md-nav__title" for="toc">Table of contents</label>
    <ul class="md-nav__list" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#configure-the-firewall-with-ansible" title="Configure the firewall with Ansible" class="md-nav__link">
    Configure the firewall with Ansible
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#prepare-openshift-for-cns" title="Prepare OpenShift for CNS" class="md-nav__link">
    Prepare OpenShift for CNS
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#build-container-native-storage-topology" title="Build Container-native Storage Topology" class="md-nav__link">
    Build Container-native Storage Topology
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#deploy-container-native-storage" title="Deploy Container-native Storage" class="md-nav__link">
    Deploy Container-native Storage
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#verifying-the-deployment" title="Verifying the deployment" class="md-nav__link">
    Verifying the deployment
  </a>
  
</li>
      
      
      
    </ul>
  
</nav>
    
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../module-3-using-cns/" title="Module 3 - Using Persistent Storage" class="md-nav__link">
      Module 3 - Using Persistent Storage
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../module-1-ocp-in-5/" title="Module 4 - Cluster Operations" class="md-nav__link">
      Module 4 - Cluster Operations
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../module-1-ocp-in-5/" title="Module 5 - Advanced Topics" class="md-nav__link">
      Module 5 - Advanced Topics
    </a>
  </li>

        
      </ul>
    </nav>
  </li>

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              <div class="md-sidebar md-sidebar--secondary" data-md-component="toc">
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    
<nav class="md-nav md-nav--secondary">
  
  
  
    <label class="md-nav__title" for="toc">Table of contents</label>
    <ul class="md-nav__list" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#configure-the-firewall-with-ansible" title="Configure the firewall with Ansible" class="md-nav__link">
    Configure the firewall with Ansible
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#prepare-openshift-for-cns" title="Prepare OpenShift for CNS" class="md-nav__link">
    Prepare OpenShift for CNS
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#build-container-native-storage-topology" title="Build Container-native Storage Topology" class="md-nav__link">
    Build Container-native Storage Topology
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#deploy-container-native-storage" title="Deploy Container-native Storage" class="md-nav__link">
    Deploy Container-native Storage
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#verifying-the-deployment" title="Verifying the deployment" class="md-nav__link">
    Verifying the deployment
  </a>
  
</li>
      
      
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          <div class="md-content">
            <article class="md-content__inner md-typeset">
              
                
                
                  <h1>Module 2 - Deploying Container-Native Storage</h1>
                
                <div class="admonition summary">
<p class="admonition-title">Overview</p>
<p>In this module you will set up container-native storage (CNS) in your OpenShift environment. You will use this to dynamically provision storage to be available to workloads in OpenShift. It is provided by GlusterFS running in containers. GlusterFS in turn is backed by local storage available to the OpenShift nodes.</p>
</div>
<p>&#8680; Make sure you are logged on as the <code>ec2-user</code> to the master node:</p>
<div class="codehilite"><pre><span></span>[ec2-user@master ~]$ hostname -f
master.lab
</pre></div>


<div class="admonition caution">
<p class="admonition-title">Caution</p>
<p>All of the following tasks are carried out as the ec2-user from the master node. For Copy &amp; Paste convenience we will omit the shell prompt unless necessary.</p>
<p>All files created can be stored in root’s home directory unless a particular path is specified.</p>
</div>
<p>&#8680; First ensure the CNS deployment tool is available (it should already be installed)</p>
<div class="codehilite"><pre><span></span>yum list installed cns-deploy
</pre></div>


<h2 id="configure-the-firewall-with-ansible">Configure the firewall with Ansible<a class="headerlink" href="#configure-the-firewall-with-ansible" title="Permanent link">#</a></h2>
<div class="admonition hint">
<p class="admonition-title">Hint</p>
<p>In the following we will use Ansible&rsquo;s configuration management capabilities in order to make sure all the OpenShift nodes have the right firewall settings. This is for your convenience.</p>
</div>
<hr />
<p>&#8680; You should be able to ping all hosts using Ansible:</p>
<div class="codehilite"><pre><span></span>ansible nodes -m ping
</pre></div>


<p>All 6 OpenShift application nodes should respond</p>
<div class="codehilite"><pre><span></span>node-4.lab | SUCCESS =&gt; {
    &quot;changed&quot;: false,
    &quot;ping&quot;: &quot;pong&quot;
}
node-1.lab | SUCCESS =&gt; {
    &quot;changed&quot;: false,
    &quot;ping&quot;: &quot;pong&quot;
}
master.lab | SUCCESS =&gt; {
    &quot;changed&quot;: false,
    &quot;ping&quot;: &quot;pong&quot;
}
node-2.lab | SUCCESS =&gt; {
    &quot;changed&quot;: false,
    &quot;ping&quot;: &quot;pong&quot;
}
node-3.lab | SUCCESS =&gt; {
    &quot;changed&quot;: false,
    &quot;ping&quot;: &quot;pong&quot;
}
node-5.lab | SUCCESS =&gt; {
    &quot;changed&quot;: false,
    &quot;ping&quot;: &quot;pong&quot;
}
node-6.lab | SUCCESS =&gt; {
    &quot;changed&quot;: false,
    &quot;ping&quot;: &quot;pong&quot;
}
</pre></div>


<p>&#8680; Next, create a file called <code>configure-firewall.yml</code> and copy&amp;paste the following contents:</p>
<p><kbd>configure-firewall.yml:</kbd></p>
<div class="codehilite"><pre><span></span><span class="nn">---</span>

<span class="p p-Indicator">-</span> <span class="l l-Scalar l-Scalar-Plain">hosts</span><span class="p p-Indicator">:</span> <span class="l l-Scalar l-Scalar-Plain">nodes</span>

  <span class="l l-Scalar l-Scalar-Plain">tasks</span><span class="p p-Indicator">:</span>

    <span class="p p-Indicator">-</span> <span class="l l-Scalar l-Scalar-Plain">name</span><span class="p p-Indicator">:</span> <span class="l l-Scalar l-Scalar-Plain">insert iptables rules required for GlusterFS</span>
      <span class="l l-Scalar l-Scalar-Plain">blockinfile</span><span class="p p-Indicator">:</span>
        <span class="l l-Scalar l-Scalar-Plain">dest</span><span class="p p-Indicator">:</span> <span class="l l-Scalar l-Scalar-Plain">/etc/sysconfig/iptables</span>
        <span class="l l-Scalar l-Scalar-Plain">block</span><span class="p p-Indicator">:</span> <span class="p p-Indicator">|</span>
          <span class="no">-A OS_FIREWALL_ALLOW -p tcp -m state --state NEW -m tcp --dport 24007 -j ACCEPT</span>
          <span class="no">-A OS_FIREWALL_ALLOW -p tcp -m state --state NEW -m tcp --dport 24008 -j ACCEPT</span>
          <span class="no">-A OS_FIREWALL_ALLOW -p tcp -m state --state NEW -m tcp --dport 2222 -j ACCEPT</span>
          <span class="no">-A OS_FIREWALL_ALLOW -p tcp -m state --state NEW -m multiport --dports 49152:49664 -j ACCEPT</span>
        <span class="l l-Scalar l-Scalar-Plain">insertbefore</span><span class="p p-Indicator">:</span> <span class="s">&quot;^COMMIT&quot;</span>

    <span class="p p-Indicator">-</span> <span class="l l-Scalar l-Scalar-Plain">name</span><span class="p p-Indicator">:</span> <span class="l l-Scalar l-Scalar-Plain">reload iptables</span>
      <span class="l l-Scalar l-Scalar-Plain">systemd</span><span class="p p-Indicator">:</span>
        <span class="l l-Scalar l-Scalar-Plain">name</span><span class="p p-Indicator">:</span> <span class="l l-Scalar l-Scalar-Plain">iptables</span>
        <span class="l l-Scalar l-Scalar-Plain">state</span><span class="p p-Indicator">:</span> <span class="l l-Scalar l-Scalar-Plain">reloaded</span>

<span class="nn">...</span>
</pre></div>


<p>Done. This small playbook will save us some work in configuring the firewall top open required ports for CNS on each individual node.</p>
<p>&#8680; Run it with the following command:</p>
<div class="codehilite"><pre><span></span>ansible-playbook configure-firewall.yml
</pre></div>


<p>Your output should look like this.</p>
<div class="codehilite"><pre><span></span>PLAY [nodes]****************************************************************

TASK [Gathering Facts]******************************************************
ok: [node-4.lab]
ok: [node-2.lab]
ok: [node-1.lab]
ok: [node-3.lab]
ok: [master.lab]
ok: [node-5.lab]
ok: [node-6.lab]

TASK [insert iptables rules required for GlusterFS]*************************
changed: [node-1.lab]
changed: [node-4.lab]
changed: [node-2.lab]
changed: [node-3.lab]
changed: [master.lab]
changed: [node-5.lab]
changed: [node-6.lab]

TASK [reload iptables]******************************************************
changed: [node-4.lab]
changed: [node-1.lab]
changed: [node-2.lab]
changed: [node-3.lab]
changed: [master.lab]
changed: [node-6.lab]
changed: [node-5.lab]

PLAY RECAP*****************************************************************
master.lab                 : ok=3    changed=2    unreachable=0    failed=0
node-1.lab                 : ok=3    changed=2    unreachable=0    failed=0
node-2.lab                 : ok=3    changed=2    unreachable=0    failed=0
node-3.lab                 : ok=3    changed=2    unreachable=0    failed=0
node-4.lab                 : ok=3    changed=2    unreachable=0    failed=0
node-5.lab                 : ok=3    changed=2    unreachable=0    failed=0
node-6.lab                 : ok=3    changed=2    unreachable=0    failed=0
</pre></div>


<p>With this we checked the requirement for additional firewall ports to be opened on the OpenShift app nodes.</p>
<hr />
<h2 id="prepare-openshift-for-cns">Prepare OpenShift for CNS<a class="headerlink" href="#prepare-openshift-for-cns" title="Permanent link">#</a></h2>
<p>Next we will create a namespace (also referred to as a <em>Project</em>) in OpenShift. It will be used to group the GlusterFS pods.</p>
<p>&#8680; For this you need to be logged as the cluster admin user in OpenShift.</p>
<div class="codehilite"><pre><span></span>[ec2-user@master ~]# oc whoami
system:admin
</pre></div>


<p>&#8680; If you are for some reason not a cluster admin, login to this built-in admin account like this:</p>
<div class="codehilite"><pre><span></span>oc login -u system:admin -n default
</pre></div>


<p>&#8680; Create a namespace with the designation <code>container-native-storage</code>:</p>
<div class="codehilite"><pre><span></span>oc new-project container-native-storage
</pre></div>


<p>GlusterFS pods need access to the physical block devices on the host. Hence they need elevated permissions.</p>
<p>&#8680; Enable containers to run in privileged mode:</p>
<div class="codehilite"><pre><span></span>oadm policy add-scc-to-user privileged -z default
</pre></div>


<h2 id="build-container-native-storage-topology">Build Container-native Storage Topology<a class="headerlink" href="#build-container-native-storage-topology" title="Permanent link">#</a></h2>
<p>CNS will virtualize locally attached block storage on the OpenShift App nodes. In order to deploy you will need to supply the installer with information about where to find these nodes and what network and which block devices to use.<br />
This is done using JSON file describing the topology of your OpenShift deployment.</p>
<p>We&rsquo;ll start with the first 3 OpenShift app nodes. For this purpose, create the file topology.json like the example below.</p>
<div class="admonition warning">
<p class="admonition-title">Important</p>
<p>The deployment tool always expects fully-qualified domains names for the <code>manage</code> property and always IP addresses for the <code>storage</code> property for the hostnames.</p>
</div>
<p><kbd>topology.json:</kbd></p>
<div class="codehilite"><pre><span></span><span class="p">{</span>
    <span class="nt">&quot;clusters&quot;</span><span class="p">:</span> <span class="p">[</span>
        <span class="p">{</span>
            <span class="nt">&quot;nodes&quot;</span><span class="p">:</span> <span class="p">[</span>
                <span class="p">{</span>
                    <span class="nt">&quot;node&quot;</span><span class="p">:</span> <span class="p">{</span>
                        <span class="nt">&quot;hostnames&quot;</span><span class="p">:</span> <span class="p">{</span>
                            <span class="nt">&quot;manage&quot;</span><span class="p">:</span> <span class="p">[</span>
                                <span class="s2">&quot;node-1.lab&quot;</span>
                            <span class="p">],</span>
                            <span class="nt">&quot;storage&quot;</span><span class="p">:</span> <span class="p">[</span>
                                <span class="s2">&quot;10.0.2.101&quot;</span>
                            <span class="p">]</span>
                        <span class="p">},</span>
                        <span class="nt">&quot;zone&quot;</span><span class="p">:</span> <span class="mi">1</span>
                    <span class="p">},</span>
                    <span class="nt">&quot;devices&quot;</span><span class="p">:</span> <span class="p">[</span>
                        <span class="s2">&quot;/dev/xvdc&quot;</span>
                    <span class="p">]</span>
                <span class="p">},</span>
                <span class="p">{</span>
                    <span class="nt">&quot;node&quot;</span><span class="p">:</span> <span class="p">{</span>
                        <span class="nt">&quot;hostnames&quot;</span><span class="p">:</span> <span class="p">{</span>
                            <span class="nt">&quot;manage&quot;</span><span class="p">:</span> <span class="p">[</span>
                                <span class="s2">&quot;node-2.lab&quot;</span>
                            <span class="p">],</span>
                            <span class="nt">&quot;storage&quot;</span><span class="p">:</span> <span class="p">[</span>
                                <span class="s2">&quot;10.0.3.102&quot;</span>
                            <span class="p">]</span>
                        <span class="p">},</span>
                        <span class="nt">&quot;zone&quot;</span><span class="p">:</span> <span class="mi">2</span>
                    <span class="p">},</span>
                    <span class="nt">&quot;devices&quot;</span><span class="p">:</span> <span class="p">[</span>
                        <span class="s2">&quot;/dev/xvdc&quot;</span>
                    <span class="p">]</span>
                <span class="p">},</span>
                <span class="p">{</span>
                    <span class="nt">&quot;node&quot;</span><span class="p">:</span> <span class="p">{</span>
                        <span class="nt">&quot;hostnames&quot;</span><span class="p">:</span> <span class="p">{</span>
                            <span class="nt">&quot;manage&quot;</span><span class="p">:</span> <span class="p">[</span>
                                <span class="s2">&quot;node-3.lab&quot;</span>
                            <span class="p">],</span>
                            <span class="nt">&quot;storage&quot;</span><span class="p">:</span> <span class="p">[</span>
                                <span class="s2">&quot;10.0.4.103&quot;</span>
                            <span class="p">]</span>
                        <span class="p">},</span>
                        <span class="nt">&quot;zone&quot;</span><span class="p">:</span> <span class="mi">3</span>
                    <span class="p">},</span>
                    <span class="nt">&quot;devices&quot;</span><span class="p">:</span> <span class="p">[</span>
                        <span class="s2">&quot;/dev/xvdc&quot;</span>
                    <span class="p">]</span>
                <span class="p">}</span>
            <span class="p">]</span>
        <span class="p">}</span>
    <span class="p">]</span>
<span class="p">}</span>
</pre></div>


<div class="admonition tip">
<p class="admonition-title">What is the zone ID for?</p>
<p>Next to the obvious information like fully-qualified hostnames, IP address and device names required for Gluster the topology contains an additional property called <code>zone</code> per node.</p>
<p>A zone identifies a failure domain. In CNS data is always replicated 3 times. Reflecting these by zone IDs as arbitrary but distinct numerical values allows CNS to ensure that two copies are never stored on nodes in the same failure domain.</p>
<p>This information is considered when building new volumes, expanding existing volumes or replacing bricks in degraded volumes.<br />
An example for failure domains are AWS Availability Zones or physical servers sharing the same PDU.</p>
</div>
<hr />
<h2 id="deploy-container-native-storage">Deploy Container-native Storage<a class="headerlink" href="#deploy-container-native-storage" title="Permanent link">#</a></h2>
<p>You are now ready to deploy CNS. Alongside GlusterFS pods the API front-end known as <strong>heketi</strong> is deployed. By default it runs without any authentication layer.<br />
To protect the API from unauthorized access we will define passwords for the <code>admin</code> and <code>user</code> role in heketi like below. We will refer to these later again.</p>
<table>
<thead>
<tr>
<th>Heketi Role</th>
<th>Password</th>
</tr>
</thead>
<tbody>
<tr>
<td>admin</td>
<td>myS3cr3tpassw0rd</td>
</tr>
<tr>
<td>user</td>
<td>mys3rs3cr3tpassw0rd</td>
</tr>
</tbody>
</table>
<p>&#8680; Next start the deployment routine with the following command:</p>
<div class="codehilite"><pre><span></span>cns-deploy -n container-native-storage -g topology.json --admin-key &#39;myS3cr3tpassw0rd&#39; --user-key &#39;mys3rs3cr3tpassw0rd&#39;
</pre></div>


<p>Answer the interactive prompt with <strong>Y</strong>.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The deployment will take several minutes to complete. You may want to monitor the progress in parallel also in the OpenShift UI.</p>
<p>Log in as the <code>operator</code> user to the UI and select the <code>container-native-storage</code> project.</p>
</div>
<p>On the command line the output should look like this:</p>
<div class="codehilite"><pre><span></span>Welcome to the deployment tool for GlusterFS on Kubernetes and OpenShift.

Before getting started, this script has some requirements of the execution
environment and of the container platform that you should verify.

The client machine that will run this script must have:
 * Administrative access to an existing Kubernetes or OpenShift cluster
 * Access to a python interpreter &#39;python&#39;
 * Access to the heketi client &#39;heketi-cli&#39;

Each of the nodes that will host GlusterFS must also have appropriate firewall
rules for the required GlusterFS ports:
 * 2222  - sshd (if running GlusterFS in a pod)
 * 24007 - GlusterFS Daemon
 * 24008 - GlusterFS Management
 * 49152 to 49251 - Each brick for every volume on the host requires its own
   port. For every new brick, one new port will be used starting at 49152. We
   recommend a default range of 49152-49251 on each host, though you can adjust
   this to fit your needs.

In addition, for an OpenShift deployment you must:
 * Have &#39;cluster_admin&#39; role on the administrative account doing the deployment
 * Add the &#39;default&#39; and &#39;router&#39; Service Accounts to the &#39;privileged&#39; SCC
 * Have a router deployed that is configured to allow apps to access services
   running in the cluster

<span class="hll">Do you wish to proceed with deployment? Y
</span>[Y]es, [N]o? [Default: Y]:

Using OpenShift CLI.
NAME                       STATUS    AGE
container-native-storage   Active    28m
Using namespace &quot;container-native-storage&quot;.
Checking that heketi pod is not running ... OK
template &quot;deploy-heketi&quot; created
serviceaccount &quot;heketi-service-account&quot; created
template &quot;heketi&quot; created
template &quot;glusterfs&quot; created
role &quot;edit&quot; added: &quot;system:serviceaccount:container-native-storage:heketi-service-account&quot;
<span class="hll">node &quot;node1.example.com&quot; labeled
</span><span class="hll">node &quot;node2.example.com&quot; labeled
</span><span class="hll">node &quot;node3.example.com&quot; labeled
</span>daemonset &quot;glusterfs&quot; created
<span class="hll">Waiting for GlusterFS pods to start ... OK
</span>service &quot;deploy-heketi&quot; created
route &quot;deploy-heketi&quot; created
deploymentconfig &quot;deploy-heketi&quot; created
Waiting for deploy-heketi pod to start ... OK
Creating cluster ... ID: 307f708621f4e0c9eda962b713272e81
<span class="hll">Creating node node1.example.com ... ID: f60a225a16e8678d5ef69afb4815e417
</span>Adding device /dev/vdc ... OK
<span class="hll">Creating node node2.example.com ... ID: 13b7c17c541069862d7e66d142ab789e
</span>Adding device /dev/vdc ... OK
<span class="hll">Creating node node3.example.com ... ID: 5a6fbe5eb1864e711f8bd9b0cb5946ea
</span>Adding device /dev/vdc ... OK
heketi topology loaded.
Saving heketi-storage.json
secret &quot;heketi-storage-secret&quot; created
endpoints &quot;heketi-storage-endpoints&quot; created
service &quot;heketi-storage-endpoints&quot; created
job &quot;heketi-storage-copy-job&quot; created
deploymentconfig &quot;deploy-heketi&quot; deleted
route &quot;deploy-heketi&quot; deleted
service &quot;deploy-heketi&quot; deleted
job &quot;heketi-storage-copy-job&quot; deleted
pod &quot;deploy-heketi-1-599rc&quot; deleted
secret &quot;heketi-storage-secret&quot; deleted
service &quot;heketi&quot; created
<span class="hll">route &quot;heketi&quot; created
</span>deploymentconfig &quot;heketi&quot; created
<span class="hll">Waiting for heketi pod to start ... OK
</span>heketi is now running.
Ready to create and provide GlusterFS volumes.
</pre></div>


<p>In order of the appearance of the highlighted lines above in a nutshell what happens here is the following:</p>
<ul>
<li>
<p>Enter <strong>Y</strong> and press Enter.</p>
</li>
<li>
<p>OpenShift nodes are labeled. Label is referred to in a DaemonSet.</p>
</li>
<li>
<p>GlusterFS daemonset is started. DaemonSet means: start exactly <strong>one</strong> pod per node.</p>
</li>
<li>
<p>All nodes will be referenced in heketi’s database by a UUID. Node block devices are formatted for mounting by GlusterFS.</p>
</li>
<li>
<p>A public route is created for the heketi pod to expose it&rsquo;s API.</p>
</li>
<li>
<p>heketi is deployed in a pod as well.</p>
</li>
</ul>
<p>During the deployment the UI output will look like this:</p>
<p><a href="../img/openshift_cns_deploy_1.png"><img alt="CNS Deployment" src="../img/openshift_cns_deploy_1.png" /></a></p>
<p><a href="../img/openshift_cns_deploy_2.png"><img alt="CNS Deployment" src="../img/openshift_cns_deploy_2.png" /></a></p>
<hr />
<h2 id="verifying-the-deployment">Verifying the deployment<a class="headerlink" href="#verifying-the-deployment" title="Permanent link">#</a></h2>
<p>You now have deployed CNS. Let’s verify all components are in place.</p>
<p>&#8680; If not already there on the CLI change back to the <code>container-native-storage</code> namespace:</p>
<div class="codehilite"><pre><span></span>oc project container-native-storage
</pre></div>


<p>&#8680; List all running pods:</p>
<div class="codehilite"><pre><span></span>oc get pods -o wide
</pre></div>


<p>You should see all pods up and running. Highlighted containerized gluster daemons on each pods carry the IP of the OpenShift node they are running on.</p>
<div class="codehilite"><pre><span></span>NAME              READY     STATUS    RESTARTS   AGE       IP           NODE
<span class="hll">glusterfs-5rc2g   1/1       Running   0          3m        10.0.2.101   node-1.lab
</span><span class="hll">glusterfs-jbvdk   1/1       Running   0          3m        10.0.3.102   node-2.lab
</span><span class="hll">glusterfs-rchtr   1/1       Running   0          3m        10.0.4.103   node-3.lab
</span>heketi-1-tn0s9    1/1       Running   0          2m        10.130.2.3   node-6.lab
</pre></div>


<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The exact pod names will be different in your environment, since they are auto-generated.</p>
</div>
<p>The GlusterFS pods use the hosts network and disk devices to run the software-defined storage system. Hence they attached to the host’s network. See schematic below for a visualization.</p>
<p><a href="../img/cns_diagram_pod.svg"><img alt="GlusterFS pods in CNS in detail." src="../img/cns_diagram_pod.svg" /></a></p>
<p><em>heketi</em> is a component that will expose an API for GlusterFS to OpenShift. This allows OpenShift to dynamically allocate storage from CNS in a programmatic fashion. See below for a visualization. Note that for simplicity, in our example heketi runs on the OpenShift App nodes, not on the Infra node.</p>
<p><a href="../img/cns_diagram_heketi.svg"><img alt="heketi pod running in CNS" src="../img/cns_diagram_heketi.svg" /></a></p>
<p>To expose heketi’s API a <code>service</code> named <em>heketi</em> has been generated in OpenShift.</p>
<p>&#8680; Check the service with:</p>
<div class="codehilite"><pre><span></span>oc get service/heketi
</pre></div>


<p>The output should look similar to the below:</p>
<div class="codehilite"><pre><span></span>NAME      CLUSTER-IP     EXTERNAL-IP   PORT(S)    AGE
heketi    172.30.5.231   &lt;none&gt;        8080/TCP   31m
</pre></div>


<p>To also use heketi outside of OpenShift in addition to the service a route has been deployed.</p>
<p>&#8680; Display the route with:</p>
<div class="codehilite"><pre><span></span>oc get route/heketi
</pre></div>


<p>The output should look similar to the below:</p>
<div class="codehilite"><pre><span></span>NAME      HOST/PORT                                                        PATH      SERVICES   PORT      TERMINATION   WILDCARD
heketi    heketi-container-native-storage.cloudapps.34.252.58.209.nip.io             heketi     &lt;all&gt;                   None
</pre></div>


<div class="admonition note">
<p class="admonition-title">Note</p>
<p>In your environment the URL will be slightly different. It will contain the public IPv4 address of your deployment, dynamically resolved by the nip.io service.</p>
</div>
<p>Based on this <em>heketi</em> will be available on this URL:<br />
http://<em>heketi-container-native-storage.cloudapps.34.252.58.209.nip.io</em></p>
<p>&#8680; You may verify this with a trivial health check:</p>
<div class="codehilite"><pre><span></span>curl http://heketi-container-native-storage.cloudapps.34.252.58.209.nip.io/hello
</pre></div>


<p>This should say:</p>
<div class="codehilite"><pre><span></span>Hello from Heketi
</pre></div>


<p>It appears heketi is running. To ensure it&rsquo;s functional and has been set up with authentication we are going to query it with the heketi CLI client.<br />
The client needs to know the heketi service URL above and the password for the <code>admin</code> noted in the <a href="#deploy-container-native-storage">deployment step</a>.</p>
<p>&#8680; Configure the heketi client with environment variables.</p>
<div class="codehilite"><pre><span></span>export HEKETI_CLI_SERVER=heketi-container-native-storage.cloudapps.34.252.58.209.nip.io
export HEKETI_CLI_USER=admin
export HEKETI_CLI_KEY=myS3cr3tpassw0rd
</pre></div>


<p>&#8680; You are now able to use the heketi CLI tool:</p>
<div class="codehilite"><pre><span></span>heketi-cli cluster list
</pre></div>


<p>This should list at least one cluster by it&rsquo;s UUID:</p>
<div class="codehilite"><pre><span></span>Clusters:
fb67f97166c58f161b85201e1fd9b8ed
</pre></div>


<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The UUID is auto-generated and will be different for you.</p>
</div>
<p>&#8680; Use the UUID unique to your environment and obtain more information about it:</p>
<div class="codehilite"><pre><span></span>heketi-cli cluster info fb67f97166c58f161b85201e1fd9b8e
</pre></div>


<p>There should be 3 nodes and 1 volume, again displayed with their UUIDs.</p>
<div class="codehilite"><pre><span></span>Cluster id: fb67f97166c58f161b85201e1fd9b8ed
Nodes:
22cbcd136fa40ffe766a13f305cc1e3b
bfc006b571e85a083118054233bfb16d
c5979019ac13b9fe02f4e4e2dc6d62cb
Volumes:
2415fba2b9364a65711da2a8311a663a
</pre></div>


<p>&#8680; To display a comprehensive overview of everything heketi knows about query it&rsquo;s topology:</p>
<div class="codehilite"><pre><span></span>heketi-cli topology info
</pre></div>


<p>You will get lengthy output that shows what nodes and disk devices CNS has used to deploy a containerised GlusterFS cluster.</p>
<div class="codehilite"><pre><span></span>Cluster Id: fb67f97166c58f161b85201e1fd9b8ed

Volumes:

Name: heketidbstorage
Size: 2
Id: 2415fba2b9364a65711da2a8311a663a
Cluster Id: fb67f97166c58f161b85201e1fd9b8ed
Mount: 10.0.2.101:heketidbstorage
Mount Options: backup-volfile-servers=10.0.3.102,10.0.4.103
Durability Type: replicate
Replica: 3
Snapshot: Disabled

Bricks:
  Id: 55851d8ab270112c07ab7a38d55c8045
  Path: /var/lib/heketi/mounts/vg_41b8a921f8e6d31cb04c7dd35b6b4cf2/brick_55851d8ab270112c07ab7a38d55c8045/brick
  Size (GiB): 2
  Node: bfc006b571e85a083118054233bfb16d
  Device: 41b8a921f8e6d31cb04c7dd35b6b4cf2

  Id: 67161e0e607c38677a0ef3f617b8dc1e
  Path: /var/lib/heketi/mounts/vg_8ea71174529a35f41fc0d1b288da6299/brick_67161e0e607c38677a0ef3f617b8dc1e/brick
  Size (GiB): 2
  Node: 22cbcd136fa40ffe766a13f305cc1e3b
  Device: 8ea71174529a35f41fc0d1b288da6299

  Id: a8bf049dcea2d5245b64a792d4b85e6b
  Path: /var/lib/heketi/mounts/vg_2a49883a5cb39c3b845477ff85a729ba/brick_a8bf049dcea2d5245b64a792d4b85e6b/brick
  Size (GiB): 2
  Node: c5979019ac13b9fe02f4e4e2dc6d62cb
  Device: 2a49883a5cb39c3b845477ff85a729ba


Nodes:

Node Id: 22cbcd136fa40ffe766a13f305cc1e3b
State: online
Cluster Id: fb67f97166c58f161b85201e1fd9b8ed
Zone: 2
Management Hostname: node-2.lab
Storage Hostname: 10.0.3.102
Devices:
Id:8ea71174529a35f41fc0d1b288da6299   Name:/dev/xvdc           State:online    Size (GiB):49      Used (GiB):2       Free (GiB):47
  Bricks:
    Id:67161e0e607c38677a0ef3f617b8dc1e   Size (GiB):2       Path: /var/lib/heketi/mounts/vg_8ea71174529a35f41fc0d1b288da6299/brick_67161e0e607c38677a0ef3f617b8dc1e/brick

Node Id: bfc006b571e85a083118054233bfb16d
State: online
Cluster Id: fb67f97166c58f161b85201e1fd9b8ed
Zone: 3
Management Hostname: node-3.lab
Storage Hostname: 10.0.4.103
Devices:
Id:41b8a921f8e6d31cb04c7dd35b6b4cf2   Name:/dev/xvdc           State:online    Size (GiB):49      Used (GiB):2       Free (GiB):47
  Bricks:
    Id:55851d8ab270112c07ab7a38d55c8045   Size (GiB):2       Path: /var/lib/heketi/mounts/vg_41b8a921f8e6d31cb04c7dd35b6b4cf2/brick_55851d8ab270112c07ab7a38d55c8045/brick

Node Id: c5979019ac13b9fe02f4e4e2dc6d62cb
State: online
Cluster Id: fb67f97166c58f161b85201e1fd9b8ed
Zone: 1
Management Hostname: node-1.lab
Storage Hostname: 10.0.2.101
Devices:
Id:2a49883a5cb39c3b845477ff85a729ba   Name:/dev/xvdc           State:online    Size (GiB):49      Used (GiB):2       Free (GiB):47
  Bricks:
    Id:a8bf049dcea2d5245b64a792d4b85e6b   Size (GiB):2       Path: /var/lib/heketi/mounts/vg_2a49883a5cb39c3b845477ff85a729ba/brick_a8bf049dcea2d5245b64a792d4b85e6b/brick
</pre></div>


<p>This information should correspond to the topology.json file you supplied to the installer. With this we successfully verified the CNS deployment.</p>
                
                  
                
              
              
                
              
            </article>
          </div>
        </div>
      </main>
      
        
<footer class="md-footer">
  
    <div class="md-footer-nav">
      <nav class="md-footer-nav__inner md-grid">
        
          <a href="../module-1-ocp-in-5/" title="Module 1 - OpenShift in 5 Minutes" class="md-flex md-footer-nav__link md-footer-nav__link--prev" rel="prev">
            <div class="md-flex__cell md-flex__cell--shrink">
              <i class="md-icon md-icon--arrow-back md-footer-nav__button"></i>
            </div>
            <div class="md-flex__cell md-flex__cell--stretch md-footer-nav__title">
              <span class="md-flex__ellipsis">
                <span class="md-footer-nav__direction">
                  Previous
                </span>
                Module 1 - OpenShift in 5 Minutes
              </span>
            </div>
          </a>
        
        
          <a href="../module-3-using-cns/" title="Module 3 - Using Persistent Storage" class="md-flex md-footer-nav__link md-footer-nav__link--next" rel="next">
            <div class="md-flex__cell md-flex__cell--stretch md-footer-nav__title">
              <span class="md-flex__ellipsis">
                <span class="md-footer-nav__direction">
                  Next
                </span>
                Module 3 - Using Persistent Storage
              </span>
            </div>
            <div class="md-flex__cell md-flex__cell--shrink">
              <i class="md-icon md-icon--arrow-forward md-footer-nav__button"></i>
            </div>
          </a>
        
      </nav>
    </div>
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-footer-copyright">
        
        powered by
        <a href="http://www.mkdocs.org" title="MkDocs">MkDocs</a>
        and
        <a href="http://squidfunk.github.io/mkdocs-material/" title="Material for MkDocs">
          Material for MkDocs</a>
      </div>
      
        
      
    </div>
  </div>
</footer>
      
    </div>
    
      <script src="../assets/javascripts/application-b30566c560.js"></script>
      
      
      <script>app.initialize({url:{base:".."}})</script>
      
    
    
      
    
  </body>
</html>