{
    "docs": [
        {
            "location": "/", 
            "text": "Welcome to Container-Native Storage Hands-on Lab\n#\n\n\nFor full documentation visit \nmkdocs.org\n.\n\n\nCommands\n#\n\n\n\n\nmkdocs new [dir-name]\n - Create a new project.\n\n\nmkdocs serve\n - Start the live-reloading docs server.\n\n\nmkdocs build\n - Build the documentation site.\n\n\nmkdocs help\n - Print this help message.\n\n\n\n\nCode\n#\n\n\n# This line is emphasized\n\n\n# This line isn\nt\n\n\n# This line is emphasized\n\n\n\n\n\n\n Bubble sort \n\n\ndef\n \nbubble_sort\n(\nitems\n):\n\n    \nfor\n \ni\n \nin\n \nrange\n(\nlen\n(\nitems\n)):\n\n        \nfor\n \nj\n \nin\n \nrange\n(\nlen\n(\nitems\n)\n \n-\n \n1\n \n-\n \ni\n):\n\n            \nif\n \nitems\n[\nj\n]\n \n \nitems\n[\nj\n \n+\n \n1\n]:\n\n                \nitems\n[\nj\n],\n \nitems\n[\nj\n \n+\n \n1\n]\n \n=\n \nitems\n[\nj\n \n+\n \n1\n],\n \nitems\n[\nj\n]\n\n\n\n\n\n\nProject layout\n#\n\n\nmkdocs.yml    # The configuration file.\ndocs/\n    index.md  # The documentation homepage.\n    ...       # Other markdown pages, images and other files.", 
            "title": "Overview"
        }, 
        {
            "location": "/#welcome-to-container-native-storage-hands-on-lab", 
            "text": "For full documentation visit  mkdocs.org .", 
            "title": "Welcome to Container-Native Storage Hands-on Lab"
        }, 
        {
            "location": "/#commands", 
            "text": "mkdocs new [dir-name]  - Create a new project.  mkdocs serve  - Start the live-reloading docs server.  mkdocs build  - Build the documentation site.  mkdocs help  - Print this help message.", 
            "title": "Commands"
        }, 
        {
            "location": "/#code", 
            "text": "# This line is emphasized  # This line isn t  # This line is emphasized    Bubble sort   def   bubble_sort ( items ): \n     for   i   in   range ( len ( items )): \n         for   j   in   range ( len ( items )   -   1   -   i ): \n             if   items [ j ]     items [ j   +   1 ]: \n                 items [ j ],   items [ j   +   1 ]   =   items [ j   +   1 ],   items [ j ]", 
            "title": "Code"
        }, 
        {
            "location": "/#project-layout", 
            "text": "mkdocs.yml    # The configuration file.\ndocs/\n    index.md  # The documentation homepage.\n    ...       # Other markdown pages, images and other files.", 
            "title": "Project layout"
        }, 
        {
            "location": "/module-1-ocp-in-5/", 
            "text": "Overview\n\n\nThis module is a \ncrash course\n in OpenShift intended for Storage Solution Architects. You can safely skip this if you have existing working knowledge in OpenShift Container Platform\n\n\n\n\nOpenShift in a nutshell\n#\n\n\nOpenShift Container Platform\n (OCP) is Red Hat\ns productization of the Kubernetes project. 80% of OpenShift is based on it.\n\nKubernetes is a container orchestration engine for running docker-formatted containers at scale in a distributed environment.\n\n\nOpenShifts main purpose is to allow a developer to focus on writing applications. They can leave it to OpenShift how to schedule and run those.\n\n\nIn the simplest form, a user supplies a URL to a source code repository. OCP takes it from there. It will know how to build the application, how to package it in a container image, run one or more instances of that image and make the application accessible to the outside world.\n\n\n\n\nApplication Lifecycle with OpenShift\n#\n\n\nLet\ns walk through the basic workflow at an example.\n\n\n\n\nTip\n\n\nFor each task we will provide instructions for the UI as well as the equivalent on the CLI.\n\n\n\n\nLogging in\n#\n\n\n Log on to the OpenShift User Interface with the URL provided in the \nOverview section\n section as user \ndeveloper\n with password \nr3dh4t\n:\n\n\n\n\nclick on the screenshot for better resolution\n\n\nThe UI focusses mainly on the tasks a developer / user would carry out. Administrative tasks are CLI only.\n\n\n\n\nNote\n\n\nThe \noc\n client is the primary CLI tool to access and manipulate entities in an OpenShift deployment.\n\nBasic syntax is always \n$ oc \ncommand\n\n\n\n\n On the CLI you use the pre-installed \noc\n tool (OpenShift Client). Log on to the master node with it\ns public IP provided in the \nOverview section\n section:\n\n\nssh -i ~/qwiklabs.pem -l ec2-user \nyour-public-IP\n\n\n\n\n\n\noc login -u developer\n\n\n\n\n\nLog on with the password \nr3dh4t\n:\n\n\nAuthentication required for https://master.lab:8443 (openshift)\nUsername: admin\nPassword:\nLogin successful.\n\nYou don\nt have any projects. You can try to create a new project, by running\n\n    oc new-project \nprojectname\n\n\n\n\n\n\n\n\nExploring the environment\n#\n\n\n\n\nNote\n\n\nThe operations in this step can only be done on the command line and require cluster admin privileges in OpenShift. By default these are not available to anyone else than the built-in system admin.\n\n\n\n\n Log on with the built-in system admin (no password required)\n\n\noc login -u system:admin\n\n\n\n\n\n Add cluster admin privileges to the \noperator\n user in order to have an administrative user capable of logging into externally:\n\n\noadm policy add-cluster-role-to-user cluster-admin operator\n\n\n\n\n\n Now log out\n\n\noc logout\n\n\n\n\n\n \nand back in as \noperator\n to the \ndefault\n namespace\n\n\noc login -u operator -n default\n\n\n\n\n\n Display all available nodes in the system\n\n\noc get nodes\n\n\n\n\n\nYou should see 7 nodes in \nREADY\n state:\n\n\nNAME         STATUS    AGE\nmaster.lab   Ready     1h\nnode-1.lab   Ready     1h\nnode-2.lab   Ready     1h\nnode-3.lab   Ready     1h\nnode-4.lab   Ready     1h\nnode-5.lab   Ready     1h\nnode-6.lab   Ready     1h\n\n\n\n\n\n A slight variant of that command will show us some tags (called \nlabels\n):\n\n\noc get nodes --show-labels\n\n\n\n\n\nYou should see that 1 node has the label \nregion=infra\n applied whereas the other 6 have \nregion=apps\n and additionally \nstoragenode=glusterfs\n set:\n\n\nNAME         STATUS    AGE       LABELS\nmaster.lab   Ready     1h        beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,kubernetes.io/hostname=master.lab,region=infra\nnode-1.lab   Ready     1h        beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,kubernetes.io/hostname=node-1.lab,region=apps,storagenode=glusterfs\nnode-2.lab   Ready     1h        beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,kubernetes.io/hostname=node-2.lab,region=apps,storagenode=glusterfs\nnode-3.lab   Ready     1h        beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,kubernetes.io/hostname=node-3.lab,region=apps,storagenode=glusterfs\nnode-4.lab   Ready     1h        beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,kubernetes.io/hostname=node-4.lab,region=apps,storagenode=glusterfs\nnode-5.lab   Ready     1h        beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,kubernetes.io/hostname=node-5.lab,region=apps,storagenode=glusterfs\nnode-6.lab   Ready     1h        beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,kubernetes.io/hostname=node-6.lab,region=apps,storagenode=glusterfs\n\n\n\n\n\nThese are the \nphysical\n systems available to OpenShift. One of them is special: the master. It runs important system services like schedulers, container registries and a database. Labels on hosts are used as a selector for scheduling decisions.\n\nNon-master nodes run containerized applications.\n\n\n Display the current project context or namespace:\n\n\noc project\n\n\n\n\n\nYou should be in the default namespace:\n\n\nUsing project \ndefault\n on server \nhttps://master.lab:8443\n.\n\n\n\n\n\n Display the running containers in this namespace.\n\n\noc get pods\n\n\n\n\n\n\n\nNote\n\n\nContainers are not the smallest entity OpenShift/Kubernetes knows about. \nPods\n are. Pods typically run a single container only and are subject to scheduling, networking and storage configuration.\n\nThere are some corner cases in which multiple containers run in a single pod. Then they share a single IP address, all storage and are always scheduled together.\n\n\n\n\nYou should have the 3 default services running in pods that every OpenShift deployment has:\n\n\nNAME                       READY     STATUS    RESTARTS   AGE\ndocker-registry-1-rktxs    1/1       Running   0          1h\nregistry-console-1-28hlh   1/1       Running   0          1h\nrouter-1-0btlh             1/1       Running   0          1h\n\n\n\n\n\nThe registry is where images that get created in OpenShift are stored. The registry console is a Web UI for the container registry. The router is an HAproxy instance running internally in OpenShift, exposing applications to the external network.\n\n\n Finally get more information about the deployment with this command:\n\n\noc status\n\n\n\n\n\nYou see output similar to this:\n\n\nIn project default on server https://master.lab:8443\n\nhttps://docker-registry-default.cloudapps.34.198.122.20.nip.io (passthrough) (svc/docker-registry)\ndc/docker-registry deploys docker.io/openshift3/ose-docker-registry:v3.5.5.15\ndeployment #1 deployed 2 hours ago - 1 pod\n\nsvc/kubernetes - 172.30.0.1 ports 443, 53-\n8053, 53-\n8053\n\nhttps://registry-console-default.cloudapps.34.198.122.20.nip.io (passthrough) (svc/registry-console)\ndc/registry-console deploys registry.access.redhat.com/openshift3/registry-console:3.5\ndeployment #1 deployed 2 hours ago - 1 pod\n\nsvc/router - 172.30.63.192 ports 80, 443, 1936\ndc/router deploys docker.io/openshift3/ose-haproxy-router:v3.5.5.15\ndeployment #1 deployed 2 hours ago - 1 pod\n\nView details with \noc describe \nresource\n/\nname\n or list everything with \noc get all\n.\n\n\n\n\n\nIf these 3 pods are in place your environment is working.\n\n\n Now log out from the operator session:\n\n\noc logout\n\n\n\n\n\n\n\nCreating a Project\n#\n\n\nSimilar to OpenStack \nProjects\n exists in OpenShift. They group users, objects and quotas in the system. They are also called \nnamespaces\n. Nothing can exist outside a project/namespace.\n\n\n Select \nNew Project\n in the UI. Enter at least a system name, optionally a human-readable name (label) and a description.\n\n\n\n\n\n\n To do this on the CLI log back in as the \ndeveloper\n user with the password \nr3dh4t\n:\n\n\noc login -u developer\n\n\n\n\n\n Create a new project with the \noc\n client:\n\n\noc new-project my-first-app \\\n  --display-name=\nMy First Application\n \\\n  --description=\nMy first application on OpenShift Container Platform\n\n\n\n\n\n\n\n\nCreating an Application\n#\n\n\nIn the UI you should be looking at the following overview page:\n\n\n\n\nIf not, click on the OpenShift Product Logo in the upper left hand corner, select your project again and select \nAdd to Project\n.\n\n\n From the catalog select the category \nRuby\n from the group of available runtime environments.\n\n\n\n\nOpenShift ships a number of useful example of simple and more complex application stacks in the form of templates. These are a great way to see how OpenShift supports application development and deployment.\n\n\n Select the example app template labeled \nRails + PostgreSQL (Ephemeral)\n\n\n\n\nThe template specifies various OpenShift resources, some of which are customizable via parameters. This page allows to override some.\n\n\n\n\nWe will accept the defaults. Note the GitHub repository URL - this is where the code for this app will come from.\n\n\n Click \nCreate\n to start the app deployment:\n\n\n\n\n Click \nContinue to Overview\n to follow the deployment process\n\n\n\n\nAfter a few minutes the deployment is done.\n\n\n\n\n Click on the link provided starting with \nhttp://rails-postgresql-example-my-first-app.cloudapps\n in the upper right hand corner to get to the application instances.\n\n\nOn the CLI the process would have been kicked off with this command chain:\n\n\noc export template/rails-postgresql-example -n openshift | oc process -f - | oc create -f -\n\n\n\n\n\nDon\nt worry about the exact mechanics of this method - it will be explained in a later module.\n\n\nAt this point you deployed an application stack consisting of a database pod and a pod running the ruby on rails app.\n\nThat\ns it. The application has been built from source and deployed from scratch.\n\n\nThere is only one problem. The database stores it\ns data internally in the pod\ns ephemeral local filesystem.\n\n\n\n\nNote\n\n\nAt this point 5 minutes are probably over. For the curious reader there is an extra section below that explains what has happened in the background.\n\n\nIt\ns not crucial for this lab - so feel free to skip it.\n\n\n\n\n\n\nBonus: Exploring the application infrastructure\n#\n\n\nWhat has happened behind the scenes in the previous step in a nutshell:\n\n\n\n\n\n\nA Git repository with the applications source code was provided to OpenShift\n\n\n\n\n\n\nOpenShift was instructed to build a Ruby application from the source\n\n\n\n\n\n\nThe Ruby application was packaged into a container image alongside the Ruby on Rails runtime\n\n\n\n\n\n\nA pointer to an existing container image containing a PostgresSQL database installation was provided\n\n\n\n\n\n\nAn instance of postgres container image was started, with configuration supplied via environment variables\n\n\n\n\n\n\nAn instance of the ruby app container image was started with connection data to the database container supplied via environment variables\n\n\n\n\n\n\nAn internal forwarding mechanism was put in place based on a custom subdomain for the app below *.cloudapps.\nyour-domain-name>.com\n\n\n\n\n\n\nLet\ns look at the objects and resources in OpenShif that where created during this process.\n\n\n In the OpenShift UI, click the link labeled \npostgresql\n in the right hand box\n\n\n\n\nWhat you will see is known as \nDeploymentConfig\n in OpenShift. It is an instruction how to run instances of a certain container image, i.e. how many instances, how much resources per container, whether to apply auto-scaling and what to do when the deployment config get\ns changed.\n\n\nClick on the \nConfiguration\n tab in the \nDeploymentConfig\n to look at this.\n\n\n\n\nWhen the config changes (e.g. increase number of container from 1 to 2) a new deployment will be triggered. The container(s) will be recreated from scratch. This follows the principle of \nimmutable infrastructre\n.\n\n\n Click on the \nHistory\n tab to see a list of all deployments of this config so far (should be only 1).\n\n\n\n\nDeployments are versioned with simple integers.\n\n\n Click on version \n#1\n which should be the latest version.\n\n\n\n\nThis deployment contains exactly one container instance of the postgres image. A container is handled in OpenShift/Kubernetes within the concept of a \nPod\n.\n\nA pod is a logical entity to abstract the fact that there could be multiple container instances scheduled as a single entity (with a single IP, single set of environment variables, storage etc.). In 90% of the case 1 pod = 1 container.\n\n\n Scroll down to see the pods associated with this deployment. Click it.\n\n\n\n\nThis will show you the environment in which the pod runs. On which node, under what IP address etc. Explore the various tabs shown here. They will tell you the environment variables available to this pod, it\ns output so far and various events OpenShift recorded during it\ns lifetime.\n\n\n On the CLI you can look at the DeploymentConfig like this:\n\n\noc get deploymentconfig\n\n\n\n\n\nYou\nll see there are actually two DeploymentConfig objects, the other one is for the ruby app container.\n\n\nNAME                       REVISION   DESIRED   CURRENT   TRIGGERED BY\npostgresql                 1          1         1         config,image(postgresql:9.5)\nrails-postgresql-example   1          1         1         config,image(rails-postgresql-example:latest)\n\n\n\n\n\n You can get more information on a specific object like this:\n\n\noc describe dc/postgresql\n\n\n\n\n\nThe output will more or less show the same as the UI but in plain text.\n\n\n In the UI go back to the \nOverview\n. Click on the link labeled \nrails-postgresql-example\n above the \nDeploymentConfig\n of the same name.\n\n\n\n\nWhat you see is a \nService\n. Think of it as a cluster virtual-IP for a set of containers. The \nService\n IP will be constant while pod IPs can change with pods getting rescheduled, new instances spawned etc. The \nService\n will also forward network traffic on a certain port to a certain port on the pods.\n\nIt will do this in a round-robin fashion.\n\n\n\n\nHowever as you can tell even the \nService\n IP is still internal to OpenShift. This is where \nRoutes\n come in.\n\n\n From the sidebar select \nApplications\n \n \nRoutes\n\n\n\n\n Click the entry labelled \nrails-postgresql-example\n\n\n\n\nA \nRoute\n in OpenShift is a way of exposing a \nService\n to the public. While a \nService\n is what pods in OpenShift internally would use to talk to other pods, a \nRoute\n is used by external clients like humans.", 
            "title": "Module 1 - OpenShift in 5 Minutes"
        }, 
        {
            "location": "/module-1-ocp-in-5/#openshift-in-a-nutshell", 
            "text": "OpenShift Container Platform  (OCP) is Red Hat s productization of the Kubernetes project. 80% of OpenShift is based on it. \nKubernetes is a container orchestration engine for running docker-formatted containers at scale in a distributed environment.  OpenShifts main purpose is to allow a developer to focus on writing applications. They can leave it to OpenShift how to schedule and run those.  In the simplest form, a user supplies a URL to a source code repository. OCP takes it from there. It will know how to build the application, how to package it in a container image, run one or more instances of that image and make the application accessible to the outside world.", 
            "title": "OpenShift in a nutshell"
        }, 
        {
            "location": "/module-1-ocp-in-5/#application-lifecycle-with-openshift", 
            "text": "Let s walk through the basic workflow at an example.   Tip  For each task we will provide instructions for the UI as well as the equivalent on the CLI.", 
            "title": "Application Lifecycle with OpenShift"
        }, 
        {
            "location": "/module-1-ocp-in-5/#logging-in", 
            "text": "Log on to the OpenShift User Interface with the URL provided in the  Overview section  section as user  developer  with password  r3dh4t :   click on the screenshot for better resolution  The UI focusses mainly on the tasks a developer / user would carry out. Administrative tasks are CLI only.   Note  The  oc  client is the primary CLI tool to access and manipulate entities in an OpenShift deployment. \nBasic syntax is always  $ oc  command    On the CLI you use the pre-installed  oc  tool (OpenShift Client). Log on to the master node with it s public IP provided in the  Overview section  section:  ssh -i ~/qwiklabs.pem -l ec2-user  your-public-IP   oc login -u developer  Log on with the password  r3dh4t :  Authentication required for https://master.lab:8443 (openshift)\nUsername: admin\nPassword:\nLogin successful.\n\nYou don t have any projects. You can try to create a new project, by running\n\n    oc new-project  projectname", 
            "title": "Logging in"
        }, 
        {
            "location": "/module-1-ocp-in-5/#exploring-the-environment", 
            "text": "Note  The operations in this step can only be done on the command line and require cluster admin privileges in OpenShift. By default these are not available to anyone else than the built-in system admin.    Log on with the built-in system admin (no password required)  oc login -u system:admin   Add cluster admin privileges to the  operator  user in order to have an administrative user capable of logging into externally:  oadm policy add-cluster-role-to-user cluster-admin operator   Now log out  oc logout    and back in as  operator  to the  default  namespace  oc login -u operator -n default   Display all available nodes in the system  oc get nodes  You should see 7 nodes in  READY  state:  NAME         STATUS    AGE\nmaster.lab   Ready     1h\nnode-1.lab   Ready     1h\nnode-2.lab   Ready     1h\nnode-3.lab   Ready     1h\nnode-4.lab   Ready     1h\nnode-5.lab   Ready     1h\nnode-6.lab   Ready     1h   A slight variant of that command will show us some tags (called  labels ):  oc get nodes --show-labels  You should see that 1 node has the label  region=infra  applied whereas the other 6 have  region=apps  and additionally  storagenode=glusterfs  set:  NAME         STATUS    AGE       LABELS\nmaster.lab   Ready     1h        beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,kubernetes.io/hostname=master.lab,region=infra\nnode-1.lab   Ready     1h        beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,kubernetes.io/hostname=node-1.lab,region=apps,storagenode=glusterfs\nnode-2.lab   Ready     1h        beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,kubernetes.io/hostname=node-2.lab,region=apps,storagenode=glusterfs\nnode-3.lab   Ready     1h        beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,kubernetes.io/hostname=node-3.lab,region=apps,storagenode=glusterfs\nnode-4.lab   Ready     1h        beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,kubernetes.io/hostname=node-4.lab,region=apps,storagenode=glusterfs\nnode-5.lab   Ready     1h        beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,kubernetes.io/hostname=node-5.lab,region=apps,storagenode=glusterfs\nnode-6.lab   Ready     1h        beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,kubernetes.io/hostname=node-6.lab,region=apps,storagenode=glusterfs  These are the  physical  systems available to OpenShift. One of them is special: the master. It runs important system services like schedulers, container registries and a database. Labels on hosts are used as a selector for scheduling decisions. \nNon-master nodes run containerized applications.   Display the current project context or namespace:  oc project  You should be in the default namespace:  Using project  default  on server  https://master.lab:8443 .   Display the running containers in this namespace.  oc get pods   Note  Containers are not the smallest entity OpenShift/Kubernetes knows about.  Pods  are. Pods typically run a single container only and are subject to scheduling, networking and storage configuration. \nThere are some corner cases in which multiple containers run in a single pod. Then they share a single IP address, all storage and are always scheduled together.   You should have the 3 default services running in pods that every OpenShift deployment has:  NAME                       READY     STATUS    RESTARTS   AGE\ndocker-registry-1-rktxs    1/1       Running   0          1h\nregistry-console-1-28hlh   1/1       Running   0          1h\nrouter-1-0btlh             1/1       Running   0          1h  The registry is where images that get created in OpenShift are stored. The registry console is a Web UI for the container registry. The router is an HAproxy instance running internally in OpenShift, exposing applications to the external network.   Finally get more information about the deployment with this command:  oc status  You see output similar to this:  In project default on server https://master.lab:8443\n\nhttps://docker-registry-default.cloudapps.34.198.122.20.nip.io (passthrough) (svc/docker-registry)\ndc/docker-registry deploys docker.io/openshift3/ose-docker-registry:v3.5.5.15\ndeployment #1 deployed 2 hours ago - 1 pod\n\nsvc/kubernetes - 172.30.0.1 ports 443, 53- 8053, 53- 8053\n\nhttps://registry-console-default.cloudapps.34.198.122.20.nip.io (passthrough) (svc/registry-console)\ndc/registry-console deploys registry.access.redhat.com/openshift3/registry-console:3.5\ndeployment #1 deployed 2 hours ago - 1 pod\n\nsvc/router - 172.30.63.192 ports 80, 443, 1936\ndc/router deploys docker.io/openshift3/ose-haproxy-router:v3.5.5.15\ndeployment #1 deployed 2 hours ago - 1 pod\n\nView details with  oc describe  resource / name  or list everything with  oc get all .  If these 3 pods are in place your environment is working.   Now log out from the operator session:  oc logout", 
            "title": "Exploring the environment"
        }, 
        {
            "location": "/module-1-ocp-in-5/#creating-a-project", 
            "text": "Similar to OpenStack  Projects  exists in OpenShift. They group users, objects and quotas in the system. They are also called  namespaces . Nothing can exist outside a project/namespace.   Select  New Project  in the UI. Enter at least a system name, optionally a human-readable name (label) and a description.     To do this on the CLI log back in as the  developer  user with the password  r3dh4t :  oc login -u developer   Create a new project with the  oc  client:  oc new-project my-first-app \\\n  --display-name= My First Application  \\\n  --description= My first application on OpenShift Container Platform", 
            "title": "Creating a Project"
        }, 
        {
            "location": "/module-1-ocp-in-5/#creating-an-application", 
            "text": "In the UI you should be looking at the following overview page:   If not, click on the OpenShift Product Logo in the upper left hand corner, select your project again and select  Add to Project .   From the catalog select the category  Ruby  from the group of available runtime environments.   OpenShift ships a number of useful example of simple and more complex application stacks in the form of templates. These are a great way to see how OpenShift supports application development and deployment.   Select the example app template labeled  Rails + PostgreSQL (Ephemeral)   The template specifies various OpenShift resources, some of which are customizable via parameters. This page allows to override some.   We will accept the defaults. Note the GitHub repository URL - this is where the code for this app will come from.   Click  Create  to start the app deployment:    Click  Continue to Overview  to follow the deployment process   After a few minutes the deployment is done.    Click on the link provided starting with  http://rails-postgresql-example-my-first-app.cloudapps  in the upper right hand corner to get to the application instances.  On the CLI the process would have been kicked off with this command chain:  oc export template/rails-postgresql-example -n openshift | oc process -f - | oc create -f -  Don t worry about the exact mechanics of this method - it will be explained in a later module.  At this point you deployed an application stack consisting of a database pod and a pod running the ruby on rails app. \nThat s it. The application has been built from source and deployed from scratch.  There is only one problem. The database stores it s data internally in the pod s ephemeral local filesystem.   Note  At this point 5 minutes are probably over. For the curious reader there is an extra section below that explains what has happened in the background.  It s not crucial for this lab - so feel free to skip it.", 
            "title": "Creating an Application"
        }, 
        {
            "location": "/module-1-ocp-in-5/#bonus-exploring-the-application-infrastructure", 
            "text": "What has happened behind the scenes in the previous step in a nutshell:    A Git repository with the applications source code was provided to OpenShift    OpenShift was instructed to build a Ruby application from the source    The Ruby application was packaged into a container image alongside the Ruby on Rails runtime    A pointer to an existing container image containing a PostgresSQL database installation was provided    An instance of postgres container image was started, with configuration supplied via environment variables    An instance of the ruby app container image was started with connection data to the database container supplied via environment variables    An internal forwarding mechanism was put in place based on a custom subdomain for the app below *.cloudapps. your-domain-name>.com    Let s look at the objects and resources in OpenShif that where created during this process.   In the OpenShift UI, click the link labeled  postgresql  in the right hand box   What you will see is known as  DeploymentConfig  in OpenShift. It is an instruction how to run instances of a certain container image, i.e. how many instances, how much resources per container, whether to apply auto-scaling and what to do when the deployment config get s changed.  Click on the  Configuration  tab in the  DeploymentConfig  to look at this.   When the config changes (e.g. increase number of container from 1 to 2) a new deployment will be triggered. The container(s) will be recreated from scratch. This follows the principle of  immutable infrastructre .   Click on the  History  tab to see a list of all deployments of this config so far (should be only 1).   Deployments are versioned with simple integers.   Click on version  #1  which should be the latest version.   This deployment contains exactly one container instance of the postgres image. A container is handled in OpenShift/Kubernetes within the concept of a  Pod . \nA pod is a logical entity to abstract the fact that there could be multiple container instances scheduled as a single entity (with a single IP, single set of environment variables, storage etc.). In 90% of the case 1 pod = 1 container.   Scroll down to see the pods associated with this deployment. Click it.   This will show you the environment in which the pod runs. On which node, under what IP address etc. Explore the various tabs shown here. They will tell you the environment variables available to this pod, it s output so far and various events OpenShift recorded during it s lifetime.   On the CLI you can look at the DeploymentConfig like this:  oc get deploymentconfig  You ll see there are actually two DeploymentConfig objects, the other one is for the ruby app container.  NAME                       REVISION   DESIRED   CURRENT   TRIGGERED BY\npostgresql                 1          1         1         config,image(postgresql:9.5)\nrails-postgresql-example   1          1         1         config,image(rails-postgresql-example:latest)   You can get more information on a specific object like this:  oc describe dc/postgresql  The output will more or less show the same as the UI but in plain text.   In the UI go back to the  Overview . Click on the link labeled  rails-postgresql-example  above the  DeploymentConfig  of the same name.   What you see is a  Service . Think of it as a cluster virtual-IP for a set of containers. The  Service  IP will be constant while pod IPs can change with pods getting rescheduled, new instances spawned etc. The  Service  will also forward network traffic on a certain port to a certain port on the pods. \nIt will do this in a round-robin fashion.   However as you can tell even the  Service  IP is still internal to OpenShift. This is where  Routes  come in.   From the sidebar select  Applications     Routes    Click the entry labelled  rails-postgresql-example   A  Route  in OpenShift is a way of exposing a  Service  to the public. While a  Service  is what pods in OpenShift internally would use to talk to other pods, a  Route  is used by external clients like humans.", 
            "title": "Bonus: Exploring the application infrastructure"
        }, 
        {
            "location": "/module-2-deploy-cns/", 
            "text": "Overview\n\n\nIn this module you will set up container-native storage (CNS) in your OpenShift environment. You will use this to dynamically provision storage to be available to workloads in OpenShift. It is provided by GlusterFS running in containers. GlusterFS in turn is backed by local storage available to the OpenShift nodes.\n\n\n\n\n Make sure you are logged on as the \nec2-user\n to the master node:\n\n\n[ec2-user@master ~]$ hostname -f\nmaster.lab\n\n\n\n\n\n\n\nCaution\n\n\nAll of the following tasks are carried out as the ec2-user from the master node. For Copy \n Paste convenience we will omit the shell prompt unless necessary.\n\n\nAll files created can be stored in root\u2019s home directory unless a particular path is specified.\n\n\n\n\n First ensure the CNS deployment tool is available (it should already be installed)\n\n\nyum list installed cns-deploy\n\n\n\n\n\nConfigure the firewall with Ansible\n#\n\n\n\n\nHint\n\n\nIn the following we will use Ansible\ns configuration management capabilities in order to make sure all the OpenShift nodes have the right firewall settings. This is for your convenience.\n\n\n\n\n\n\n You should be able to ping all hosts using Ansible:\n\n\nansible nodes -m ping\n\n\n\n\n\nAll 6 OpenShift application nodes should respond\n\n\nnode-4.lab | SUCCESS =\n {\n    \nchanged\n: false,\n    \nping\n: \npong\n\n}\nnode-1.lab | SUCCESS =\n {\n    \nchanged\n: false,\n    \nping\n: \npong\n\n}\nmaster.lab | SUCCESS =\n {\n    \nchanged\n: false,\n    \nping\n: \npong\n\n}\nnode-2.lab | SUCCESS =\n {\n    \nchanged\n: false,\n    \nping\n: \npong\n\n}\nnode-3.lab | SUCCESS =\n {\n    \nchanged\n: false,\n    \nping\n: \npong\n\n}\nnode-5.lab | SUCCESS =\n {\n    \nchanged\n: false,\n    \nping\n: \npong\n\n}\nnode-6.lab | SUCCESS =\n {\n    \nchanged\n: false,\n    \nping\n: \npong\n\n}\n\n\n\n\n\n Next, create a file called \nconfigure-firewall.yml\n and copy\npaste the following contents:\n\n\nconfigure-firewall.yml:\n\n\n---\n\n\n\n-\n \nhosts\n:\n \nnodes\n\n\n  \ntasks\n:\n\n\n    \n-\n \nname\n:\n \ninsert iptables rules required for GlusterFS\n\n      \nblockinfile\n:\n\n        \ndest\n:\n \n/etc/sysconfig/iptables\n\n        \nblock\n:\n \n|\n\n          \n-A OS_FIREWALL_ALLOW -p tcp -m state --state NEW -m tcp --dport 24007 -j ACCEPT\n\n          \n-A OS_FIREWALL_ALLOW -p tcp -m state --state NEW -m tcp --dport 24008 -j ACCEPT\n\n          \n-A OS_FIREWALL_ALLOW -p tcp -m state --state NEW -m tcp --dport 2222 -j ACCEPT\n\n          \n-A OS_FIREWALL_ALLOW -p tcp -m state --state NEW -m multiport --dports 49152:49664 -j ACCEPT\n\n        \ninsertbefore\n:\n \n^COMMIT\n\n\n    \n-\n \nname\n:\n \nreload iptables\n\n      \nsystemd\n:\n\n        \nname\n:\n \niptables\n\n        \nstate\n:\n \nreloaded\n\n\n\n...\n\n\n\n\n\n\nDone. This small playbook will save us some work in configuring the firewall top open required ports for CNS on each individual node.\n\n\n Run it with the following command:\n\n\nansible-playbook configure-firewall.yml\n\n\n\n\n\nYour output should look like this.\n\n\nPLAY [nodes]****************************************************************\n\nTASK [Gathering Facts]******************************************************\nok: [node-4.lab]\nok: [node-2.lab]\nok: [node-1.lab]\nok: [node-3.lab]\nok: [master.lab]\nok: [node-5.lab]\nok: [node-6.lab]\n\nTASK [insert iptables rules required for GlusterFS]*************************\nchanged: [node-1.lab]\nchanged: [node-4.lab]\nchanged: [node-2.lab]\nchanged: [node-3.lab]\nchanged: [master.lab]\nchanged: [node-5.lab]\nchanged: [node-6.lab]\n\nTASK [reload iptables]******************************************************\nchanged: [node-4.lab]\nchanged: [node-1.lab]\nchanged: [node-2.lab]\nchanged: [node-3.lab]\nchanged: [master.lab]\nchanged: [node-6.lab]\nchanged: [node-5.lab]\n\nPLAY RECAP*****************************************************************\nmaster.lab                 : ok=3    changed=2    unreachable=0    failed=0\nnode-1.lab                 : ok=3    changed=2    unreachable=0    failed=0\nnode-2.lab                 : ok=3    changed=2    unreachable=0    failed=0\nnode-3.lab                 : ok=3    changed=2    unreachable=0    failed=0\nnode-4.lab                 : ok=3    changed=2    unreachable=0    failed=0\nnode-5.lab                 : ok=3    changed=2    unreachable=0    failed=0\nnode-6.lab                 : ok=3    changed=2    unreachable=0    failed=0\n\n\n\n\n\nWith this we checked the requirement for additional firewall ports to be opened on the OpenShift app nodes.\n\n\n\n\nPrepare OpenShift for CNS\n#\n\n\nNext we will create a namespace (also referred to as a \nProject\n) in OpenShift. It will be used to group the GlusterFS pods.\n\n\n\n\nImportant\n\n\nIf you skipped Module 1 you need give the \noperator\n user cluster admin privileges first:\n\n\n Log in the built-in system admin\n\n\noc login -u system:admin\n\n\n Grant the user \noperator\n cluster admin privileges in OpenShift\n\n\noadm policy add-cluster-role-to-user cluster-admin operator\n\n\n\n\n For the deployment you need to be logged as the \noperator\n user in OpenShift.\n\n\n[ec2-user@master ~]# oc whoami\noperator\n\n\n\n\n\n If you are for some reason not the operator, login to the default namespace like this:\n\n\noc login -u operator -n default\n\n\n\n\n\n Create a namespace with the designation \ncontainer-native-storage\n:\n\n\noc new-project container-native-storage\n\n\n\n\n\nGlusterFS pods need access to the physical block devices on the host. Hence they need elevated permissions.\n\n\n Enable containers to run in privileged mode:\n\n\noadm policy add-scc-to-user privileged -z default\n\n\n\n\n\nBuild Container-native Storage Topology\n#\n\n\nCNS will virtualize locally attached block storage on the OpenShift App nodes. In order to deploy you will need to supply the installer with information about where to find these nodes and what network and which block devices to use.\n\nThis is done using JSON file describing the topology of your OpenShift deployment.\n\n\nWe\nll start with the first 3 OpenShift app nodes. For this purpose, create the file topology.json like the example below.\n\n\n\n\nImportant\n\n\nThe deployment tool always expects fully-qualified domains names for the \nmanage\n property and always IP addresses for the \nstorage\n property for the hostnames.\n\n\n\n\ntopology.json:\n\n\n{\n\n    \nclusters\n:\n \n[\n\n        \n{\n\n            \nnodes\n:\n \n[\n\n                \n{\n\n                    \nnode\n:\n \n{\n\n                        \nhostnames\n:\n \n{\n\n                            \nmanage\n:\n \n[\n\n                                \nnode-1.lab\n\n                            \n],\n\n                            \nstorage\n:\n \n[\n\n                                \n10.0.2.101\n\n                            \n]\n\n                        \n},\n\n                        \nzone\n:\n \n1\n\n                    \n},\n\n                    \ndevices\n:\n \n[\n\n                        \n/dev/xvdc\n\n                    \n]\n\n                \n},\n\n                \n{\n\n                    \nnode\n:\n \n{\n\n                        \nhostnames\n:\n \n{\n\n                            \nmanage\n:\n \n[\n\n                                \nnode-2.lab\n\n                            \n],\n\n                            \nstorage\n:\n \n[\n\n                                \n10.0.3.102\n\n                            \n]\n\n                        \n},\n\n                        \nzone\n:\n \n2\n\n                    \n},\n\n                    \ndevices\n:\n \n[\n\n                        \n/dev/xvdc\n\n                    \n]\n\n                \n},\n\n                \n{\n\n                    \nnode\n:\n \n{\n\n                        \nhostnames\n:\n \n{\n\n                            \nmanage\n:\n \n[\n\n                                \nnode-3.lab\n\n                            \n],\n\n                            \nstorage\n:\n \n[\n\n                                \n10.0.4.103\n\n                            \n]\n\n                        \n},\n\n                        \nzone\n:\n \n3\n\n                    \n},\n\n                    \ndevices\n:\n \n[\n\n                        \n/dev/xvdc\n\n                    \n]\n\n                \n}\n\n            \n]\n\n        \n}\n\n    \n]\n\n\n}\n\n\n\n\n\n\n\n\nWhat is the zone ID for?\n\n\nNext to the obvious information like fully-qualified hostnames, IP address and device names required for Gluster the topology contains an additional property called \nzone\n per node.\n\n\nA zone identifies a failure domain. In CNS data is always replicated 3 times. Reflecting these by zone IDs as arbitrary but distinct numerical values allows CNS to ensure that two copies are never stored on nodes in the same failure domain.\n\n\nThis information is considered when building new volumes, expanding existing volumes or replacing bricks in degraded volumes.\n\nAn example for failure domains are AWS Availability Zones or physical servers sharing the same PDU.\n\n\n\n\n\n\nDeploy Container-native Storage\n#\n\n\nYou are now ready to deploy CNS. Alongside GlusterFS pods the API front-end known as \nheketi\n is deployed. By default it runs without any authentication layer.\n\nTo protect the API from unauthorized access we will define passwords for the \nadmin\n and \nuser\n role in heketi like below. We will refer to these later again.\n\n\n\n\n\n\n\n\nHeketi Role\n\n\nPassword\n\n\n\n\n\n\n\n\n\n\nadmin\n\n\nmyS3cr3tpassw0rd\n\n\n\n\n\n\nuser\n\n\nmys3rs3cr3tpassw0rd\n\n\n\n\n\n\n\n\n Next start the deployment routine with the following command:\n\n\ncns-deploy -n container-native-storage -g topology.json --admin-key \nmyS3cr3tpassw0rd\n --user-key \nmys3rs3cr3tpassw0rd\n\n\n\n\n\n\nAnswer the interactive prompt with \nY\n.\n\n\n\n\nNote\n\n\nThe deployment will take several minutes to complete. You may want to monitor the progress in parallel also in the OpenShift UI.\n\n\nLog in as the \noperator\n user to the UI and select the \ncontainer-native-storage\n project.\n\n\n\n\nOn the command line the output should look like this:\n\n\nWelcome to the deployment tool for GlusterFS on Kubernetes and OpenShift.\n\nBefore getting started, this script has some requirements of the execution\nenvironment and of the container platform that you should verify.\n\nThe client machine that will run this script must have:\n * Administrative access to an existing Kubernetes or OpenShift cluster\n * Access to a python interpreter \npython\n\n * Access to the heketi client \nheketi-cli\n\n\nEach of the nodes that will host GlusterFS must also have appropriate firewall\nrules for the required GlusterFS ports:\n * 2222  - sshd (if running GlusterFS in a pod)\n * 24007 - GlusterFS Daemon\n * 24008 - GlusterFS Management\n * 49152 to 49251 - Each brick for every volume on the host requires its own\n   port. For every new brick, one new port will be used starting at 49152. We\n   recommend a default range of 49152-49251 on each host, though you can adjust\n   this to fit your needs.\n\nIn addition, for an OpenShift deployment you must:\n * Have \ncluster_admin\n role on the administrative account doing the deployment\n * Add the \ndefault\n and \nrouter\n Service Accounts to the \nprivileged\n SCC\n * Have a router deployed that is configured to allow apps to access services\n   running in the cluster\n\n\nDo you wish to proceed with deployment? Y\n\n[Y]es, [N]o? [Default: Y]:\n\nUsing OpenShift CLI.\nNAME                       STATUS    AGE\ncontainer-native-storage   Active    28m\nUsing namespace \ncontainer-native-storage\n.\nChecking that heketi pod is not running ... OK\ntemplate \ndeploy-heketi\n created\nserviceaccount \nheketi-service-account\n created\ntemplate \nheketi\n created\ntemplate \nglusterfs\n created\nrole \nedit\n added: \nsystem:serviceaccount:container-native-storage:heketi-service-account\n\n\nnode \nnode1.example.com\n labeled\n\nnode \nnode2.example.com\n labeled\n\nnode \nnode3.example.com\n labeled\n\ndaemonset \nglusterfs\n created\n\nWaiting for GlusterFS pods to start ... OK\n\nservice \ndeploy-heketi\n created\nroute \ndeploy-heketi\n created\ndeploymentconfig \ndeploy-heketi\n created\nWaiting for deploy-heketi pod to start ... OK\nCreating cluster ... ID: 307f708621f4e0c9eda962b713272e81\n\nCreating node node1.example.com ... ID: f60a225a16e8678d5ef69afb4815e417\n\nAdding device /dev/vdc ... OK\n\nCreating node node2.example.com ... ID: 13b7c17c541069862d7e66d142ab789e\n\nAdding device /dev/vdc ... OK\n\nCreating node node3.example.com ... ID: 5a6fbe5eb1864e711f8bd9b0cb5946ea\n\nAdding device /dev/vdc ... OK\nheketi topology loaded.\nSaving heketi-storage.json\nsecret \nheketi-storage-secret\n created\nendpoints \nheketi-storage-endpoints\n created\nservice \nheketi-storage-endpoints\n created\njob \nheketi-storage-copy-job\n created\ndeploymentconfig \ndeploy-heketi\n deleted\nroute \ndeploy-heketi\n deleted\nservice \ndeploy-heketi\n deleted\njob \nheketi-storage-copy-job\n deleted\npod \ndeploy-heketi-1-599rc\n deleted\nsecret \nheketi-storage-secret\n deleted\nservice \nheketi\n created\n\nroute \nheketi\n created\n\ndeploymentconfig \nheketi\n created\n\nWaiting for heketi pod to start ... OK\n\nheketi is now running.\nReady to create and provide GlusterFS volumes.\n\n\n\n\n\nIn order of the appearance of the highlighted lines above in a nutshell what happens here is the following:\n\n\n\n\n\n\nEnter \nY\n and press Enter.\n\n\n\n\n\n\nOpenShift nodes are labeled. Label is referred to in a DaemonSet.\n\n\n\n\n\n\nGlusterFS daemonset is started. DaemonSet means: start exactly \none\n pod per node.\n\n\n\n\n\n\nAll nodes will be referenced in heketi\u2019s database by a UUID. Node block devices are formatted for mounting by GlusterFS.\n\n\n\n\n\n\nA public route is created for the heketi pod to expose it\ns API.\n\n\n\n\n\n\nheketi is deployed in a pod as well.\n\n\n\n\n\n\nDuring the deployment the UI output will look like this:\n\n\n\n\n\n\n\n\nVerifying the deployment\n#\n\n\nYou now have deployed CNS. Let\u2019s verify all components are in place.\n\n\n If not already there on the CLI change back to the \ncontainer-native-storage\n namespace:\n\n\noc project container-native-storage\n\n\n\n\n\n List all running pods:\n\n\noc get pods -o wide\n\n\n\n\n\nYou should see all pods up and running. Highlighted containerized gluster daemons on each pods carry the IP of the OpenShift node they are running on.\n\n\nNAME              READY     STATUS    RESTARTS   AGE       IP           NODE\n\nglusterfs-5rc2g   1/1       Running   0          3m        10.0.2.101   node-1.lab\n\nglusterfs-jbvdk   1/1       Running   0          3m        10.0.3.102   node-2.lab\n\nglusterfs-rchtr   1/1       Running   0          3m        10.0.4.103   node-3.lab\n\nheketi-1-tn0s9    1/1       Running   0          2m        10.130.2.3   node-6.lab\n\n\n\n\n\n\n\nNote\n\n\nThe exact pod names will be different in your environment, since they are auto-generated.\n\n\n\n\nThe GlusterFS pods use the hosts network and disk devices to run the software-defined storage system. Hence they attached to the host\u2019s network. See schematic below for a visualization.\n\n\n\n\nheketi\n is a component that will expose an API for GlusterFS to OpenShift. This allows OpenShift to dynamically allocate storage from CNS in a programmatic fashion. See below for a visualization. Note that for simplicity, in our example heketi runs on the OpenShift App nodes, not on the Infra node.\n\n\n\n\nTo expose heketi\u2019s API a \nservice\n named \nheketi\n has been generated in OpenShift.\n\n\n Check the service with:\n\n\noc get service/heketi\n\n\n\n\n\nThe output should look similar to the below:\n\n\nNAME      CLUSTER-IP     EXTERNAL-IP   PORT(S)    AGE\nheketi    172.30.5.231   \nnone\n        8080/TCP   31m\n\n\n\n\n\nTo also use heketi outside of OpenShift in addition to the service a route has been deployed.\n\n\n Display the route with:\n\n\noc get route/heketi\n\n\n\n\n\nThe output should look similar to the below:\n\n\nNAME      HOST/PORT                                                        PATH      SERVICES   PORT      TERMINATION   WILDCARD\nheketi    heketi-container-native-storage.cloudapps.34.252.58.209.nip.io             heketi     \nall\n                   None\n\n\n\n\n\n\n\nNote\n\n\nIn your environment the URL will be slightly different. It will contain the public IPv4 address of your deployment, dynamically resolved by the nip.io service.\n\n\n\n\nBased on this \nheketi\n will be available on this URL:\n\nhttp://\nheketi-container-native-storage.cloudapps.34.252.58.209.nip.io\n\n\n You may verify this with a trivial health check:\n\n\ncurl http://heketi-container-native-storage.cloudapps.34.252.58.209.nip.io/hello\n\n\n\n\n\nThis should say:\n\n\nHello from Heketi\n\n\n\n\n\nIt appears heketi is running. To ensure it\ns functional and has been set up with authentication we are going to query it with the heketi CLI client.\n\nThe client needs to know the heketi service URL above and the password for the \nadmin\n noted in the \ndeployment step\n.\n\n\n\n\n Configure the heketi client with environment variables.\n\n\nexport HEKETI_CLI_SERVER=http://heketi-container-native-storage.cloudapps.34.252.58.209.nip.io\nexport HEKETI_CLI_USER=admin\nexport HEKETI_CLI_KEY=myS3cr3tpassw0rd\n\n\n\n\n\n You are now able to use the heketi CLI tool:\n\n\nheketi-cli cluster list\n\n\n\n\n\nThis should list at least one cluster by it\ns UUID:\n\n\nClusters:\nfb67f97166c58f161b85201e1fd9b8ed\n\n\n\n\n\n\n\nNote\n\n\nThe UUID is auto-generated and will be different for you.\n\n\n\n\n Use the UUID unique to your environment and obtain more information about it:\n\n\nheketi-cli cluster info fb67f97166c58f161b85201e1fd9b8e\n\n\n\n\n\nThere should be 3 nodes and 1 volume, again displayed with their UUIDs.\n\n\nCluster id: fb67f97166c58f161b85201e1fd9b8ed\nNodes:\n22cbcd136fa40ffe766a13f305cc1e3b\nbfc006b571e85a083118054233bfb16d\nc5979019ac13b9fe02f4e4e2dc6d62cb\nVolumes:\n2415fba2b9364a65711da2a8311a663a\n\n\n\n\n\n To display a comprehensive overview of everything heketi knows about query it\ns topology:\n\n\nheketi-cli topology info\n\n\n\n\n\nYou will get lengthy output that shows what nodes and disk devices CNS has used to deploy a containerised GlusterFS cluster.\n\n\nCluster Id: fb67f97166c58f161b85201e1fd9b8ed\n\nVolumes:\n\nName: heketidbstorage\nSize: 2\nId: 2415fba2b9364a65711da2a8311a663a\nCluster Id: fb67f97166c58f161b85201e1fd9b8ed\nMount: 10.0.2.101:heketidbstorage\nMount Options: backup-volfile-servers=10.0.3.102,10.0.4.103\nDurability Type: replicate\nReplica: 3\nSnapshot: Disabled\n\nBricks:\n  Id: 55851d8ab270112c07ab7a38d55c8045\n  Path: /var/lib/heketi/mounts/vg_41b8a921f8e6d31cb04c7dd35b6b4cf2/brick_55851d8ab270112c07ab7a38d55c8045/brick\n  Size (GiB): 2\n  Node: bfc006b571e85a083118054233bfb16d\n  Device: 41b8a921f8e6d31cb04c7dd35b6b4cf2\n\n  Id: 67161e0e607c38677a0ef3f617b8dc1e\n  Path: /var/lib/heketi/mounts/vg_8ea71174529a35f41fc0d1b288da6299/brick_67161e0e607c38677a0ef3f617b8dc1e/brick\n  Size (GiB): 2\n  Node: 22cbcd136fa40ffe766a13f305cc1e3b\n  Device: 8ea71174529a35f41fc0d1b288da6299\n\n  Id: a8bf049dcea2d5245b64a792d4b85e6b\n  Path: /var/lib/heketi/mounts/vg_2a49883a5cb39c3b845477ff85a729ba/brick_a8bf049dcea2d5245b64a792d4b85e6b/brick\n  Size (GiB): 2\n  Node: c5979019ac13b9fe02f4e4e2dc6d62cb\n  Device: 2a49883a5cb39c3b845477ff85a729ba\n\n\nNodes:\n\nNode Id: 22cbcd136fa40ffe766a13f305cc1e3b\nState: online\nCluster Id: fb67f97166c58f161b85201e1fd9b8ed\nZone: 2\nManagement Hostname: node-2.lab\nStorage Hostname: 10.0.3.102\nDevices:\nId:8ea71174529a35f41fc0d1b288da6299   Name:/dev/xvdc           State:online    Size (GiB):49      Used (GiB):2       Free (GiB):47\n  Bricks:\n    Id:67161e0e607c38677a0ef3f617b8dc1e   Size (GiB):2       Path: /var/lib/heketi/mounts/vg_8ea71174529a35f41fc0d1b288da6299/brick_67161e0e607c38677a0ef3f617b8dc1e/brick\n\nNode Id: bfc006b571e85a083118054233bfb16d\nState: online\nCluster Id: fb67f97166c58f161b85201e1fd9b8ed\nZone: 3\nManagement Hostname: node-3.lab\nStorage Hostname: 10.0.4.103\nDevices:\nId:41b8a921f8e6d31cb04c7dd35b6b4cf2   Name:/dev/xvdc           State:online    Size (GiB):49      Used (GiB):2       Free (GiB):47\n  Bricks:\n    Id:55851d8ab270112c07ab7a38d55c8045   Size (GiB):2       Path: /var/lib/heketi/mounts/vg_41b8a921f8e6d31cb04c7dd35b6b4cf2/brick_55851d8ab270112c07ab7a38d55c8045/brick\n\nNode Id: c5979019ac13b9fe02f4e4e2dc6d62cb\nState: online\nCluster Id: fb67f97166c58f161b85201e1fd9b8ed\nZone: 1\nManagement Hostname: node-1.lab\nStorage Hostname: 10.0.2.101\nDevices:\nId:2a49883a5cb39c3b845477ff85a729ba   Name:/dev/xvdc           State:online    Size (GiB):49      Used (GiB):2       Free (GiB):47\n  Bricks:\n    Id:a8bf049dcea2d5245b64a792d4b85e6b   Size (GiB):2       Path: /var/lib/heketi/mounts/vg_2a49883a5cb39c3b845477ff85a729ba/brick_a8bf049dcea2d5245b64a792d4b85e6b/brick\n\n\n\n\n\nThis information should correspond to the topology.json file you supplied to the installer. With this we successfully verified the CNS deployment.", 
            "title": "Module 2 - Deploying Container-Native Storage"
        }, 
        {
            "location": "/module-2-deploy-cns/#configure-the-firewall-with-ansible", 
            "text": "Hint  In the following we will use Ansible s configuration management capabilities in order to make sure all the OpenShift nodes have the right firewall settings. This is for your convenience.     You should be able to ping all hosts using Ansible:  ansible nodes -m ping  All 6 OpenShift application nodes should respond  node-4.lab | SUCCESS =  {\n     changed : false,\n     ping :  pong \n}\nnode-1.lab | SUCCESS =  {\n     changed : false,\n     ping :  pong \n}\nmaster.lab | SUCCESS =  {\n     changed : false,\n     ping :  pong \n}\nnode-2.lab | SUCCESS =  {\n     changed : false,\n     ping :  pong \n}\nnode-3.lab | SUCCESS =  {\n     changed : false,\n     ping :  pong \n}\nnode-5.lab | SUCCESS =  {\n     changed : false,\n     ping :  pong \n}\nnode-6.lab | SUCCESS =  {\n     changed : false,\n     ping :  pong \n}   Next, create a file called  configure-firewall.yml  and copy paste the following contents:  configure-firewall.yml:  ---  -   hosts :   nodes \n\n   tasks : \n\n     -   name :   insert iptables rules required for GlusterFS \n       blockinfile : \n         dest :   /etc/sysconfig/iptables \n         block :   | \n           -A OS_FIREWALL_ALLOW -p tcp -m state --state NEW -m tcp --dport 24007 -j ACCEPT \n           -A OS_FIREWALL_ALLOW -p tcp -m state --state NEW -m tcp --dport 24008 -j ACCEPT \n           -A OS_FIREWALL_ALLOW -p tcp -m state --state NEW -m tcp --dport 2222 -j ACCEPT \n           -A OS_FIREWALL_ALLOW -p tcp -m state --state NEW -m multiport --dports 49152:49664 -j ACCEPT \n         insertbefore :   ^COMMIT \n\n     -   name :   reload iptables \n       systemd : \n         name :   iptables \n         state :   reloaded  ...   Done. This small playbook will save us some work in configuring the firewall top open required ports for CNS on each individual node.   Run it with the following command:  ansible-playbook configure-firewall.yml  Your output should look like this.  PLAY [nodes]****************************************************************\n\nTASK [Gathering Facts]******************************************************\nok: [node-4.lab]\nok: [node-2.lab]\nok: [node-1.lab]\nok: [node-3.lab]\nok: [master.lab]\nok: [node-5.lab]\nok: [node-6.lab]\n\nTASK [insert iptables rules required for GlusterFS]*************************\nchanged: [node-1.lab]\nchanged: [node-4.lab]\nchanged: [node-2.lab]\nchanged: [node-3.lab]\nchanged: [master.lab]\nchanged: [node-5.lab]\nchanged: [node-6.lab]\n\nTASK [reload iptables]******************************************************\nchanged: [node-4.lab]\nchanged: [node-1.lab]\nchanged: [node-2.lab]\nchanged: [node-3.lab]\nchanged: [master.lab]\nchanged: [node-6.lab]\nchanged: [node-5.lab]\n\nPLAY RECAP*****************************************************************\nmaster.lab                 : ok=3    changed=2    unreachable=0    failed=0\nnode-1.lab                 : ok=3    changed=2    unreachable=0    failed=0\nnode-2.lab                 : ok=3    changed=2    unreachable=0    failed=0\nnode-3.lab                 : ok=3    changed=2    unreachable=0    failed=0\nnode-4.lab                 : ok=3    changed=2    unreachable=0    failed=0\nnode-5.lab                 : ok=3    changed=2    unreachable=0    failed=0\nnode-6.lab                 : ok=3    changed=2    unreachable=0    failed=0  With this we checked the requirement for additional firewall ports to be opened on the OpenShift app nodes.", 
            "title": "Configure the firewall with Ansible"
        }, 
        {
            "location": "/module-2-deploy-cns/#prepare-openshift-for-cns", 
            "text": "Next we will create a namespace (also referred to as a  Project ) in OpenShift. It will be used to group the GlusterFS pods.   Important  If you skipped Module 1 you need give the  operator  user cluster admin privileges first:   Log in the built-in system admin  oc login -u system:admin   Grant the user  operator  cluster admin privileges in OpenShift  oadm policy add-cluster-role-to-user cluster-admin operator    For the deployment you need to be logged as the  operator  user in OpenShift.  [ec2-user@master ~]# oc whoami\noperator   If you are for some reason not the operator, login to the default namespace like this:  oc login -u operator -n default   Create a namespace with the designation  container-native-storage :  oc new-project container-native-storage  GlusterFS pods need access to the physical block devices on the host. Hence they need elevated permissions.   Enable containers to run in privileged mode:  oadm policy add-scc-to-user privileged -z default", 
            "title": "Prepare OpenShift for CNS"
        }, 
        {
            "location": "/module-2-deploy-cns/#build-container-native-storage-topology", 
            "text": "CNS will virtualize locally attached block storage on the OpenShift App nodes. In order to deploy you will need to supply the installer with information about where to find these nodes and what network and which block devices to use. \nThis is done using JSON file describing the topology of your OpenShift deployment.  We ll start with the first 3 OpenShift app nodes. For this purpose, create the file topology.json like the example below.   Important  The deployment tool always expects fully-qualified domains names for the  manage  property and always IP addresses for the  storage  property for the hostnames.   topology.json:  { \n     clusters :   [ \n         { \n             nodes :   [ \n                 { \n                     node :   { \n                         hostnames :   { \n                             manage :   [ \n                                 node-1.lab \n                             ], \n                             storage :   [ \n                                 10.0.2.101 \n                             ] \n                         }, \n                         zone :   1 \n                     }, \n                     devices :   [ \n                         /dev/xvdc \n                     ] \n                 }, \n                 { \n                     node :   { \n                         hostnames :   { \n                             manage :   [ \n                                 node-2.lab \n                             ], \n                             storage :   [ \n                                 10.0.3.102 \n                             ] \n                         }, \n                         zone :   2 \n                     }, \n                     devices :   [ \n                         /dev/xvdc \n                     ] \n                 }, \n                 { \n                     node :   { \n                         hostnames :   { \n                             manage :   [ \n                                 node-3.lab \n                             ], \n                             storage :   [ \n                                 10.0.4.103 \n                             ] \n                         }, \n                         zone :   3 \n                     }, \n                     devices :   [ \n                         /dev/xvdc \n                     ] \n                 } \n             ] \n         } \n     ]  }    What is the zone ID for?  Next to the obvious information like fully-qualified hostnames, IP address and device names required for Gluster the topology contains an additional property called  zone  per node.  A zone identifies a failure domain. In CNS data is always replicated 3 times. Reflecting these by zone IDs as arbitrary but distinct numerical values allows CNS to ensure that two copies are never stored on nodes in the same failure domain.  This information is considered when building new volumes, expanding existing volumes or replacing bricks in degraded volumes. \nAn example for failure domains are AWS Availability Zones or physical servers sharing the same PDU.", 
            "title": "Build Container-native Storage Topology"
        }, 
        {
            "location": "/module-2-deploy-cns/#deploy-container-native-storage", 
            "text": "You are now ready to deploy CNS. Alongside GlusterFS pods the API front-end known as  heketi  is deployed. By default it runs without any authentication layer. \nTo protect the API from unauthorized access we will define passwords for the  admin  and  user  role in heketi like below. We will refer to these later again.     Heketi Role  Password      admin  myS3cr3tpassw0rd    user  mys3rs3cr3tpassw0rd      Next start the deployment routine with the following command:  cns-deploy -n container-native-storage -g topology.json --admin-key  myS3cr3tpassw0rd  --user-key  mys3rs3cr3tpassw0rd   Answer the interactive prompt with  Y .   Note  The deployment will take several minutes to complete. You may want to monitor the progress in parallel also in the OpenShift UI.  Log in as the  operator  user to the UI and select the  container-native-storage  project.   On the command line the output should look like this:  Welcome to the deployment tool for GlusterFS on Kubernetes and OpenShift.\n\nBefore getting started, this script has some requirements of the execution\nenvironment and of the container platform that you should verify.\n\nThe client machine that will run this script must have:\n * Administrative access to an existing Kubernetes or OpenShift cluster\n * Access to a python interpreter  python \n * Access to the heketi client  heketi-cli \n\nEach of the nodes that will host GlusterFS must also have appropriate firewall\nrules for the required GlusterFS ports:\n * 2222  - sshd (if running GlusterFS in a pod)\n * 24007 - GlusterFS Daemon\n * 24008 - GlusterFS Management\n * 49152 to 49251 - Each brick for every volume on the host requires its own\n   port. For every new brick, one new port will be used starting at 49152. We\n   recommend a default range of 49152-49251 on each host, though you can adjust\n   this to fit your needs.\n\nIn addition, for an OpenShift deployment you must:\n * Have  cluster_admin  role on the administrative account doing the deployment\n * Add the  default  and  router  Service Accounts to the  privileged  SCC\n * Have a router deployed that is configured to allow apps to access services\n   running in the cluster Do you wish to proceed with deployment? Y [Y]es, [N]o? [Default: Y]:\n\nUsing OpenShift CLI.\nNAME                       STATUS    AGE\ncontainer-native-storage   Active    28m\nUsing namespace  container-native-storage .\nChecking that heketi pod is not running ... OK\ntemplate  deploy-heketi  created\nserviceaccount  heketi-service-account  created\ntemplate  heketi  created\ntemplate  glusterfs  created\nrole  edit  added:  system:serviceaccount:container-native-storage:heketi-service-account  node  node1.example.com  labeled node  node2.example.com  labeled node  node3.example.com  labeled daemonset  glusterfs  created Waiting for GlusterFS pods to start ... OK service  deploy-heketi  created\nroute  deploy-heketi  created\ndeploymentconfig  deploy-heketi  created\nWaiting for deploy-heketi pod to start ... OK\nCreating cluster ... ID: 307f708621f4e0c9eda962b713272e81 Creating node node1.example.com ... ID: f60a225a16e8678d5ef69afb4815e417 Adding device /dev/vdc ... OK Creating node node2.example.com ... ID: 13b7c17c541069862d7e66d142ab789e Adding device /dev/vdc ... OK Creating node node3.example.com ... ID: 5a6fbe5eb1864e711f8bd9b0cb5946ea Adding device /dev/vdc ... OK\nheketi topology loaded.\nSaving heketi-storage.json\nsecret  heketi-storage-secret  created\nendpoints  heketi-storage-endpoints  created\nservice  heketi-storage-endpoints  created\njob  heketi-storage-copy-job  created\ndeploymentconfig  deploy-heketi  deleted\nroute  deploy-heketi  deleted\nservice  deploy-heketi  deleted\njob  heketi-storage-copy-job  deleted\npod  deploy-heketi-1-599rc  deleted\nsecret  heketi-storage-secret  deleted\nservice  heketi  created route  heketi  created deploymentconfig  heketi  created Waiting for heketi pod to start ... OK heketi is now running.\nReady to create and provide GlusterFS volumes.  In order of the appearance of the highlighted lines above in a nutshell what happens here is the following:    Enter  Y  and press Enter.    OpenShift nodes are labeled. Label is referred to in a DaemonSet.    GlusterFS daemonset is started. DaemonSet means: start exactly  one  pod per node.    All nodes will be referenced in heketi\u2019s database by a UUID. Node block devices are formatted for mounting by GlusterFS.    A public route is created for the heketi pod to expose it s API.    heketi is deployed in a pod as well.    During the deployment the UI output will look like this:", 
            "title": "Deploy Container-native Storage"
        }, 
        {
            "location": "/module-2-deploy-cns/#verifying-the-deployment", 
            "text": "You now have deployed CNS. Let\u2019s verify all components are in place.   If not already there on the CLI change back to the  container-native-storage  namespace:  oc project container-native-storage   List all running pods:  oc get pods -o wide  You should see all pods up and running. Highlighted containerized gluster daemons on each pods carry the IP of the OpenShift node they are running on.  NAME              READY     STATUS    RESTARTS   AGE       IP           NODE glusterfs-5rc2g   1/1       Running   0          3m        10.0.2.101   node-1.lab glusterfs-jbvdk   1/1       Running   0          3m        10.0.3.102   node-2.lab glusterfs-rchtr   1/1       Running   0          3m        10.0.4.103   node-3.lab heketi-1-tn0s9    1/1       Running   0          2m        10.130.2.3   node-6.lab   Note  The exact pod names will be different in your environment, since they are auto-generated.   The GlusterFS pods use the hosts network and disk devices to run the software-defined storage system. Hence they attached to the host\u2019s network. See schematic below for a visualization.   heketi  is a component that will expose an API for GlusterFS to OpenShift. This allows OpenShift to dynamically allocate storage from CNS in a programmatic fashion. See below for a visualization. Note that for simplicity, in our example heketi runs on the OpenShift App nodes, not on the Infra node.   To expose heketi\u2019s API a  service  named  heketi  has been generated in OpenShift.   Check the service with:  oc get service/heketi  The output should look similar to the below:  NAME      CLUSTER-IP     EXTERNAL-IP   PORT(S)    AGE\nheketi    172.30.5.231    none         8080/TCP   31m  To also use heketi outside of OpenShift in addition to the service a route has been deployed.   Display the route with:  oc get route/heketi  The output should look similar to the below:  NAME      HOST/PORT                                                        PATH      SERVICES   PORT      TERMINATION   WILDCARD\nheketi    heketi-container-native-storage.cloudapps.34.252.58.209.nip.io             heketi      all                    None   Note  In your environment the URL will be slightly different. It will contain the public IPv4 address of your deployment, dynamically resolved by the nip.io service.   Based on this  heketi  will be available on this URL: \nhttp:// heketi-container-native-storage.cloudapps.34.252.58.209.nip.io   You may verify this with a trivial health check:  curl http://heketi-container-native-storage.cloudapps.34.252.58.209.nip.io/hello  This should say:  Hello from Heketi  It appears heketi is running. To ensure it s functional and has been set up with authentication we are going to query it with the heketi CLI client. \nThe client needs to know the heketi service URL above and the password for the  admin  noted in the  deployment step .    Configure the heketi client with environment variables.  export HEKETI_CLI_SERVER=http://heketi-container-native-storage.cloudapps.34.252.58.209.nip.io\nexport HEKETI_CLI_USER=admin\nexport HEKETI_CLI_KEY=myS3cr3tpassw0rd   You are now able to use the heketi CLI tool:  heketi-cli cluster list  This should list at least one cluster by it s UUID:  Clusters:\nfb67f97166c58f161b85201e1fd9b8ed   Note  The UUID is auto-generated and will be different for you.    Use the UUID unique to your environment and obtain more information about it:  heketi-cli cluster info fb67f97166c58f161b85201e1fd9b8e  There should be 3 nodes and 1 volume, again displayed with their UUIDs.  Cluster id: fb67f97166c58f161b85201e1fd9b8ed\nNodes:\n22cbcd136fa40ffe766a13f305cc1e3b\nbfc006b571e85a083118054233bfb16d\nc5979019ac13b9fe02f4e4e2dc6d62cb\nVolumes:\n2415fba2b9364a65711da2a8311a663a   To display a comprehensive overview of everything heketi knows about query it s topology:  heketi-cli topology info  You will get lengthy output that shows what nodes and disk devices CNS has used to deploy a containerised GlusterFS cluster.  Cluster Id: fb67f97166c58f161b85201e1fd9b8ed\n\nVolumes:\n\nName: heketidbstorage\nSize: 2\nId: 2415fba2b9364a65711da2a8311a663a\nCluster Id: fb67f97166c58f161b85201e1fd9b8ed\nMount: 10.0.2.101:heketidbstorage\nMount Options: backup-volfile-servers=10.0.3.102,10.0.4.103\nDurability Type: replicate\nReplica: 3\nSnapshot: Disabled\n\nBricks:\n  Id: 55851d8ab270112c07ab7a38d55c8045\n  Path: /var/lib/heketi/mounts/vg_41b8a921f8e6d31cb04c7dd35b6b4cf2/brick_55851d8ab270112c07ab7a38d55c8045/brick\n  Size (GiB): 2\n  Node: bfc006b571e85a083118054233bfb16d\n  Device: 41b8a921f8e6d31cb04c7dd35b6b4cf2\n\n  Id: 67161e0e607c38677a0ef3f617b8dc1e\n  Path: /var/lib/heketi/mounts/vg_8ea71174529a35f41fc0d1b288da6299/brick_67161e0e607c38677a0ef3f617b8dc1e/brick\n  Size (GiB): 2\n  Node: 22cbcd136fa40ffe766a13f305cc1e3b\n  Device: 8ea71174529a35f41fc0d1b288da6299\n\n  Id: a8bf049dcea2d5245b64a792d4b85e6b\n  Path: /var/lib/heketi/mounts/vg_2a49883a5cb39c3b845477ff85a729ba/brick_a8bf049dcea2d5245b64a792d4b85e6b/brick\n  Size (GiB): 2\n  Node: c5979019ac13b9fe02f4e4e2dc6d62cb\n  Device: 2a49883a5cb39c3b845477ff85a729ba\n\n\nNodes:\n\nNode Id: 22cbcd136fa40ffe766a13f305cc1e3b\nState: online\nCluster Id: fb67f97166c58f161b85201e1fd9b8ed\nZone: 2\nManagement Hostname: node-2.lab\nStorage Hostname: 10.0.3.102\nDevices:\nId:8ea71174529a35f41fc0d1b288da6299   Name:/dev/xvdc           State:online    Size (GiB):49      Used (GiB):2       Free (GiB):47\n  Bricks:\n    Id:67161e0e607c38677a0ef3f617b8dc1e   Size (GiB):2       Path: /var/lib/heketi/mounts/vg_8ea71174529a35f41fc0d1b288da6299/brick_67161e0e607c38677a0ef3f617b8dc1e/brick\n\nNode Id: bfc006b571e85a083118054233bfb16d\nState: online\nCluster Id: fb67f97166c58f161b85201e1fd9b8ed\nZone: 3\nManagement Hostname: node-3.lab\nStorage Hostname: 10.0.4.103\nDevices:\nId:41b8a921f8e6d31cb04c7dd35b6b4cf2   Name:/dev/xvdc           State:online    Size (GiB):49      Used (GiB):2       Free (GiB):47\n  Bricks:\n    Id:55851d8ab270112c07ab7a38d55c8045   Size (GiB):2       Path: /var/lib/heketi/mounts/vg_41b8a921f8e6d31cb04c7dd35b6b4cf2/brick_55851d8ab270112c07ab7a38d55c8045/brick\n\nNode Id: c5979019ac13b9fe02f4e4e2dc6d62cb\nState: online\nCluster Id: fb67f97166c58f161b85201e1fd9b8ed\nZone: 1\nManagement Hostname: node-1.lab\nStorage Hostname: 10.0.2.101\nDevices:\nId:2a49883a5cb39c3b845477ff85a729ba   Name:/dev/xvdc           State:online    Size (GiB):49      Used (GiB):2       Free (GiB):47\n  Bricks:\n    Id:a8bf049dcea2d5245b64a792d4b85e6b   Size (GiB):2       Path: /var/lib/heketi/mounts/vg_2a49883a5cb39c3b845477ff85a729ba/brick_a8bf049dcea2d5245b64a792d4b85e6b/brick  This information should correspond to the topology.json file you supplied to the installer. With this we successfully verified the CNS deployment.", 
            "title": "Verifying the deployment"
        }, 
        {
            "location": "/module-3-using-cns/", 
            "text": "Overview\n\n\nIn this module you will use CNS as a developer would do in OpenShift. For that purpose you will dynamically provision storage both in standalone fashion and in context of an application deployment.\n\n\n\n\nCreating a StorageClass\n#\n\n\nOpenShift uses Kubernetes\n PersistentStorage facility to dynamically allocate storage of any kind for applications. This is a fairly simple framework in which only 3 components exists: the storage provider, the storage volume and the request for a storage volume.\n\n\n\n\nOpenShift knows non-ephemeral storage as \npersistent\n volumes. This is storage that is decoupled from pod lifecycles. Users can request such storage by submitting a \nPersistentVolumeClaim\n to the system, which carries aspects like desired capacity or access mode (shared, single, read-only).\n\n\nA storage provider in the system is represented by a \nStorageClass\n and is referenced in the claim. Upon receiving the claim it talks to the API of the actual storage system to provision the storage.  \n\n\nThe storage is represented in OpenShift as a \nPersistentVolume\n which can directly be used by pods to mount it.\n\n\nWith these basics defined we can configure our system for CNS. First we will set up the credentials for CNS in OpenShift.\n\n\n Make sure you are logged on as \noperator\n and you are in the \ndefault\n namespace:\n\n\noc whoami\n\n\n\n\n\n If you are not \noperator\n log in again to the default namespace\n\n\noc login -u operator -n default\n\n\n\n\n\n Create an encoded value for the CNS admin user like below:\n\n\necho -n \nmyS3cr3tpassw0rd\n | base64\n\n\n\n\n\nThe encoded string looks like this:\n\n\nbXlTM2NyM3RwYXNzdzByZA==\n\n\n\n\n\nWe will store this encoded value in an OpenShift secret.\n\n\n Create a file called \ncns-secret.yml\n with the as per below (highlight shows where to put encoded password):\n\n\ncns-secret.yml:\n\n\napiVersion\n:\n \nv1\n\n\nkind\n:\n \nSecret\n\n\nmetadata\n:\n\n  \nname\n:\n \ncns-secret\n\n  \nnamespace\n:\n \ndefault\n\n\ndata\n:\n\n\n  \nkey\n:\n \nbXlTM2NyM3RwYXNzdzByZA==\n\n\ntype\n:\n \nkubernetes.io/glusterfs\n\n\n\n\n\n\n Create the secret in OpenShift with the following command:\n\n\noc create -f cns-secret.yml\n\n\n\n\n\nTo represent CNS as a storage provider in the system you first have to create a StorageClass. Define by creating a file called \ncns-storageclass.yml\n which references the secret and the heketi URL shown earlier with the contents as below:\n\n\n\n\nImportant\n\n\nReplace the \nresturl\n parameter with your heketi URL.\n\n\n\n\n\n\ncns-storageclass.yml:\n\n\napiVersion\n:\n \nstorage.k8s.io/v1beta1\n\n\nkind\n:\n \nStorageClass\n\n\nmetadata\n:\n\n  \nname\n:\n \ncontainer-native-storage\n\n  \nannotations\n:\n\n    \nstorageclass.beta.kubernetes.io/is-default-class\n:\n \ntrue\n\n\nprovisioner\n:\n \nkubernetes.io/glusterfs\n\n\nparameters\n:\n\n  \nresturl\n:\n \nhttp://heketi-container-native-storage.cloudapps.34.252.58.209.nip.io\n\n  \nrestauthenabled\n:\n \ntrue\n\n  \nrestuser\n:\n \nadmin\n\n  \nvolumetype\n:\n \nreplicate:3\n\n  \nsecretNamespace\n:\n \ndefault\n\n  \nsecretName\n:\n \ncns-secret\n\n\n\n\n\n\n Create the StorageClass in OpenShift with the following command:\n\n\noc create -f cns-storageclass.yml\n\n\n\n\n\nWith these components in place the system is ready to dynamically provision storage capacity from Container-native Storage.\n\n\n\n\nRequesting Storage\n#\n\n\nTo get storage provisioned as a user you have to \nclaim\n storage. The \nPersistentVolumeClaim\n (PVC) basically acts a request to the system to provision storage with certain properties, like a specific capacity.\n\nAlso the access mode is set here, where \nReadWriteOnce\n allows one container at a time to mount this storage.\n\n\n Where are going to do this as a user. Login in as user \ndeveloper\n:\n\n\noc login -u developer\n\n\n\n\n\n If you no projects, create one:\n\n\noc new-project playground\n\n\n\n\n\n Create a claim by specifying a file called \ncns-pvc.yml\n with the following contents:\n\n\ncns-pvc.yml:\n\n\nkind\n:\n \nPersistentVolumeClaim\n\n\napiVersion\n:\n \nv1\n\n\nmetadata\n:\n\n  \nname\n:\n \nmy-container-storage\n\n  \nannotations\n:\n\n    \nvolume.beta.kubernetes.io/storage-class\n:\n \ncontainer-native-storage\n\n\nspec\n:\n\n  \naccessModes\n:\n\n  \n-\n \nReadWriteOnce\n\n  \nresources\n:\n\n    \nrequests\n:\n\n      \nstorage\n:\n \n10Gi\n\n\n\n\n\n\nWith above PVC we are requesting 10 GiB of non-shared storage. Instead of \nReadWriteOnce\n you could also have specified \nReadWriteOnly\n (for read-only) and \nReadWriteMany\n (for shared storage).\n\n\n Submit the PVC to the system like so:\n\n\noc create -f cns-pvc.yml\n\n\n\n\n\n After a couple of seconds, look at the requests state with the following command:\n\n\noc get pvc\n\n\n\n\n\nYou should see the PVC listed and in \nBound\n state.\n\n\nNAME                   STATUS    VOLUME                                     CAPACITY   ACCESSMODES   AGE\nmy-container-storage   Bound     pvc-382ac13d-4a9f-11e7-b56f-2cc2602a6dc8   10Gi       RWO           16s\n\n\n\n\n\n\n\nNote\n\n\nIt may take a couple seconds for the claim to be in \nbound\n.\n\n\n\n\n\n\nCaution\n\n\nIf the PVC is stuck in \nPENDING\n state you will need to investigate. Run \noc describe pvc/my-container-storage\n to see a more detailed explanation. Typically there are two root causes - the StorageClass is not properly setup (wrong name, wrong credentials, incorrect secret name, wrong heketi URL, heketi service not up, heketi pod not up\u2026) or the PVC is malformed (wrong StorageClass, name already taken \u2026)\n\n\n\n\n\n\nTip\n\n\nYou can also do this step with the UI. Log on as \ndeveloper\n and select or create a Project. Then go to the \nStorage\n tab. Select \nCreate\n storage and make selections accordingly to the PVC described before.\n\n\n\n\n\n\nWhen the claim was fulfilled successfully it is in the \nBound\n state. That means the system has successfully (via the StorageClass) reached out to the storage backend (in our case GlusterFS). The backend in turn provisioned the storage and provided a handle back OpenShift. In OpenShift the provisioned storage is then represented by a \nPersistentVolume\n (PV) which is \nbound\n to the PVC.\n\n\n Look at the PVC for these details:\n\n\noc describe pvc/my-container-storage\n\n\n\n\n\nThe details of the PVC show against which \nStorageClass\n it has been submitted and the name of the \nPersistentVolume\n which was generated to fulfill the claim.\n\n\nName:           my-container-storage\n\nNamespace:      container-native-storage\nStorageClass:   container-native-storage\nStatus:         Bound\n\nVolume:         pvc-382ac13d-4a9f-11e7-b56f-2cc2602a6dc8\n\nLabels:         \nnone\n\nCapacity:       10Gi\nAccess Modes:   RWO\nNo events.\n\n\n\n\n\n\n\nNote\n\n\nThe PV name will be different in your environment since it\u2019s automatically generated.\n\n\n\n\n Look at the corresponding PV by it\u2019s name:\n\n\noc describe pv/pvc-382ac13d-4a9f-11e7-b56f-2cc2602a6dc8\n\n\n\n\n\nThe output shows several interesting things, like the access mode (RWO = ReadWriteOnce), the reclaim policy (what happens when the PV object gets deleted), the capacity and the type of storage backing this PV (in our case GlusterFS as part of CNS):\n\n\nName:           pvc-382ac13d-4a9f-11e7-b56f-2cc2602a6dc8\nLabels:         \nnone\n\nStorageClass:   container-native-storage\n\nStatus:         Bound\n\nClaim:          container-native-storage/my-container-storage\n\nReclaim Policy: Delete\n\nAccess Modes:   RWO\n\nCapacity:       10Gi\n\nMessage:\nSource:\n\n    Type:               Glusterfs (a Glusterfs mount on the host that shares a pod\ns lifetime)\n\n    EndpointsName:      glusterfs-dynamic-my-container-storage\n    Path:               vol_304670f0d50bf5aa4717a69652bd48ff\n    ReadOnly:           false\nNo events.\n\n\n\n\n\n\n\nWhy is it called \nBound\n?\n\n\nOriginally PVs weren\nt automatically created. Hence in earlier documentation you may also find references about administrators actually \npre-provisioning\n PVs. Later PVCs would \npick up\n a suitable PV by looking at it\u2019s capacity. When successful they are \nbound\n to this PV.\n\nThis was needed for storage like NFS that does not have an API and therefore does not support \ndynamic provisioning\n. Hence it\ns called \nstatic provisioning\n.\n\nThis kind of storage should not be used anymore as it requires manual intervention, risky capacity planning and incurs inefficient storage utilization.\n\n\n\n\nLet\u2019s release this storage capacity again.\n\nStorage is freed up by deleting the \nPVC\n. The PVC controls the lifecycle of the storage, not the PV.\n\n\n\n\nImportant\n\n\nNever delete PVs that are dynamically provided. They are only handles for pods mounting the storage. Storage lifecycle is entirely controlled via PVCs.\n\n\n\n\n Delete the storage by deleting the PVC like this:\n\n\noc delete pvc/my-container-storage\n\n\n\n\n\n\n\nUsing non-shared storage for databases\n#\n\n\nNormally a user doesn\u2019t request storage with a PVC directly. Rather the PVC is integrated in a larger template that describe the entire application. Such examples ship with OpenShift out of the box.\n\n\n\n\nTip\n\n\nThe steps described in this section can again also be done with the UI. For this purpose follow these steps similar to the one in Module 1:\n\n\n\n\n\n\n\n\nLog on to the OpenShift UI as the \ndeveloper\n user\n\n\n\n\n\n\nIf you don\nt have a project, create one called \nmy-test-project\n, label and description is optional\n\n\n\n\n\n\nIn the Overview, next to the project\u2019s name select \nAdd to project\n\n\n\n\n\n\nIn the \nBrowse Catalog\n view select \nRuby\n from the list of programming languages\n\n\n\n\n\n\nSelect the example app entitled \nRails + PostgreSQL (Persistent)\n\n\n\n\n\n\n(optional) Change the \nVolume Capacity\n parameter to something greater than 1GiB, e.g. 15 GiB\n\n\n\n\n\n\nSelect \nCreate\n to start deploying the app\n\n\n\n\n\n\nSelect \nContinue to Overview\n in the confirmation screen\n\n\n\n\n\n\nWait for the application deployment to finish and continue below at\n\n\n\n\n\n\n\n\n\n\n\n\nTo create an application from the OpenShift Example templates on the CLI follow these steps.\n\n\n Create a new project with a name of your choice:\n\n\noc new-project my-test-project\n\n\n\n\n\nTo use the example applications that ship with OpenShift we can export and modify the template for a sample Ruby on Rails with PostgreSQL application. All these templates ship in pre-defined namespace called \nopenshift\n.\n\n\n Export the template from the \nopenshift\n namespace in YAML format:\n\n\noc export template/rails-pgsql-persistent -n openshift -o yaml \n rails-app-template.yml\n\n\n\n\n\nIn the file \nrails-app-template.yml\n you can now review the template for this entire application stack in all it\u2019s glory.\n\n\n\n\nWhat does the template file contain?\n\n\nIn essence it creates Ruby on Rails instance in a pod which functionality mimics a very basic blogging application. The blog articles are saved in a PostgreSQL database which runs in a separate pod.\n\nThe template describes all OpenShift resources necessary to stand up the rails pod and the postgres pod and make them accessible via services and routes.\n\nIn addition a PVC is issued (line 194) to supply this pod with persistent storage below the mount point \n/var/lib/pgsql/data\n (line 275).\n\n\n\n\nThe template contains a couple of parameters which default values we can override.\n\n\n\n\nTip\n\n\nTo list all available parameters from this template run \noc process -f rails-app-template.yml --parameters\n\nThe \noc process\n command parses the template and replaces any parameters with their default values if not supplied explicitly like in the next step.\n\n\n\n\nThere is a parameter in the template is called \nVOLUME_CAPACITY\n. It is used to customize the capacity in the PVC. We will process the template with the CLI client and override this parameter with a value of \n15Gi\n as follows:\n\n\n Render the template with the custom parameter value as follows:\n\n\noc process -f rails-app-template.yml -o yaml -p VOLUME_CAPACITY=15Gi \n my-rails-app.yml\n\n\n\n\n\nThe result \nmy-rails-app.yml\n file contains all resources including our custom PVC for this application.\n\n\n Deploy these resources like so:\n\n\noc create -f my-rails-app.yml\n\n\n\n\n\nAmong various OpenShift resource also our PVC will be created:\n\n\nsecret \nrails-pgsql-persistent\n created\nservice \nrails-pgsql-persistent\n created\nroute \nrails-pgsql-persistent\n created\nimagestream \nrails-pgsql-persistent\n created\nbuildconfig \nrails-pgsql-persistent\n created\ndeploymentconfig \nrails-pgsql-persistent\n created\n\npersistentvolumeclaim \npostgresql\n created\n\nservice \npostgresql\n created\ndeploymentconfig \npostgresql\n created\n\n\n\n\n\nYou can now use the OpenShift UI (while being logged in the newly created project) to follow the deployment process.\n\n\n Alternatively watch the containers deploy like this:\n\n\noc get pods -w\n\n\n\n\n\nThe complete output should look like this:\n\n\nNAME                             READY     STATUS              RESTARTS   AGE\npostgresql-1-deploy              0/1       ContainerCreating   0          11s\nrails-pgsql-persistent-1-build   0/1       ContainerCreating   0          11s\nNAME                  READY     STATUS    RESTARTS   AGE\npostgresql-1-deploy   1/1       Running   0          14s\npostgresql-1-81gnm   0/1       Pending   0         0s\npostgresql-1-81gnm   0/1       Pending   0         0s\nrails-pgsql-persistent-1-build   1/1       Running   0         19s\npostgresql-1-81gnm   0/1       Pending   0         15s\npostgresql-1-81gnm   0/1       ContainerCreating   0         16s\npostgresql-1-81gnm   0/1       Running   0         47s\npostgresql-1-81gnm   1/1       Running   0         4m\npostgresql-1-deploy   0/1       Completed   0         4m\npostgresql-1-deploy   0/1       Terminating   0         4m\npostgresql-1-deploy   0/1       Terminating   0         4m\nrails-pgsql-persistent-1-deploy   0/1       Pending   0         0s\nrails-pgsql-persistent-1-deploy   0/1       Pending   0         0s\nrails-pgsql-persistent-1-deploy   0/1       ContainerCreating   0         0s\nrails-pgsql-persistent-1-build   0/1       Completed   0         11m\nrails-pgsql-persistent-1-deploy   1/1       Running   0         6s\nrails-pgsql-persistent-1-hook-pre   0/1       Pending   0         0s\nrails-pgsql-persistent-1-hook-pre   0/1       Pending   0         0s\nrails-pgsql-persistent-1-hook-pre   0/1       ContainerCreating   0         0s\nrails-pgsql-persistent-1-hook-pre   1/1       Running   0         6s\nrails-pgsql-persistent-1-hook-pre   0/1       Completed   0         15s\nrails-pgsql-persistent-1-dkj7w   0/1       Pending   0         0s\nrails-pgsql-persistent-1-dkj7w   0/1       Pending   0         0s\nrails-pgsql-persistent-1-dkj7w   0/1       ContainerCreating   0         0s\nrails-pgsql-persistent-1-dkj7w   0/1       Running   0         1m\nrails-pgsql-persistent-1-dkj7w   1/1       Running   0         1m\nrails-pgsql-persistent-1-deploy   0/1       Completed   0         1m\nrails-pgsql-persistent-1-deploy   0/1       Terminating   0         1m\nrails-pgsql-persistent-1-deploy   0/1       Terminating   0         1m\nrails-pgsql-persistent-1-hook-pre   0/1       Terminating   0         1m\nrails-pgsql-persistent-1-hook-pre   0/1       Terminating   0         1m\n\n\n\n\n\nExit out of the watch mode with: \nCtrl\n + \nc\n\n\n\n\nNote\n\n\nIt may take up to 5 minutes for the deployment to complete.\n\n\nIf you did it via the UI the deployment is finished when both, rails app and postgres database are up and running:\n\n\n\n\n\n\nYou should also see a PVC being issued and in the \nBound\n state.\n\n\n Look at the PVC created:\n\n\noc get pvc/postgresql\n\n\n\n\n\nOutput:\n\n\nNAME         STATUS    VOLUME                                     CAPACITY   ACCESSMODES   AGE\npostgresql   Bound     pvc-6c348fbb-4e9d-11e7-970e-0a9938370404   15Gi       RWO           4m\n\n\n\n\n\n\n\nWhy did this even work?\n\n\nIf you paid close attention you likely noticed that the PVC in the template does not specify a particular \nStorageClass\n. This still yields a PV deployed because our \nStorageClass\n has actually been defined as the system-wide default. PVCs that don\nt specify a \nStorageClass\n will use the default class.\n\n\n\n\nNow go ahead and try out the application. The overview page in the OpenShift UI will tell you the \nroute\n which has been deployed as well. Use it and append \n/articles\n to the URL to get to the actual app.\n\n\n Otherwise get it on the CLI like this:\n\n\noc get route\n\n\n\n\n\nOutput:\n\n\nNAME                     HOST/PORT                                                               PATH      SERVICES                 PORT      TERMINATION   WILDCARD\nrails-pgsql-persistent   rails-pgsql-persistent-my-test-project.cloudapps.34.252.58.209.nip.io             rails-pgsql-persistent   \nall\n                   None\n\n\n\n\n\n\n\nNote\n\n\nAgain, the URL will be slightly different for you.\n\n\n\n\nFollowing this output, point your browser to the URL and append \n/articles\n to reach the actual application, in this case:\n\n\nhttp://\nrails-pgsql-persistent-my-test-project.cloudapps.34.252.58.209.nip.io\n/\narticles\n\n\nYou should be able to successfully create articles and comments. The username/password to create articles and comments is by default \nopenshift\n/\nsecret\n.\n\nWhen they are saved they are actually saved in the PostgreSQL database which stores it\u2019s table spaces on a GlusterFS volume provided by CNS.\n\n\nNow let\u2019s take a look at how this was actually achieved.\n\n\n A normal user cannot see the details of a PersistentVolume. Log in as \noperator\n:\n\n\noc login -u operator\n\n\n\n\n\n Look at the PVC to determine the PV:\n\n\noc get pvc\n\n\n\n\n\nOutput:\n\n\nNAME         STATUS    VOLUME                                     CAPACITY   ACCESSMODES   AGE\npostgresql   Bound     pvc-6c348fbb-4e9d-11e7-970e-0a9938370404   15Gi       RWO           10m\n\n\n\n\n\n\n\nNote\n\n\nYour volume (PV) name will be different as it\u2019s dynamically generated.\n\n\n\n\n Look at the details of this PV:\n\n\noc describe pv/pvc-6c348fbb-4e9d-11e7-970e-0a9938370404\n\n\n\n\n\nOutput shows (in highlight) the name of the volume, the backend type (GlusterFS) and the volume name GlusterFS uses internally.\n\n\nName:       pvc-6c348fbb-4e9d-11e7-970e-0a9938370404\n\nLabels:     \nnone\n\nStorageClass:   container-native-storage\nStatus:     Bound\nClaim:      my-test-project/postgresql\nReclaim Policy: Delete\nAccess Modes:   RWO\nCapacity:   15Gi\nMessage:\nSource:\n\n    Type:       Glusterfs (a Glusterfs mount on the host that shares a pod\ns lifetime)\n\n    EndpointsName:  glusterfs-dynamic-postgresql\n\n    Path:       vol_efac3ddb9d339fa680c0807a1d91c5a3\n\n    ReadOnly:       false\nNo events.\n\n\n\n\n\nNote the GlusterFS volume name, in this case \nvol_e8fe7f46fedf7af7628feda0dcbf2f60\n.\n\n\n Now let\u2019s switch to the namespace we used for CNS deployment:\n\n\noc project container-native-storage\n\n\n\n\n\n Look at the GlusterFS pods running\n\n\noc get pods -o wide\n\n\n\n\n\nPick one of the GlusterFS pods by name (which one is not important):\n\n\nNAME              READY     STATUS    RESTARTS   AGE       IP           NODE\nglusterfs-5rc2g   1/1       Running   0          51m       10.0.2.101   node-1.lab\nglusterfs-jbvdk   1/1       Running   0          51m       10.0.3.102   node-2.lab\nglusterfs-rchtr   1/1       Running   0          51m       10.0.4.103   node-3.lab\nheketi-1-tn0s9    1/1       Running   0          49m       10.130.2.3   node-6.lab\n\n\n\n\n\nRemember the IP address\n of the pod you select. In this case \n10.0.2.101\n.\n\n\n Log on to GlusterFS pod with a remote terminal session like so:\n\n\noc rsh glusterfs-5rc2g\n\n\n\n\n\nYou will end up in shell session in the container with root privileges.\n\n\nsh-4.2#\n\n\n\n\n\nYou have now access to this container\u2019s process and filesystem namespace which has the GlusterFS CLI utilities installed.\n\n\n Let\u2019s list all known volumes:\n\n\nsh-4.2# gluster volume list\n\n\n\n\n\nYou will see two volumes:\n\n\nheketidbstorage\nvol_efac3ddb9d339fa680c0807a1d91c5a3\n\n\n\n\n\n\n\n\n\nheketidbstorage\n is a internal-only volume dedicated to heketi\u2019s internal database.\n\n\n\n\n\n\nvol_efac3ddb9d339fa680c0807a1d91c5a3\n is the volume backing the PV of the PostgreSQL database deployed earlier.\n\n\n\n\n\n\n Interrogate GlusterFS about the topology of this volume:\n\n\nsh-4.2# gluster volume info vol_efac3ddb9d339fa680c0807a1d91c5a3\n\n\n\n\n\nThe output will show you how the volume has been created. You will also see that the pod you are currently logged on serves one the bricks (in highlight).\n\n\nVolume Name: vol_efac3ddb9d339fa680c0807a1d91c5a3\nType: Replicate\nVolume ID: cfaccdec-3c97-4e43-b80f-c9677e7a726a\nStatus: Started\nSnapshot Count: 0\nNumber of Bricks: 1 x 3 = 3\nTransport-type: tcp\nBricks:\nBrick1: 10.0.3.102:/var/lib/heketi/mounts/vg_8ea71174529a35f41fc0d1b288da6299/brick_b2e6975e246d896038604a7c0efcd83f/brick\n\nBrick2: 10.0.2.101:/var/lib/heketi/mounts/vg_2a49883a5cb39c3b845477ff85a729ba/brick_c5b00eeae2c57862b4eddeeb9b3903ad/brick\n\nBrick3: 10.0.4.103:/var/lib/heketi/mounts/vg_41b8a921f8e6d31cb04c7dd35b6b4cf2/brick_4f691eb2ba90a3ee31cb882f12786400/brick\nOptions Reconfigured:\ntransport.address-family: inet\nperformance.readdir-ahead: on\nnfs.disable: on\n\n\n\n\n\n\n\nNote\n\n\nIdentify the right brick by looking at the host IP of the pod you have just logged on to. \noc get pods -o wide\n will give you this information.\n\n\n\n\nGlusterFS created this volume as a 3-way replica set across all GlusterFS pods, therefore across all your OpenShift App nodes running CNS. This is currently the only supported volume type in production. Later you will see how can schedule (unsupported) volume types like dispersed or distributed.\n\n\n You can even look at the local brick:\n\n\nsh-4.2# ls -ahl /var/lib/heketi/mounts/vg_2a49883a5cb39c3b845477ff85a729ba/brick_c5b00eeae2c57862b4eddeeb9b3903ad/brick\ntotal 16K\ndrwxrwsr-x.   5 root       2001   57 Jun  6 14:44 .\ndrwxr-xr-x.   3 root       root   19 Jun  6 14:44 ..\ndrw---S---. 263 root       2001 8.0K Jun  6 14:46 .glusterfs\ndrwxr-sr-x.   3 root       2001   25 Jun  6 14:44 .trashcan\ndrwx------.  20 1000080000 2001 8.0K Jun  6 14:46 userdata\n\nsh-4.2# ls -ahl /var/lib/heketi/mounts/vg_2a49883a5cb39c3b845477ff85a729ba/brick_c5b00eeae2c57862b4eddeeb9b3903ad/brick/userdata\n\ntotal 68K\ndrwx------. 20 1000080000 2001 8.0K Jun  6 14:46 .\ndrwxrwsr-x.  5 root       2001   57 Jun  6 14:44 ..\n-rw-------.  2 1000080000 root    4 Jun  6 14:44 PG_VERSION\ndrwx------.  6 1000080000 root   54 Jun  6 14:46 base\ndrwx------.  2 1000080000 root 8.0K Jun  6 14:47 global\ndrwx------.  2 1000080000 root   18 Jun  6 14:44 pg_clog\ndrwx------.  2 1000080000 root    6 Jun  6 14:44 pg_commit_ts\ndrwx------.  2 1000080000 root    6 Jun  6 14:44 pg_dynshmem\n-rw-------.  2 1000080000 root 4.6K Jun  6 14:46 pg_hba.conf\n-rw-------.  2 1000080000 root 1.6K Jun  6 14:44 pg_ident.conf\ndrwx------.  2 1000080000 root   32 Jun  6 14:46 pg_log\ndrwx------.  4 1000080000 root   39 Jun  6 14:44 pg_logical\ndrwx------.  4 1000080000 root   36 Jun  6 14:44 pg_multixact\ndrwx------.  2 1000080000 root   18 Jun  6 14:46 pg_notify\ndrwx------.  2 1000080000 root    6 Jun  6 14:44 pg_replslot\ndrwx------.  2 1000080000 root    6 Jun  6 14:44 pg_serial\ndrwx------.  2 1000080000 root    6 Jun  6 14:44 pg_snapshots\ndrwx------.  2 1000080000 root    6 Jun  6 14:46 pg_stat\ndrwx------.  2 1000080000 root   84 Jun  6 15:16 pg_stat_tmp\ndrwx------.  2 1000080000 root   18 Jun  6 14:44 pg_subtrans\ndrwx------.  2 1000080000 root    6 Jun  6 14:44 pg_tblspc\ndrwx------.  2 1000080000 root    6 Jun  6 14:44 pg_twophase\ndrwx------.  3 1000080000 root   60 Jun  6 14:44 pg_xlog\n-rw-------.  2 1000080000 root   88 Jun  6 14:44 postgresql.auto.conf\n-rw-------.  2 1000080000 root  21K Jun  6 14:46 postgresql.conf\n-rw-------.  2 1000080000 root   46 Jun  6 14:46 postmaster.opts\n-rw-------.  2 1000080000 root   89 Jun  6 14:46 postmaster.pid\n\n\n\n\n\n\n\nNote\n\n\nThe exact path name will be different in your environment as it has been automatically generated.\n\n\n\n\nYou are looking at the PostgreSQL internal data file structure from the perspective of the GlusterFS server side. Evidence that the database uses CNS.\n\n\nClients, like the OpenShift nodes and their application pods talk to this storage with the GlusterFS protocol as it where an ordinary GlusterFS deployment.\n\nWhen a pod starts that mounts storage from a PV backed by CNS the GlusterFS mount plugin in OpenShift will mount the GlusterFS volume on the right App Node and then \nbind-mount\n this directory to the right pod.\n\nThis is happen transparently to the application and looks like a normal local filesystem inside the pod.\n\n\n You may exit your remote session to the GlusterFS pod.\n\n\nsh-4.2# exit\n\n\n\n\n\n\n\nProviding shared storage to multiple application instances\n#\n\n\nIn the previous example we provisioned an RWO PV - the volume is only usable with one pod at a time. RWO is what most of the OpenShift storage backends support.\n\nSo far only very few options, like the basic NFS support existed, to provide a PersistentVolume to more than one container at once. The access mode used for this is \nReadWriteMany\n.\n\n\nWith CNS this capabilities is now available to all OpenShift deployments, no matter where they are deployed. To demonstrate this capability with an application we will deploy a PHP-based file uploader that has multiple front-end instances sharing a common storage repository.\n\n\n Log back in as developer\n\n\noc login -u developer\n\n\n\n\n\n First make sure you are still in the example project created earlier\n\n\noc project my-test-project\n\n\n\n\n\n Next deploy the example application:\n\n\noc new-app openshift/php:7.0~https://github.com/christianh814/openshift-php-upload-demo --name=file-uploader\n\n\n\n\n\n\n\nNote\n\n\nThis is yet another way to build and launch an application from source code in OpenShift. The content before the ~ is the name of a Source-to-Image builder (a container that knows how to build applications of a certain type from source, in this case PHP) and the URL following is a GitHub repository hosting the source code.\n\nFeel free to check it out.\n\n\n\n\nOutput:\n\n\n--\n Found image a1ebebb (6 weeks old) in image stream \nopenshift/php\n under tag \n7.0\n for \nopenshift/php:7.0\n\n\n    Apache 2.4 with PHP 7.0\n    -----------------------\n    Platform for building and running PHP 7.0 applications\n\n    Tags: builder, php, php70, rh-php70\n\n    * A source build using source code from https://github.com/christianh814/openshift-php-upload-demo will be created\n      * The resulting image will be pushed to image stream \nfile-uploader:latest\n\n      * Use \nstart-build\n to trigger a new build\n    * This image will be deployed in deployment config \nfile-uploader\n\n    * Port 8080/tcp will be load balanced by service \nfile-uploader\n\n      * Other containers can access this service through the hostname \nfile-uploader\n\n\n--\n Creating resources ...\n    imagestream \nfile-uploader\n created\n    buildconfig \nfile-uploader\n created\n    deploymentconfig \nfile-uploader\n created\n    service \nfile-uploader\n created\n--\n Success\n    Build scheduled, use \noc logs -f bc/file-uploader\n to track its progress.\n    Run \noc status\n to view your app.\n\n\n\n\n\n Wait for the application to be deployed with the suggest command:\n\n\noc logs -f bc/file-uploader\n\n\n\n\n\nThe follow-mode of the above command ends automatically when the build is successful and you return to your shell.\n\n\n...\nCloning \nhttps://github.com/christianh814/openshift-php-upload-demo\n ...\n        Commit: 7508da63d78b4abc8d03eac480ae930beec5d29d (Update index.html)\n        Author: Christian Hernandez \nchristianh814@users.noreply.github.com\n\n        Date:   Thu Mar 23 09:59:38 2017 -0700\n---\n Installing application source...\nPushing image 172.30.120.134:5000/my-test-project/file-uploader:latest ...\nPushed 0/5 layers, 2% complete\nPushed 1/5 layers, 20% complete\nPushed 2/5 layers, 40% complete\nPush successful\n\n\n\n\n\n When the build is completed ensure the pods are running:\n\n\noc get pods\n\n\n\n\n\nAmong your existing pods you should see new pods running.\n\n\nNAME                             READY     STATUS      RESTARTS   AGE\nfile-uploader-1-build            0/1       Completed   0          2m\nfile-uploader-1-k2v0d            1/1       Running     0          1m\n...\n\n\n\n\n\nA service has been created for our app but not exposed yet.\n\n\n Let\u2019s fix this:\n\n\noc expose svc/file-uploader\n\n\n\n\n\n Check the route that has been created:\n\n\noc get route/file-uploader\n\n\n\n\n\nThe route forwards all traffic to port 8080 of the container running the app.\n\n\nNAME            HOST/PORT                                                      PATH      SERVICES        PORT       TERMINATION   WILDCARD\nfile-uploader   file-uploader-my-test-project.cloudapps.34.252.58.209.nip.io             file-uploader   8080-tcp                 None\n\n\n\n\n\nPoint your browser the the URL advertised by the route (in this case http://file-uploader-my-test-project.cloudapps.34.252.58.209.nip.io)\n\n\nThe application simply lists all file previously uploaded and offers the ability to upload new ones as well as download the existing data. Right now there is nothing.\n\n\nSelect an arbitrary from your local system and upload it to the app.\n\n\n\n\nAfter uploading a file validate it has been stored locally in the container by following the link \nList Uploaded Files\n in the browser.\n\n\nLet\ns see how this is stored locally in the container.\n\n\n List the running pods of our application:\n\n\noc get pods | grep file-uploader\n\n\n\n\n\nYou will see two entries:\n\n\nfile-uploader-1-build            0/1       Completed   0          7m\nfile-uploader-1-k2v0d            1/1       Running     0          6m\n\n\n\n\n\nNote the name of the single pod currently running the app: \nfile-uploader-1-k2v0d\n.\n\nThe container called \nfile-uploader-1-build\n is the builder container that deployed the application and it has already terminated.\n\n\n Log into the application pod via a remote session (using the name noted earlier):\n\n\noc rsh file-uploader-1-k2v0d\n\n\n\n\n\nIn the container explore the directory in which the uploaded files will be stored.\n\n\nsh-4.2$ cd uploaded\nsh-4.2$ pwd\n/opt/app-root/src/uploaded\nsh-4.2$ ls -lh\ntotal 16K\n-rw-r--r--. 1 1000080000 root 16K May 26 09:32 cns-deploy-4.0.0-15.el7rhgs.x86_64.rpm.gz\n\n\n\n\n\n\n\nNote\n\n\nThe exact name of the pod will be different in your environment.\n\n\n\n\nThe app should also list the file in the overview:\n\n\n\n\nThis pod currently does not use any persistent storage. It stores the file locally.\n\n\n\n\nImportant\n\n\nNever store data in a pod. It\u2019s ephemeral by definition and will be lost as soon as the pod terminates.\n\n\n\n\nLet\u2019s see when this become a problem.\n\n\n Exit out of the container shell:\n\n\nsh-4.2$ exit\n\n\n\n\n\n Let\u2019s scale the deployment to 3 instances of the app:\n\n\noc scale dc/file-uploader --replicas=3\n\n\n\n\n\n Watch the additional pods getting spawned:\n\n\noc get pods\n\n\n\n\n\nYou will see 2 additional pods being spawned:\n\n\nNAME                             READY     STATUS      RESTARTS   AGE\nfile-uploader-1-3cgh1            1/1       Running     0          20s\nfile-uploader-1-3hckj            1/1       Running     0          20s\nfile-uploader-1-build            0/1       Completed   0          4m\nfile-uploader-1-k2v0d            1/1       Running     0          3m\n...\n\n\n\n\n\n\n\nNote\n\n\nThe pod names will be different in your environment since they are automatically generated.\n\n\n\n\nThese 3 pods now make up our application. OpenShift will load balance incoming traffic between them.\n\nHowever, when you log on to one of the new instances you will see they have no data.\n\n\n Log on to one of the new containers:\n\n\noc rsh file-uploader-1-3cgh1\n\n\n\n\n\n Again check the upload directory:\n\n\nsh-4.2$ cd uploaded\nsh-4.2$ pwd\n/opt/app-root/src/uploaded\nsh-4.2$ ls -hl\ntotal 0\n\n\n\n\n\nIt\ns empty because the previously uploaded files were stored locally in the first container and are not available to the others.\n\nSimilarly, other users of the app will sometimes see your uploaded files and sometimes not. With the deployment scaled to 3 instances OpenShifts router will simply round-robin across them. Whenever the load balancing service in OpenShift points to the pod that has the file stored locally users will see it or not. You can simulate this with another instance of your browser in \nIncognito mode\n pointing to your app.\n\n\nThe app is of course not usable like this. We can fix this by providing shared storage to this app.\n\n\n First create a PVC with the appropriate setting in a file called \ncns-rwx-pvc.yml\n with below contents:\n\n\ncns-rwx-pvc.yml:\n\n\nkind\n:\n \nPersistentVolumeClaim\n\n\napiVersion\n:\n \nv1\n\n\nmetadata\n:\n\n  \nname\n:\n \nmy-shared-storage\n\n  \nannotations\n:\n\n    \nvolume.beta.kubernetes.io/storage-class\n:\n \ncontainer-native-storage\n\n\nspec\n:\n\n  \naccessModes\n:\n\n\n  \n-\n \nReadWriteMany\n\n\n  \nresources\n:\n\n    \nrequests\n:\n\n      \nstorage\n:\n \n10Gi\n\n\n\n\n\n\nNotice the access mode explicitly requested to be \nReadWriteMany\n (also referred to as RWX). Storage provisioned like this can be mounted by multiple containers on multiple hosts at the same time.\n\n\n Submit the request to the system:\n\n\noc create -f cns-rwx-pvc.yml\n\n\n\n\n\n Let\u2019s look at the result:\n\n\noc get pvc\n\n\n\n\n\nACCESSMODES\n is set to \nRWX\n:\n\n\nNAME                STATUS    VOLUME                                     CAPACITY   ACCESSMODES   AGE\nmy-shared-storage   Bound     pvc-62aa4dfe-4ad2-11e7-b56f-2cc2602a6dc8   10Gi       RWX           22s\n...\n\n\n\n\n\nWe can now update the \nDeploymentConfig\n of our application to use this PVC to provide the application with persistent, shared storage for uploads.\n\n\n Update the configuration of the application by adding a volume claim like this:\n\n\noc volume dc/file-uploader --add --name=shared-storage --type=persistentVolumeClaim --claim-name=my-shared-storage --mount-path=/opt/app-root/src/uploaded\n\n\n\n\n\nOur app will now re-deploy (in a rolling fashion) with the new settings - all pods will mount the volume identified by the PVC under /opt/app-root/src/upload (the path is predictable so we can hard-code it here).\n\n\n You can watch it like this:\n\n\noc logs dc/file-uploader -f\n\n\n\n\n\nThe new \nDeploymentConfig\n will supersede the old one.\n\n\n--\n Scaling up file-uploader-2 from 0 to 3, scaling down file-uploader-1 from 3 to 0 (keep 3 pods available, don\nt exceed 4 pods)\n    Scaling file-uploader-2 up to 1\n    Scaling file-uploader-1 down to 2\n    Scaling file-uploader-2 up to 2\n    Scaling file-uploader-1 down to 1\n    Scaling file-uploader-2 up to 3\n    Scaling file-uploader-1 down to 0\n--\n Success\n\n\n\n\n\nThe new config \nfile-uploader-2\n will have 3 pods all sharing the same storage.\n\n\n Get the names of the new pods:\n\n\noc get pods\n\n\n\n\n\nOutput:\n\n\nNAME                             READY     STATUS      RESTARTS   AGE\nfile-uploader-1-build            0/1       Completed   0          18m\nfile-uploader-2-jd22b            1/1       Running     0          1m\nfile-uploader-2-kw9lq            1/1       Running     0          2m\nfile-uploader-2-xbz24            1/1       Running     0          1m\n...\n\n\n\n\n\nTry it out in your application: upload new files and watch them being visible from within all application pods. In new browser sessions, simulating other users, the application behaves normally as it circles through the pods between browser requests.\n\n\n[root@master ~]# oc rsh file-uploader-2-jd22b\nsh-4.2$ ls -lh uploaded\ntotal 16K\n-rw-r--r--. 1 1000080000 root 16K May 26 10:21 cns-deploy-4.0.0-15.el7rhgs.x86_64.rpm.gz\nsh-4.2$ exit\nexit\n[root@master ~]# oc rsh file-uploader-2-kw9lq\nsh-4.2$ ls -lh uploaded\n-rw-r--r--. 1 1000080000 root 16K May 26 10:21 cns-deploy-4.0.0-15.el7rhgs.x86_64.rpm.gz\nsh-4.2$ exit\nexit\n[root@master ~]# oc rsh file-uploader-2-xbz24\nsh-4.2$ ls -lh uploaded\n-rw-r--r--. 1 1000080000 root 16K May 26 10:21 cns-deploy-4.0.0-15.el7rhgs.x86_64.rpm.gz\nsh-4.2$ exit\n\n\n\n\n\nThat\u2019s it. You have successfully provided shared storage to pods throughout the entire system, therefore avoiding the need for data to be replicated at the application level to each pod.\n\n\nWith CNS this is available wherever OpenShift is deployed with no external dependency.", 
            "title": "Module 3 - Using Persistent Storage"
        }, 
        {
            "location": "/module-3-using-cns/#creating-a-storageclass", 
            "text": "OpenShift uses Kubernetes  PersistentStorage facility to dynamically allocate storage of any kind for applications. This is a fairly simple framework in which only 3 components exists: the storage provider, the storage volume and the request for a storage volume.   OpenShift knows non-ephemeral storage as  persistent  volumes. This is storage that is decoupled from pod lifecycles. Users can request such storage by submitting a  PersistentVolumeClaim  to the system, which carries aspects like desired capacity or access mode (shared, single, read-only).  A storage provider in the system is represented by a  StorageClass  and is referenced in the claim. Upon receiving the claim it talks to the API of the actual storage system to provision the storage.    The storage is represented in OpenShift as a  PersistentVolume  which can directly be used by pods to mount it.  With these basics defined we can configure our system for CNS. First we will set up the credentials for CNS in OpenShift.   Make sure you are logged on as  operator  and you are in the  default  namespace:  oc whoami   If you are not  operator  log in again to the default namespace  oc login -u operator -n default   Create an encoded value for the CNS admin user like below:  echo -n  myS3cr3tpassw0rd  | base64  The encoded string looks like this:  bXlTM2NyM3RwYXNzdzByZA==  We will store this encoded value in an OpenShift secret.   Create a file called  cns-secret.yml  with the as per below (highlight shows where to put encoded password):  cns-secret.yml:  apiVersion :   v1  kind :   Secret  metadata : \n   name :   cns-secret \n   namespace :   default  data :     key :   bXlTM2NyM3RwYXNzdzByZA==  type :   kubernetes.io/glusterfs    Create the secret in OpenShift with the following command:  oc create -f cns-secret.yml  To represent CNS as a storage provider in the system you first have to create a StorageClass. Define by creating a file called  cns-storageclass.yml  which references the secret and the heketi URL shown earlier with the contents as below:   Important  Replace the  resturl  parameter with your heketi URL.    cns-storageclass.yml:  apiVersion :   storage.k8s.io/v1beta1  kind :   StorageClass  metadata : \n   name :   container-native-storage \n   annotations : \n     storageclass.beta.kubernetes.io/is-default-class :   true  provisioner :   kubernetes.io/glusterfs  parameters : \n   resturl :   http://heketi-container-native-storage.cloudapps.34.252.58.209.nip.io \n   restauthenabled :   true \n   restuser :   admin \n   volumetype :   replicate:3 \n   secretNamespace :   default \n   secretName :   cns-secret    Create the StorageClass in OpenShift with the following command:  oc create -f cns-storageclass.yml  With these components in place the system is ready to dynamically provision storage capacity from Container-native Storage.", 
            "title": "Creating a StorageClass"
        }, 
        {
            "location": "/module-3-using-cns/#requesting-storage", 
            "text": "To get storage provisioned as a user you have to  claim  storage. The  PersistentVolumeClaim  (PVC) basically acts a request to the system to provision storage with certain properties, like a specific capacity. \nAlso the access mode is set here, where  ReadWriteOnce  allows one container at a time to mount this storage.   Where are going to do this as a user. Login in as user  developer :  oc login -u developer   If you no projects, create one:  oc new-project playground   Create a claim by specifying a file called  cns-pvc.yml  with the following contents:  cns-pvc.yml:  kind :   PersistentVolumeClaim  apiVersion :   v1  metadata : \n   name :   my-container-storage \n   annotations : \n     volume.beta.kubernetes.io/storage-class :   container-native-storage  spec : \n   accessModes : \n   -   ReadWriteOnce \n   resources : \n     requests : \n       storage :   10Gi   With above PVC we are requesting 10 GiB of non-shared storage. Instead of  ReadWriteOnce  you could also have specified  ReadWriteOnly  (for read-only) and  ReadWriteMany  (for shared storage).   Submit the PVC to the system like so:  oc create -f cns-pvc.yml   After a couple of seconds, look at the requests state with the following command:  oc get pvc  You should see the PVC listed and in  Bound  state.  NAME                   STATUS    VOLUME                                     CAPACITY   ACCESSMODES   AGE\nmy-container-storage   Bound     pvc-382ac13d-4a9f-11e7-b56f-2cc2602a6dc8   10Gi       RWO           16s   Note  It may take a couple seconds for the claim to be in  bound .    Caution  If the PVC is stuck in  PENDING  state you will need to investigate. Run  oc describe pvc/my-container-storage  to see a more detailed explanation. Typically there are two root causes - the StorageClass is not properly setup (wrong name, wrong credentials, incorrect secret name, wrong heketi URL, heketi service not up, heketi pod not up\u2026) or the PVC is malformed (wrong StorageClass, name already taken \u2026)    Tip  You can also do this step with the UI. Log on as  developer  and select or create a Project. Then go to the  Storage  tab. Select  Create  storage and make selections accordingly to the PVC described before.    When the claim was fulfilled successfully it is in the  Bound  state. That means the system has successfully (via the StorageClass) reached out to the storage backend (in our case GlusterFS). The backend in turn provisioned the storage and provided a handle back OpenShift. In OpenShift the provisioned storage is then represented by a  PersistentVolume  (PV) which is  bound  to the PVC.   Look at the PVC for these details:  oc describe pvc/my-container-storage  The details of the PVC show against which  StorageClass  it has been submitted and the name of the  PersistentVolume  which was generated to fulfill the claim.  Name:           my-container-storage Namespace:      container-native-storage\nStorageClass:   container-native-storage\nStatus:         Bound Volume:         pvc-382ac13d-4a9f-11e7-b56f-2cc2602a6dc8 Labels:          none \nCapacity:       10Gi\nAccess Modes:   RWO\nNo events.   Note  The PV name will be different in your environment since it\u2019s automatically generated.    Look at the corresponding PV by it\u2019s name:  oc describe pv/pvc-382ac13d-4a9f-11e7-b56f-2cc2602a6dc8  The output shows several interesting things, like the access mode (RWO = ReadWriteOnce), the reclaim policy (what happens when the PV object gets deleted), the capacity and the type of storage backing this PV (in our case GlusterFS as part of CNS):  Name:           pvc-382ac13d-4a9f-11e7-b56f-2cc2602a6dc8\nLabels:          none \nStorageClass:   container-native-storage Status:         Bound Claim:          container-native-storage/my-container-storage Reclaim Policy: Delete Access Modes:   RWO Capacity:       10Gi Message:\nSource:     Type:               Glusterfs (a Glusterfs mount on the host that shares a pod s lifetime)     EndpointsName:      glusterfs-dynamic-my-container-storage\n    Path:               vol_304670f0d50bf5aa4717a69652bd48ff\n    ReadOnly:           false\nNo events.   Why is it called  Bound ?  Originally PVs weren t automatically created. Hence in earlier documentation you may also find references about administrators actually  pre-provisioning  PVs. Later PVCs would  pick up  a suitable PV by looking at it\u2019s capacity. When successful they are  bound  to this PV. \nThis was needed for storage like NFS that does not have an API and therefore does not support  dynamic provisioning . Hence it s called  static provisioning . \nThis kind of storage should not be used anymore as it requires manual intervention, risky capacity planning and incurs inefficient storage utilization.   Let\u2019s release this storage capacity again. \nStorage is freed up by deleting the  PVC . The PVC controls the lifecycle of the storage, not the PV.   Important  Never delete PVs that are dynamically provided. They are only handles for pods mounting the storage. Storage lifecycle is entirely controlled via PVCs.    Delete the storage by deleting the PVC like this:  oc delete pvc/my-container-storage", 
            "title": "Requesting Storage"
        }, 
        {
            "location": "/module-3-using-cns/#using-non-shared-storage-for-databases", 
            "text": "Normally a user doesn\u2019t request storage with a PVC directly. Rather the PVC is integrated in a larger template that describe the entire application. Such examples ship with OpenShift out of the box.   Tip  The steps described in this section can again also be done with the UI. For this purpose follow these steps similar to the one in Module 1:     Log on to the OpenShift UI as the  developer  user    If you don t have a project, create one called  my-test-project , label and description is optional    In the Overview, next to the project\u2019s name select  Add to project    In the  Browse Catalog  view select  Ruby  from the list of programming languages    Select the example app entitled  Rails + PostgreSQL (Persistent)    (optional) Change the  Volume Capacity  parameter to something greater than 1GiB, e.g. 15 GiB    Select  Create  to start deploying the app    Select  Continue to Overview  in the confirmation screen    Wait for the application deployment to finish and continue below at       To create an application from the OpenShift Example templates on the CLI follow these steps.   Create a new project with a name of your choice:  oc new-project my-test-project  To use the example applications that ship with OpenShift we can export and modify the template for a sample Ruby on Rails with PostgreSQL application. All these templates ship in pre-defined namespace called  openshift .   Export the template from the  openshift  namespace in YAML format:  oc export template/rails-pgsql-persistent -n openshift -o yaml   rails-app-template.yml  In the file  rails-app-template.yml  you can now review the template for this entire application stack in all it\u2019s glory.   What does the template file contain?  In essence it creates Ruby on Rails instance in a pod which functionality mimics a very basic blogging application. The blog articles are saved in a PostgreSQL database which runs in a separate pod. \nThe template describes all OpenShift resources necessary to stand up the rails pod and the postgres pod and make them accessible via services and routes. \nIn addition a PVC is issued (line 194) to supply this pod with persistent storage below the mount point  /var/lib/pgsql/data  (line 275).   The template contains a couple of parameters which default values we can override.   Tip  To list all available parameters from this template run  oc process -f rails-app-template.yml --parameters \nThe  oc process  command parses the template and replaces any parameters with their default values if not supplied explicitly like in the next step.   There is a parameter in the template is called  VOLUME_CAPACITY . It is used to customize the capacity in the PVC. We will process the template with the CLI client and override this parameter with a value of  15Gi  as follows:   Render the template with the custom parameter value as follows:  oc process -f rails-app-template.yml -o yaml -p VOLUME_CAPACITY=15Gi   my-rails-app.yml  The result  my-rails-app.yml  file contains all resources including our custom PVC for this application.   Deploy these resources like so:  oc create -f my-rails-app.yml  Among various OpenShift resource also our PVC will be created:  secret  rails-pgsql-persistent  created\nservice  rails-pgsql-persistent  created\nroute  rails-pgsql-persistent  created\nimagestream  rails-pgsql-persistent  created\nbuildconfig  rails-pgsql-persistent  created\ndeploymentconfig  rails-pgsql-persistent  created persistentvolumeclaim  postgresql  created service  postgresql  created\ndeploymentconfig  postgresql  created  You can now use the OpenShift UI (while being logged in the newly created project) to follow the deployment process.   Alternatively watch the containers deploy like this:  oc get pods -w  The complete output should look like this:  NAME                             READY     STATUS              RESTARTS   AGE\npostgresql-1-deploy              0/1       ContainerCreating   0          11s\nrails-pgsql-persistent-1-build   0/1       ContainerCreating   0          11s\nNAME                  READY     STATUS    RESTARTS   AGE\npostgresql-1-deploy   1/1       Running   0          14s\npostgresql-1-81gnm   0/1       Pending   0         0s\npostgresql-1-81gnm   0/1       Pending   0         0s\nrails-pgsql-persistent-1-build   1/1       Running   0         19s\npostgresql-1-81gnm   0/1       Pending   0         15s\npostgresql-1-81gnm   0/1       ContainerCreating   0         16s\npostgresql-1-81gnm   0/1       Running   0         47s\npostgresql-1-81gnm   1/1       Running   0         4m\npostgresql-1-deploy   0/1       Completed   0         4m\npostgresql-1-deploy   0/1       Terminating   0         4m\npostgresql-1-deploy   0/1       Terminating   0         4m\nrails-pgsql-persistent-1-deploy   0/1       Pending   0         0s\nrails-pgsql-persistent-1-deploy   0/1       Pending   0         0s\nrails-pgsql-persistent-1-deploy   0/1       ContainerCreating   0         0s\nrails-pgsql-persistent-1-build   0/1       Completed   0         11m\nrails-pgsql-persistent-1-deploy   1/1       Running   0         6s\nrails-pgsql-persistent-1-hook-pre   0/1       Pending   0         0s\nrails-pgsql-persistent-1-hook-pre   0/1       Pending   0         0s\nrails-pgsql-persistent-1-hook-pre   0/1       ContainerCreating   0         0s\nrails-pgsql-persistent-1-hook-pre   1/1       Running   0         6s\nrails-pgsql-persistent-1-hook-pre   0/1       Completed   0         15s\nrails-pgsql-persistent-1-dkj7w   0/1       Pending   0         0s\nrails-pgsql-persistent-1-dkj7w   0/1       Pending   0         0s\nrails-pgsql-persistent-1-dkj7w   0/1       ContainerCreating   0         0s\nrails-pgsql-persistent-1-dkj7w   0/1       Running   0         1m\nrails-pgsql-persistent-1-dkj7w   1/1       Running   0         1m\nrails-pgsql-persistent-1-deploy   0/1       Completed   0         1m\nrails-pgsql-persistent-1-deploy   0/1       Terminating   0         1m\nrails-pgsql-persistent-1-deploy   0/1       Terminating   0         1m\nrails-pgsql-persistent-1-hook-pre   0/1       Terminating   0         1m\nrails-pgsql-persistent-1-hook-pre   0/1       Terminating   0         1m  Exit out of the watch mode with:  Ctrl  +  c   Note  It may take up to 5 minutes for the deployment to complete.  If you did it via the UI the deployment is finished when both, rails app and postgres database are up and running:    You should also see a PVC being issued and in the  Bound  state.   Look at the PVC created:  oc get pvc/postgresql  Output:  NAME         STATUS    VOLUME                                     CAPACITY   ACCESSMODES   AGE\npostgresql   Bound     pvc-6c348fbb-4e9d-11e7-970e-0a9938370404   15Gi       RWO           4m   Why did this even work?  If you paid close attention you likely noticed that the PVC in the template does not specify a particular  StorageClass . This still yields a PV deployed because our  StorageClass  has actually been defined as the system-wide default. PVCs that don t specify a  StorageClass  will use the default class.   Now go ahead and try out the application. The overview page in the OpenShift UI will tell you the  route  which has been deployed as well. Use it and append  /articles  to the URL to get to the actual app.   Otherwise get it on the CLI like this:  oc get route  Output:  NAME                     HOST/PORT                                                               PATH      SERVICES                 PORT      TERMINATION   WILDCARD\nrails-pgsql-persistent   rails-pgsql-persistent-my-test-project.cloudapps.34.252.58.209.nip.io             rails-pgsql-persistent    all                    None   Note  Again, the URL will be slightly different for you.   Following this output, point your browser to the URL and append  /articles  to reach the actual application, in this case:  http:// rails-pgsql-persistent-my-test-project.cloudapps.34.252.58.209.nip.io / articles  You should be able to successfully create articles and comments. The username/password to create articles and comments is by default  openshift / secret . \nWhen they are saved they are actually saved in the PostgreSQL database which stores it\u2019s table spaces on a GlusterFS volume provided by CNS.  Now let\u2019s take a look at how this was actually achieved.   A normal user cannot see the details of a PersistentVolume. Log in as  operator :  oc login -u operator   Look at the PVC to determine the PV:  oc get pvc  Output:  NAME         STATUS    VOLUME                                     CAPACITY   ACCESSMODES   AGE\npostgresql   Bound     pvc-6c348fbb-4e9d-11e7-970e-0a9938370404   15Gi       RWO           10m   Note  Your volume (PV) name will be different as it\u2019s dynamically generated.    Look at the details of this PV:  oc describe pv/pvc-6c348fbb-4e9d-11e7-970e-0a9938370404  Output shows (in highlight) the name of the volume, the backend type (GlusterFS) and the volume name GlusterFS uses internally.  Name:       pvc-6c348fbb-4e9d-11e7-970e-0a9938370404 Labels:      none \nStorageClass:   container-native-storage\nStatus:     Bound\nClaim:      my-test-project/postgresql\nReclaim Policy: Delete\nAccess Modes:   RWO\nCapacity:   15Gi\nMessage:\nSource:     Type:       Glusterfs (a Glusterfs mount on the host that shares a pod s lifetime)     EndpointsName:  glusterfs-dynamic-postgresql     Path:       vol_efac3ddb9d339fa680c0807a1d91c5a3     ReadOnly:       false\nNo events.  Note the GlusterFS volume name, in this case  vol_e8fe7f46fedf7af7628feda0dcbf2f60 .   Now let\u2019s switch to the namespace we used for CNS deployment:  oc project container-native-storage   Look at the GlusterFS pods running  oc get pods -o wide  Pick one of the GlusterFS pods by name (which one is not important):  NAME              READY     STATUS    RESTARTS   AGE       IP           NODE\nglusterfs-5rc2g   1/1       Running   0          51m       10.0.2.101   node-1.lab\nglusterfs-jbvdk   1/1       Running   0          51m       10.0.3.102   node-2.lab\nglusterfs-rchtr   1/1       Running   0          51m       10.0.4.103   node-3.lab\nheketi-1-tn0s9    1/1       Running   0          49m       10.130.2.3   node-6.lab  Remember the IP address  of the pod you select. In this case  10.0.2.101 .   Log on to GlusterFS pod with a remote terminal session like so:  oc rsh glusterfs-5rc2g  You will end up in shell session in the container with root privileges.  sh-4.2#  You have now access to this container\u2019s process and filesystem namespace which has the GlusterFS CLI utilities installed.   Let\u2019s list all known volumes:  sh-4.2# gluster volume list  You will see two volumes:  heketidbstorage\nvol_efac3ddb9d339fa680c0807a1d91c5a3    heketidbstorage  is a internal-only volume dedicated to heketi\u2019s internal database.    vol_efac3ddb9d339fa680c0807a1d91c5a3  is the volume backing the PV of the PostgreSQL database deployed earlier.     Interrogate GlusterFS about the topology of this volume:  sh-4.2# gluster volume info vol_efac3ddb9d339fa680c0807a1d91c5a3  The output will show you how the volume has been created. You will also see that the pod you are currently logged on serves one the bricks (in highlight).  Volume Name: vol_efac3ddb9d339fa680c0807a1d91c5a3\nType: Replicate\nVolume ID: cfaccdec-3c97-4e43-b80f-c9677e7a726a\nStatus: Started\nSnapshot Count: 0\nNumber of Bricks: 1 x 3 = 3\nTransport-type: tcp\nBricks:\nBrick1: 10.0.3.102:/var/lib/heketi/mounts/vg_8ea71174529a35f41fc0d1b288da6299/brick_b2e6975e246d896038604a7c0efcd83f/brick Brick2: 10.0.2.101:/var/lib/heketi/mounts/vg_2a49883a5cb39c3b845477ff85a729ba/brick_c5b00eeae2c57862b4eddeeb9b3903ad/brick Brick3: 10.0.4.103:/var/lib/heketi/mounts/vg_41b8a921f8e6d31cb04c7dd35b6b4cf2/brick_4f691eb2ba90a3ee31cb882f12786400/brick\nOptions Reconfigured:\ntransport.address-family: inet\nperformance.readdir-ahead: on\nnfs.disable: on   Note  Identify the right brick by looking at the host IP of the pod you have just logged on to.  oc get pods -o wide  will give you this information.   GlusterFS created this volume as a 3-way replica set across all GlusterFS pods, therefore across all your OpenShift App nodes running CNS. This is currently the only supported volume type in production. Later you will see how can schedule (unsupported) volume types like dispersed or distributed.   You can even look at the local brick:  sh-4.2# ls -ahl /var/lib/heketi/mounts/vg_2a49883a5cb39c3b845477ff85a729ba/brick_c5b00eeae2c57862b4eddeeb9b3903ad/brick\ntotal 16K\ndrwxrwsr-x.   5 root       2001   57 Jun  6 14:44 .\ndrwxr-xr-x.   3 root       root   19 Jun  6 14:44 ..\ndrw---S---. 263 root       2001 8.0K Jun  6 14:46 .glusterfs\ndrwxr-sr-x.   3 root       2001   25 Jun  6 14:44 .trashcan\ndrwx------.  20 1000080000 2001 8.0K Jun  6 14:46 userdata\n\nsh-4.2# ls -ahl /var/lib/heketi/mounts/vg_2a49883a5cb39c3b845477ff85a729ba/brick_c5b00eeae2c57862b4eddeeb9b3903ad/brick/userdata\n\ntotal 68K\ndrwx------. 20 1000080000 2001 8.0K Jun  6 14:46 .\ndrwxrwsr-x.  5 root       2001   57 Jun  6 14:44 ..\n-rw-------.  2 1000080000 root    4 Jun  6 14:44 PG_VERSION\ndrwx------.  6 1000080000 root   54 Jun  6 14:46 base\ndrwx------.  2 1000080000 root 8.0K Jun  6 14:47 global\ndrwx------.  2 1000080000 root   18 Jun  6 14:44 pg_clog\ndrwx------.  2 1000080000 root    6 Jun  6 14:44 pg_commit_ts\ndrwx------.  2 1000080000 root    6 Jun  6 14:44 pg_dynshmem\n-rw-------.  2 1000080000 root 4.6K Jun  6 14:46 pg_hba.conf\n-rw-------.  2 1000080000 root 1.6K Jun  6 14:44 pg_ident.conf\ndrwx------.  2 1000080000 root   32 Jun  6 14:46 pg_log\ndrwx------.  4 1000080000 root   39 Jun  6 14:44 pg_logical\ndrwx------.  4 1000080000 root   36 Jun  6 14:44 pg_multixact\ndrwx------.  2 1000080000 root   18 Jun  6 14:46 pg_notify\ndrwx------.  2 1000080000 root    6 Jun  6 14:44 pg_replslot\ndrwx------.  2 1000080000 root    6 Jun  6 14:44 pg_serial\ndrwx------.  2 1000080000 root    6 Jun  6 14:44 pg_snapshots\ndrwx------.  2 1000080000 root    6 Jun  6 14:46 pg_stat\ndrwx------.  2 1000080000 root   84 Jun  6 15:16 pg_stat_tmp\ndrwx------.  2 1000080000 root   18 Jun  6 14:44 pg_subtrans\ndrwx------.  2 1000080000 root    6 Jun  6 14:44 pg_tblspc\ndrwx------.  2 1000080000 root    6 Jun  6 14:44 pg_twophase\ndrwx------.  3 1000080000 root   60 Jun  6 14:44 pg_xlog\n-rw-------.  2 1000080000 root   88 Jun  6 14:44 postgresql.auto.conf\n-rw-------.  2 1000080000 root  21K Jun  6 14:46 postgresql.conf\n-rw-------.  2 1000080000 root   46 Jun  6 14:46 postmaster.opts\n-rw-------.  2 1000080000 root   89 Jun  6 14:46 postmaster.pid   Note  The exact path name will be different in your environment as it has been automatically generated.   You are looking at the PostgreSQL internal data file structure from the perspective of the GlusterFS server side. Evidence that the database uses CNS.  Clients, like the OpenShift nodes and their application pods talk to this storage with the GlusterFS protocol as it where an ordinary GlusterFS deployment. \nWhen a pod starts that mounts storage from a PV backed by CNS the GlusterFS mount plugin in OpenShift will mount the GlusterFS volume on the right App Node and then  bind-mount  this directory to the right pod. \nThis is happen transparently to the application and looks like a normal local filesystem inside the pod.   You may exit your remote session to the GlusterFS pod.  sh-4.2# exit", 
            "title": "Using non-shared storage for databases"
        }, 
        {
            "location": "/module-3-using-cns/#providing-shared-storage-to-multiple-application-instances", 
            "text": "In the previous example we provisioned an RWO PV - the volume is only usable with one pod at a time. RWO is what most of the OpenShift storage backends support. \nSo far only very few options, like the basic NFS support existed, to provide a PersistentVolume to more than one container at once. The access mode used for this is  ReadWriteMany .  With CNS this capabilities is now available to all OpenShift deployments, no matter where they are deployed. To demonstrate this capability with an application we will deploy a PHP-based file uploader that has multiple front-end instances sharing a common storage repository.   Log back in as developer  oc login -u developer   First make sure you are still in the example project created earlier  oc project my-test-project   Next deploy the example application:  oc new-app openshift/php:7.0~https://github.com/christianh814/openshift-php-upload-demo --name=file-uploader   Note  This is yet another way to build and launch an application from source code in OpenShift. The content before the ~ is the name of a Source-to-Image builder (a container that knows how to build applications of a certain type from source, in this case PHP) and the URL following is a GitHub repository hosting the source code. \nFeel free to check it out.   Output:  --  Found image a1ebebb (6 weeks old) in image stream  openshift/php  under tag  7.0  for  openshift/php:7.0 \n\n    Apache 2.4 with PHP 7.0\n    -----------------------\n    Platform for building and running PHP 7.0 applications\n\n    Tags: builder, php, php70, rh-php70\n\n    * A source build using source code from https://github.com/christianh814/openshift-php-upload-demo will be created\n      * The resulting image will be pushed to image stream  file-uploader:latest \n      * Use  start-build  to trigger a new build\n    * This image will be deployed in deployment config  file-uploader \n    * Port 8080/tcp will be load balanced by service  file-uploader \n      * Other containers can access this service through the hostname  file-uploader \n\n--  Creating resources ...\n    imagestream  file-uploader  created\n    buildconfig  file-uploader  created\n    deploymentconfig  file-uploader  created\n    service  file-uploader  created\n--  Success\n    Build scheduled, use  oc logs -f bc/file-uploader  to track its progress.\n    Run  oc status  to view your app.   Wait for the application to be deployed with the suggest command:  oc logs -f bc/file-uploader  The follow-mode of the above command ends automatically when the build is successful and you return to your shell.  ...\nCloning  https://github.com/christianh814/openshift-php-upload-demo  ...\n        Commit: 7508da63d78b4abc8d03eac480ae930beec5d29d (Update index.html)\n        Author: Christian Hernandez  christianh814@users.noreply.github.com \n        Date:   Thu Mar 23 09:59:38 2017 -0700\n---  Installing application source...\nPushing image 172.30.120.134:5000/my-test-project/file-uploader:latest ...\nPushed 0/5 layers, 2% complete\nPushed 1/5 layers, 20% complete\nPushed 2/5 layers, 40% complete\nPush successful   When the build is completed ensure the pods are running:  oc get pods  Among your existing pods you should see new pods running.  NAME                             READY     STATUS      RESTARTS   AGE\nfile-uploader-1-build            0/1       Completed   0          2m\nfile-uploader-1-k2v0d            1/1       Running     0          1m\n...  A service has been created for our app but not exposed yet.   Let\u2019s fix this:  oc expose svc/file-uploader   Check the route that has been created:  oc get route/file-uploader  The route forwards all traffic to port 8080 of the container running the app.  NAME            HOST/PORT                                                      PATH      SERVICES        PORT       TERMINATION   WILDCARD\nfile-uploader   file-uploader-my-test-project.cloudapps.34.252.58.209.nip.io             file-uploader   8080-tcp                 None  Point your browser the the URL advertised by the route (in this case http://file-uploader-my-test-project.cloudapps.34.252.58.209.nip.io)  The application simply lists all file previously uploaded and offers the ability to upload new ones as well as download the existing data. Right now there is nothing.  Select an arbitrary from your local system and upload it to the app.   After uploading a file validate it has been stored locally in the container by following the link  List Uploaded Files  in the browser.  Let s see how this is stored locally in the container.   List the running pods of our application:  oc get pods | grep file-uploader  You will see two entries:  file-uploader-1-build            0/1       Completed   0          7m\nfile-uploader-1-k2v0d            1/1       Running     0          6m  Note the name of the single pod currently running the app:  file-uploader-1-k2v0d . \nThe container called  file-uploader-1-build  is the builder container that deployed the application and it has already terminated.   Log into the application pod via a remote session (using the name noted earlier):  oc rsh file-uploader-1-k2v0d  In the container explore the directory in which the uploaded files will be stored.  sh-4.2$ cd uploaded\nsh-4.2$ pwd\n/opt/app-root/src/uploaded\nsh-4.2$ ls -lh\ntotal 16K\n-rw-r--r--. 1 1000080000 root 16K May 26 09:32 cns-deploy-4.0.0-15.el7rhgs.x86_64.rpm.gz   Note  The exact name of the pod will be different in your environment.   The app should also list the file in the overview:   This pod currently does not use any persistent storage. It stores the file locally.   Important  Never store data in a pod. It\u2019s ephemeral by definition and will be lost as soon as the pod terminates.   Let\u2019s see when this become a problem.   Exit out of the container shell:  sh-4.2$ exit   Let\u2019s scale the deployment to 3 instances of the app:  oc scale dc/file-uploader --replicas=3   Watch the additional pods getting spawned:  oc get pods  You will see 2 additional pods being spawned:  NAME                             READY     STATUS      RESTARTS   AGE\nfile-uploader-1-3cgh1            1/1       Running     0          20s\nfile-uploader-1-3hckj            1/1       Running     0          20s\nfile-uploader-1-build            0/1       Completed   0          4m\nfile-uploader-1-k2v0d            1/1       Running     0          3m\n...   Note  The pod names will be different in your environment since they are automatically generated.   These 3 pods now make up our application. OpenShift will load balance incoming traffic between them. \nHowever, when you log on to one of the new instances you will see they have no data.   Log on to one of the new containers:  oc rsh file-uploader-1-3cgh1   Again check the upload directory:  sh-4.2$ cd uploaded\nsh-4.2$ pwd\n/opt/app-root/src/uploaded\nsh-4.2$ ls -hl\ntotal 0  It s empty because the previously uploaded files were stored locally in the first container and are not available to the others. \nSimilarly, other users of the app will sometimes see your uploaded files and sometimes not. With the deployment scaled to 3 instances OpenShifts router will simply round-robin across them. Whenever the load balancing service in OpenShift points to the pod that has the file stored locally users will see it or not. You can simulate this with another instance of your browser in  Incognito mode  pointing to your app.  The app is of course not usable like this. We can fix this by providing shared storage to this app.   First create a PVC with the appropriate setting in a file called  cns-rwx-pvc.yml  with below contents:  cns-rwx-pvc.yml:  kind :   PersistentVolumeClaim  apiVersion :   v1  metadata : \n   name :   my-shared-storage \n   annotations : \n     volume.beta.kubernetes.io/storage-class :   container-native-storage  spec : \n   accessModes :     -   ReadWriteMany     resources : \n     requests : \n       storage :   10Gi   Notice the access mode explicitly requested to be  ReadWriteMany  (also referred to as RWX). Storage provisioned like this can be mounted by multiple containers on multiple hosts at the same time.   Submit the request to the system:  oc create -f cns-rwx-pvc.yml   Let\u2019s look at the result:  oc get pvc  ACCESSMODES  is set to  RWX :  NAME                STATUS    VOLUME                                     CAPACITY   ACCESSMODES   AGE\nmy-shared-storage   Bound     pvc-62aa4dfe-4ad2-11e7-b56f-2cc2602a6dc8   10Gi       RWX           22s\n...  We can now update the  DeploymentConfig  of our application to use this PVC to provide the application with persistent, shared storage for uploads.   Update the configuration of the application by adding a volume claim like this:  oc volume dc/file-uploader --add --name=shared-storage --type=persistentVolumeClaim --claim-name=my-shared-storage --mount-path=/opt/app-root/src/uploaded  Our app will now re-deploy (in a rolling fashion) with the new settings - all pods will mount the volume identified by the PVC under /opt/app-root/src/upload (the path is predictable so we can hard-code it here).   You can watch it like this:  oc logs dc/file-uploader -f  The new  DeploymentConfig  will supersede the old one.  --  Scaling up file-uploader-2 from 0 to 3, scaling down file-uploader-1 from 3 to 0 (keep 3 pods available, don t exceed 4 pods)\n    Scaling file-uploader-2 up to 1\n    Scaling file-uploader-1 down to 2\n    Scaling file-uploader-2 up to 2\n    Scaling file-uploader-1 down to 1\n    Scaling file-uploader-2 up to 3\n    Scaling file-uploader-1 down to 0\n--  Success  The new config  file-uploader-2  will have 3 pods all sharing the same storage.   Get the names of the new pods:  oc get pods  Output:  NAME                             READY     STATUS      RESTARTS   AGE\nfile-uploader-1-build            0/1       Completed   0          18m\nfile-uploader-2-jd22b            1/1       Running     0          1m\nfile-uploader-2-kw9lq            1/1       Running     0          2m\nfile-uploader-2-xbz24            1/1       Running     0          1m\n...  Try it out in your application: upload new files and watch them being visible from within all application pods. In new browser sessions, simulating other users, the application behaves normally as it circles through the pods between browser requests.  [root@master ~]# oc rsh file-uploader-2-jd22b\nsh-4.2$ ls -lh uploaded\ntotal 16K\n-rw-r--r--. 1 1000080000 root 16K May 26 10:21 cns-deploy-4.0.0-15.el7rhgs.x86_64.rpm.gz\nsh-4.2$ exit\nexit\n[root@master ~]# oc rsh file-uploader-2-kw9lq\nsh-4.2$ ls -lh uploaded\n-rw-r--r--. 1 1000080000 root 16K May 26 10:21 cns-deploy-4.0.0-15.el7rhgs.x86_64.rpm.gz\nsh-4.2$ exit\nexit\n[root@master ~]# oc rsh file-uploader-2-xbz24\nsh-4.2$ ls -lh uploaded\n-rw-r--r--. 1 1000080000 root 16K May 26 10:21 cns-deploy-4.0.0-15.el7rhgs.x86_64.rpm.gz\nsh-4.2$ exit  That\u2019s it. You have successfully provided shared storage to pods throughout the entire system, therefore avoiding the need for data to be replicated at the application level to each pod.  With CNS this is available wherever OpenShift is deployed with no external dependency.", 
            "title": "Providing shared storage to multiple application instances"
        }, 
        {
            "location": "/module-4-cluster-ops/", 
            "text": "Overview\n\n\nIn this module you be introduced to some standard operational procedures. You will learn how to run multiple GlusterFS \nTrusted Storage Pools\n on OpenShift and how to expand and maintain deployments.\n\n\nHerein, we will use the term pool (GlusterFS terminology) and cluster (heketi terminology) interchangeably.\n\n\n\n\nRunning multiple GlusterFS pools\n#\n\n\nIn the previous modules a single GlusterFS clusters was used to supply \nPersistentVolume\n to applications. CNS allows for multiple clusters to run in a single OpenShift deployment.\n\n\nThere are several use cases for this:\n\n\n\n\n\n\nProvide data isolation between clusters of different tenants\n\n\n\n\n\n\nProvide multiple performance tiers of CNS, i.e. HDD-based vs. SSD-based\n\n\n\n\n\n\nOvercome the maximum amount of 300 PVs per cluster, currently imposed in version 3.5\n\n\n\n\n\n\nTo deploy a second GlusterFS pool follow these steps:\n\n\n Log in as \noperator\n to namespace \ncontainer-native-storage\n\n\noc login -u operator -n container-native-storage\n\n\n\n\n\nYour deployment has 6 OpenShift Application Nodes in total, \nnode-1\n, \nnode-2\n and \nnode-3\n currently setup running CNS. We will now set up a second cluster using \nnode-4\n, \nnode-5\n and \nnode-6\n.\n\n\n Apply additional labels (user-defined key-value pairs) to the remaining 3 OpenShift Nodes:\n\n\noc label node/node-4.lab storagenode=glusterfs\noc label node/node-5.lab storagenode=glusterfs\noc label node/node-6.lab storagenode=glusterfs\n\n\n\n\n\nThe label will be used to control GlusterFS pod placement and availability.\n\n\n Wait for all pods to show \n1/1\n in the \nREADY\n column:\n\n\n oc get pods -o wide -n container-native-storage\n\n\n\n\n\n\n\nNote\n\n\nIt may take up to 3 minutes for the GlusterFS pods to transition into \nREADY\n state.\n\n\n\n\nFor bulk import of new nodes into a new cluster, the topology JSON file can be updated to include a second cluster with a separate set of nodes.\n\n\n Create a new file named updated-topology.json with the content below:\n\n\nupdated-topology.json:\n\n\n{\n\n    \nclusters\n:\n \n[\n\n        \n{\n\n            \nnodes\n:\n \n[\n\n                \n{\n\n                    \nnode\n:\n \n{\n\n                        \nhostnames\n:\n \n{\n\n                            \nmanage\n:\n \n[\n\n                                \nnode-1.lab\n\n                            \n],\n\n                            \nstorage\n:\n \n[\n\n                                \n10.0.2.101\n\n                            \n]\n\n                        \n},\n\n                        \nzone\n:\n \n1\n\n                    \n},\n\n                    \ndevices\n:\n \n[\n\n                        \n/dev/xvdc\n\n                    \n]\n\n                \n},\n\n                \n{\n\n                    \nnode\n:\n \n{\n\n                        \nhostnames\n:\n \n{\n\n                            \nmanage\n:\n \n[\n\n                                \nnode-2.lab\n\n                            \n],\n\n                            \nstorage\n:\n \n[\n\n                                \n10.0.3.102\n\n                            \n]\n\n                        \n},\n\n                        \nzone\n:\n \n2\n\n                    \n},\n\n                    \ndevices\n:\n \n[\n\n                        \n/dev/xvdc\n\n                    \n]\n\n                \n},\n\n                \n{\n\n                    \nnode\n:\n \n{\n\n                        \nhostnames\n:\n \n{\n\n                            \nmanage\n:\n \n[\n\n                                \nnode-3.lab\n\n                            \n],\n\n                            \nstorage\n:\n \n[\n\n                                \n10.0.4.103\n\n                            \n]\n\n                        \n},\n\n                        \nzone\n:\n \n3\n\n                    \n},\n\n                    \ndevices\n:\n \n[\n\n                        \n/dev/xvdc\n\n                    \n]\n\n                \n}\n\n            \n]\n\n\n        \n},\n\n\n        \n{\n\n            \nnodes\n:\n \n[\n\n                \n{\n\n                    \nnode\n:\n \n{\n\n                        \nhostnames\n:\n \n{\n\n                            \nmanage\n:\n \n[\n\n                                \nnode-4.lab\n\n                            \n],\n\n                            \nstorage\n:\n \n[\n\n                                \n10.0.2.104\n\n                            \n]\n\n                        \n},\n\n                        \nzone\n:\n \n1\n\n                    \n},\n\n                    \ndevices\n:\n \n[\n\n                        \n/dev/xvdc\n\n                    \n]\n\n                \n},\n\n                \n{\n\n                    \nnode\n:\n \n{\n\n                        \nhostnames\n:\n \n{\n\n                            \nmanage\n:\n \n[\n\n                                \nnode-5.lab\n\n                            \n],\n\n                            \nstorage\n:\n \n[\n\n                                \n10.0.3.105\n\n                            \n]\n\n                        \n},\n\n                        \nzone\n:\n \n2\n\n                    \n},\n\n                    \ndevices\n:\n \n[\n\n                        \n/dev/xvdc\n\n                    \n]\n\n                \n},\n\n                \n{\n\n                    \nnode\n:\n \n{\n\n                        \nhostnames\n:\n \n{\n\n                            \nmanage\n:\n \n[\n\n                                \nnode-6.lab\n\n                            \n],\n\n                            \nstorage\n:\n \n[\n\n                                \n10.0.4.106\n\n                            \n]\n\n                        \n},\n\n                        \nzone\n:\n \n3\n\n                    \n},\n\n                    \ndevices\n:\n \n[\n\n                        \n/dev/xvdc\n\n                    \n]\n\n                \n}\n\n            \n]\n\n        \n}\n\n    \n]\n\n\n}\n\n\n\n\n\n\nThe file contains the same content as \ntopology.json\n with a second cluster specification (beginning at the highlighted line).\n\n\nWhen loading this topology to heketi, it will recognize the existing cluster (leaving it unchanged) and start creating the new one, with the same bootstrapping process as seen with \ncns-deploy\n.\n\n\n Prepare the heketi CLI tool like previously in \nModule 2\n.\n\n\nexport HEKETI_CLI_SERVER=http://heketi-container-native-storage.cloudapps.34.252.58.209.nip.io\nexport HEKETI_CLI_USER=admin\nexport HEKETI_CLI_KEY=myS3cr3tpassw0rd\n\n\n\n\n\n Verify there is currently only a single cluster known to heketi\n\n\nheketi-cli cluster list\n\n\n\n\n\nExample output:\n\n\nClusters:\nfb67f97166c58f161b85201e1fd9b8ed\n\n\n\n\n\nNote this cluster\ns id\n for later reference.\n\n\n Load the new topology with the heketi client\n\n\nheketi-cli topology load --json=updated-topology.jso\n\n\n\n\n\nYou should see output similar to the following:\n\n\nFound node node-1.lab on cluster fb67f97166c58f161b85201e1fd9b8ed\nFound device /dev/xvdc\nFound node node-2.lab on cluster fb67f97166c58f161b85201e1fd9b8ed\nFound device /dev/xvdc\nFound node node-3.lab on cluster fb67f97166c58f161b85201e1fd9b8ed\nFound device /dev/xvdc\nCreating cluster ... ID: 46b205a4298c625c4bca2206b7a82dd3\nCreating node node-4.lab ... ID: 604d2eb15a5ca510ff3fc5ecf912d3c0\nAdding device /dev/xvdc ... OK\nCreating node node-5.lab ... ID: 538b860406870288af23af0fbc2cd27f\nAdding device /dev/xvdc ... OK\nCreating node node-6.lab ... ID: 7736bd0cb6a84540860303a6479cacb2\nAdding device /dev/xvdc ... OK\n\n\n\n\n\nAs indicated from above output a new cluster got created.\n\n\n List all clusters:\n\n\nheketi-cli cluster list\n\n\n\n\n\nYou should see a second cluster in the list:\n\n\nClusters:\n46b205a4298c625c4bca2206b7a82dd3\nfb67f97166c58f161b85201e1fd9b8ed\n\n\n\n\n\nThe second cluster with the ID \n46b205a4298c625c4bca2206b7a82dd3\n is an entirely independent GlusterFS deployment. \nNote this second cluster\ns ID\n as well for later reference.\n\n\nNow we have two independent GlusterFS clusters managed by the same heketi instance:\n\n\n\n\n\n\n\n\n\n\nNodes\n\n\nCluster UUID\n\n\n\n\n\n\n\n\n\n\nFirst Cluster\n\n\nnode-1, node-2, node-3\n\n\nfb67f97166c58f161b85201e1fd9b8ed\n\n\n\n\n\n\nSecond Cluster\n\n\nnode-4, node-5, node-6\n\n\n46b205a4298c625c4bca2206b7a82dd3\n\n\n\n\n\n\n\n\n Query the updated topology:\n\n\nheketi-cli topology info\n\n\n\n\n\nAbbreviated output:\n\n\nCluster Id: 46b205a4298c625c4bca2206b7a82dd3\n\n    Volumes:\n\n    Nodes:\n\n      Node Id: 538b860406870288af23af0fbc2cd27f\n      State: online\n      Cluster Id: 46b205a4298c625c4bca2206b7a82dd3\n      Zone: 2\n      Management Hostname: node-5.lab\n      Storage Hostname: 10.0.3.105\n      Devices:\n        Id:e481d022cea9bfb11e8a86c0dd8d3499   Name:/dev/xvdc           State:online    Size (GiB):499     Used (GiB):0       Free (GiB):499\n            Bricks:\n\n      Node Id: 604d2eb15a5ca510ff3fc5ecf912d3c0\n      State: online\n      Cluster Id: 46b205a4298c625c4bca2206b7a82dd3\n      Zone: 1\n      Management Hostname: node-4.lab\n      Storage Hostname: 10.0.2.104\n      Devices:\n        Id:09a25a114c53d7669235b368efd2f8d1   Name:/dev/xvdc           State:online    Size (GiB):499     Used (GiB):0       Free (GiB):499\n            Bricks:\n\n      Node Id: 7736bd0cb6a84540860303a6479cacb2\n      State: online\n      Cluster Id: 46b205a4298c625c4bca2206b7a82dd3\n      Zone: 3\n      Management Hostname: node-6.lab\n      Storage Hostname: 10.0.4.106\n      Devices:\n        Id:cccadb2b54dccd99f698d2ae137a22ff   Name:/dev/xvdc           State:online    Size (GiB):499     Used (GiB):0       Free (GiB):499\n            Bricks:\n\nCluster Id: fb67f97166c58f161b85201e1fd9b8ed\n\n[...output omitted for brevity...]\n\n\n\n\n\nheketi formed an new, independent 3-node GlusterFS cluster on those nodes.\n\n\n Check running GlusterFS pods\n\n\noc get pods -o wide\n\n\n\n\n\nFrom the output you can spot the pod names running on the new cluster\ns nodes:\n\n\nNAME              READY     STATUS    RESTARTS   AGE       IP           NODE\n\nglusterfs-1nvtj   1/1       Running   0          23m       10.0.4.106   node-6.lab\n\nglusterfs-5gvw8   1/1       Running   0          24m       10.0.2.104   node-4.lab\n\nglusterfs-5rc2g   1/1       Running   0          4h        10.0.2.101   node-1.lab\n\nglusterfs-b4wg1   1/1       Running   0          24m       10.0.3.105   node-5.lab\n\nglusterfs-jbvdk   1/1       Running   0          4h        10.0.3.102   node-2.lab\nglusterfs-rchtr   1/1       Running   0          4h        10.0.4.103   node-3.lab\nheketi-1-tn0s9    1/1       Running   0          4h        10.130.2.3   node-6.lab\n\n\n\n\n\n\n\nNote\n\n\nAgain note that the pod names are dynamically generated and will be different. Use the FQDN of your hosts to determine one of new cluster\ns pods.\n\n\n\n\n Log on to one of the new cluster\ns pods:\n\n\noc rsh glusterfs-1nvtj\n\n\n\n\n\n Query this GlusterFS node for it\ns peers:\n\n\ngluster peer status\n\n\n\n\n\nAs expected this node only has 2 peers, evidence that it\ns running in it\ns own GlusterFS pool separate from the first cluster in deployed in Module 2.\n\n\nNumber of Peers: 2\n\nHostname: node-5.lab\nUuid: 0db9b5d0-7fa8-4d2f-8b9e-6664faf34606\nState: Peer in Cluster (Connected)\nOther names:\n10.0.3.105\n\nHostname: node-4.lab\nUuid: 695b661d-2a55-4f94-b22e-40a9db79c01a\nState: Peer in Cluster (Connected)\n\n\n\n\n\nBefore you can use the second cluster two tasks have to be accomplished:\n\n\n\n\n\n\nThe \nStorageClass\n for the first cluster has to be updated to point the first cluster\ns UUID,\n\n\n\n\n\n\nA second \nStorageClass\n for the second cluster has to be created, pointing to the same heketi\n\n\n\n\n\n\n\n\nWhy do we need to update the first \nStorageClass\n?\n\n\nBy default, when no cluster UUID is specified, heketi serves volume creation requests from any cluster currently registered to it.\n\nIn order to request a volume from specific cluster you have to supply the cluster\ns UUID to heketi. The first \nStorageClass\n has no UUID specified so far.\n\n\n\n\n Edit the existing \nStorageClass\n definition by updating the \ncns-storageclass.yml\n file from \nModule 3\n like below:\n\n\ncns-storageclass.yml:\n\n\napiVersion\n:\n \nstorage.k8s.io/v1beta1\n\n\nkind\n:\n \nStorageClass\n\n\nmetadata\n:\n\n  \nname\n:\n \ncontainer-native-storage\n\n  \nannotations\n:\n\n    \nstorageclass.beta.kubernetes.io/is-default-class\n:\n \ntrue\n\n\nprovisioner\n:\n \nkubernetes.io/glusterfs\n\n\nparameters\n:\n\n  \nresturl\n:\n \nhttp://heketi-container-native-storage.cloudapps.34.252.58.209.nip.io\n\n  \nrestauthenabled\n:\n \ntrue\n\n  \nrestuser\n:\n \nadmin\n\n  \nvolumetype\n:\n \nreplicate:3\n\n  \nsecretNamespace\n:\n \ndefault\n\n  \nsecretName\n:\n \ncns-secret\n\n\n  \nclusterid\n:\n \nfb67f97166c58f161b85201e1fd9b8ed\n\n\n\n\n\n\nNote the additional \nclusterid\n parameter highlighted. It\ns the cluster\ns UUID as known by heketi. Don\nt change anything else.\n\n\n Delete the existing \nStorageClass\n definition in OpenShift\n\n\noc delete storageclass/container-native-storage\n\n\n\n\n\n Add the \nStorageClass\n again:\n\n\noc create -f cns-storageclass.yml\n\n\n\n\n\n Create a new file called cns-storageclass-slow.yml with the following contents:\n\n\ncns-storageclass-slow.yml:\n\n\napiVersion\n:\n \nstorage.k8s.io/v1beta1\n\n\nkind\n:\n \nStorageClass\n\n\nmetadata\n:\n\n  \nname\n:\n \ncontainer-native-storage-slow\n\n\nprovisioner\n:\n \nkubernetes.io/glusterfs\n\n\nparameters\n:\n\n  \nresturl\n:\n \nhttp://heketi-container-native-storage.cloudapps.34.252.58.209.nip.io\n\n  \nrestauthenabled\n:\n \ntrue\n\n  \nrestuser\n:\n \nadmin\n\n  \nvolumetype\n:\n \nreplicate:3\n\n  \nsecretNamespace\n:\n \ndefault\n\n  \nsecretName\n:\n \ncns-secret\n\n\n  \nclusterid\n:\n \n46b205a4298c625c4bca2206b7a82dd3\n\n\n\n\n\n\nAgain note the \nclusterid\n in the \nparameters\n section referencing the second cluster\ns UUID\n\n\n Add the new \nStorageClass\n:\n\n\noc create -f cns-storageclass-slow.yml\n\n\n\n\n\nLet\ns verify both \nStorageClasses\n are working as expected:\n\n\n Create the following two files containing PVCs issued against either of both GlusterFS pools via their respective \nStorageClass\n:\n\n\ncns-pvc-fast.yml:\n\n\nkind\n:\n \nPersistentVolumeClaim\n\n\napiVersion\n:\n \nv1\n\n\nmetadata\n:\n\n  \nname\n:\n \nmy-container-storage-fast\n\n  \nannotations\n:\n\n    \nvolume.beta.kubernetes.io/storage-class\n:\n \ncontainer-native-storage\n\n\nspec\n:\n\n  \naccessModes\n:\n\n  \n-\n \nReadWriteOnce\n\n  \nresources\n:\n\n    \nrequests\n:\n\n      \nstorage\n:\n \n5Gi\n\n\n\n\n\n\ncns-pvc-slow.yml:\n\n\nkind\n:\n \nPersistentVolumeClaim\n\n\napiVersion\n:\n \nv1\n\n\nmetadata\n:\n\n  \nname\n:\n \nmy-container-storage-slow\n\n  \nannotations\n:\n\n    \nvolume.beta.kubernetes.io/storage-class\n:\n \ncontainer-native-storage-slow\n\n\nspec\n:\n\n  \naccessModes\n:\n\n  \n-\n \nReadWriteOnce\n\n  \nresources\n:\n\n    \nrequests\n:\n\n      \nstorage\n:\n \n7Gi\n\n\n\n\n\n\n Create both PVCs:\n\n\noc create -f cns-pvc-fast.yml\noc create -f cns-pvc-slow.yml\n\n\n\n\n\nThey should both be in bound state after a couple of seconds:\n\n\nNAME                        STATUS    VOLUME                                     CAPACITY   ACCESSMODES   AGE\nmy-container-storage        Bound     pvc-3e249fdd-4ec0-11e7-970e-0a9938370404   5Gi        RWO           32s\nmy-container-storage-slow   Bound     pvc-405083e6-4ec0-11e7-970e-0a9938370404   7Gi        RWO           29s\n\n\n\n\n\n If you check again one of the pods of the second cluster\n\n\noc rsh glusterfs-1nvtj\n\n\n\n\n\n \nyou will see a new volume has been created\n\n\nsh-4.2# gluster volume list\nvol_0d976f357a7979060a4c32284ca8e136\n\n\n\n\n\nThis is how you use multiple, parallel GlusterFS pools/clusters on a single OpenShift cluster with a single heketi instance. Whereas the first pool is created with \ncns-deploy\n subsequent pools/cluster are created with the \nheketi-cli\n client.\n\n\nClean up the PVCs and the second \nStorageClass\n in preparation for the next section.\n\n\n Delete both PVCs (and therefore their volume)\n\n\noc delete pvc/my-container-storage-fast\noc delete pvc/my-container-storage-slow\n\n\n\n\n\n Delete the second \nStorageClass\n\n\noc delete storageclass/container-native-storage-slow\n\n\n\n\n\n\n\nDeleting a GlusterFS pool\n#\n\n\nSince we want to re-use \nnode-4\n, \nnode-5\n and \nnode-6\n for the next section we need to delete it the GlusterFS pools on top of them first.\n\n\nThis is a process that involves multiple steps of manipulating the heketi topology with the \nheketi-cli\n client.\n\n\n Make sure the client is still properly configured via environment variables:\n\n\nexport HEKETI_CLI_SERVER=http://heketi-container-native-storage.cloudapps.34.252.58.209.nip.io\nexport HEKETI_CLI_USER=admin\nexport HEKETI_CLI_KEY=myS3cr3tpassw0rd\n\n\n\n\n\n First list the topology and identify the cluster we want to delete:\n\n\nheketi-cli topology info\n\n\n\n\n\nThe portions of interest are highlighted:\n\n\nCluster Id: 46b205a4298c625c4bca2206b7a82dd3\n\n\n    Volumes:\n\n    Nodes:\n\n\n        Node Id: 538b860406870288af23af0fbc2cd27f\n\n        State: online\n        Cluster Id: 46b205a4298c625c4bca2206b7a82dd3\n        Zone: 2\n        Management Hostname: node-5.lab\n        Storage Hostname: 10.0.3.105\n        Devices:\n\n            Id:e481d022cea9bfb11e8a86c0dd8d3499   Name:/dev/xvdc           State:online    Size (GiB):499     Used (GiB):0       Free (GiB):499\n\n                Bricks:\n\n\n        Node Id: 604d2eb15a5ca510ff3fc5ecf912d3c0\n\n        State: online\n        Cluster Id: 46b205a4298c625c4bca2206b7a82dd3\n        Zone: 1\n        Management Hostname: node-4.lab\n        Storage Hostname: 10.0.2.104\n        Devices:\n\n            Id:09a25a114c53d7669235b368efd2f8d1   Name:/dev/xvdc           State:online    Size (GiB):499     Used (GiB):0       Free (GiB):499\n\n                Bricks:\n\n\n        Node Id: 7736bd0cb6a84540860303a6479cacb2\n\n        State: online\n        Cluster Id: 46b205a4298c625c4bca2206b7a82dd3\n        Zone: 3\n        Management Hostname: node-6.lab\n        Storage Hostname: 10.0.4.106\n        Devices:\n\n            Id:cccadb2b54dccd99f698d2ae137a22ff   Name:/dev/xvdc           State:online    Size (GiB):499     Used (GiB):0       Free (GiB):499\n\n            Bricks:\n\n\n\n\n\nThe hierachical dependencies in this topology works as follows: Clusters \n Nodes \n Devices.\n\nAssuming there are no volumes present these need to be deleted in reverse order.\n\n\nNote that specific values highlighted above in your environment and carry out the following steps in strict order:\n\n\n Delete all devices of all nodes:\n\n\nheketi-cli device delete e481d022cea9bfb11e8a86c0dd8d349\nheketi-cli device delete 09a25a114c53d7669235b368efd2f8d1\nheketi-cli device delete cccadb2b54dccd99f698d2ae137a22ff\n\n\n\n\n\n Delete all nodes of the cluster in question:\n\n\nheketi-cli node delete 538b860406870288af23af0fbc2cd27f\nheketi-cli node delete 604d2eb15a5ca510ff3fc5ecf912d3c0\nheketi-cli node delete 7736bd0cb6a84540860303a6479cacb2\n\n\n\n\n\n Finally delete the cluster:\n\n\nheketi-cli cluster delete 46b205a4298c625c4bca2206b7a82dd3\n\n\n\n\n\n Confirm the cluster is gone:\n\n\nheketi-cli cluster list\n\n\n\n\n\n Verify the clusters new topology back to it\ns state from Module 3.\n\n\nThis deleted all heketi database entries about the cluster. However the GlusterFS pods are still running, since they are controlled directly by OpenShift instead of heketi:\n\n\n List all the pods in the \ncontainer-native-storage\n namespace:\n\n\noc get pods -o wide -n container-native-storage\n\n\n\n\n\nThe pods formerly running the second GlusterFS pool are still there:\n\n\nNAME              READY     STATUS    RESTARTS   AGE       IP           NODE\nglusterfs-1nvtj   1/1       Running   0          1h       10.0.4.106   node-6.lab\nglusterfs-5gvw8   1/1       Running   0          1h       10.0.2.104   node-4.lab\nglusterfs-5rc2g   1/1       Running   0          4h       10.0.2.101   node-1.lab\nglusterfs-b4wg1   1/1       Running   0          1h       10.0.3.105   node-5.lab\nglusterfs-jbvdk   1/1       Running   0          4h       10.0.3.102   node-2.lab\nglusterfs-rchtr   1/1       Running   0          4h       10.0.4.103   node-3.lab\nheketi-1-tn0s9    1/1       Running   0          4h       10.130.2.3   node-6.lab\n\n\n\n\n\nThey can be stopped by removing the labels OpenShift uses to determine GlusterFS pod placement for CNS.\n\n\n Remove the labels from the last 3 OpenShift nodes like so:\n\n\noc label node/node-4.lab storagenode-\noc label node/node-5.lab storagenode-\noc label node/node-6.lab storagenode-\n\n\n\n\n\nContrary to the output of these commands the label \nstoragenode\n is actually removed.\n\n\n Verify that all GlusterFS pods running on \nnode-4\n, \nnode-5\n and \nnode-6\n are indeed terminated:\n\n\noc get pods -o wide -n container-native-storage\n\n\n\n\n\n\n\nNote\n\n\nIt can take up to 2 minutes for the pods to terminate.\n\n\n\n\nYou should be back down to 3 GlusterFS pods.\n\n\nNAME              READY     STATUS    RESTARTS   AGE       IP           NODE\nglusterfs-5rc2g   1/1       Running   0          5h        10.0.2.101   node-1.lab\nglusterfs-jbvdk   1/1       Running   0          5h        10.0.3.102   node-2.lab\nglusterfs-rchtr   1/1       Running   0          5h        10.0.4.103   node-3.lab\nheketi-1-tn0s9    1/1       Running   0          5h        10.130.2.3   node-6.lab\n\n\n\n\n\n\n\nExpanding a GlusterFS pool\n#\n\n\nInstead of creating additional GlusterFS pools in CNS on OpenShift it is also possible to expand existing pools.\n\n\nThis works similar to creating additional pools, with bulk-import via the topology file. Only this time with nodes added to the existing cluster structure in JSON.\n\n\nSince manipulating JSON can be error-prone create a new file called \nexpanded-topology.json\n with contents as below:\n\n\nexpanded-topology.json:\n\n\n{\n\n    \nclusters\n:\n \n[\n\n        \n{\n\n            \nnodes\n:\n \n[\n\n                \n{\n\n                    \nnode\n:\n \n{\n\n                        \nhostnames\n:\n \n{\n\n                            \nmanage\n:\n \n[\n\n                                \nnode-1.lab\n\n                            \n],\n\n                            \nstorage\n:\n \n[\n\n                                \n10.0.2.101\n\n                            \n]\n\n                        \n},\n\n                        \nzone\n:\n \n1\n\n                    \n},\n\n                    \ndevices\n:\n \n[\n\n                        \n/dev/xvdc\n\n                    \n]\n\n                \n},\n\n                \n{\n\n                    \nnode\n:\n \n{\n\n                        \nhostnames\n:\n \n{\n\n                            \nmanage\n:\n \n[\n\n                                \nnode-2.lab\n\n                            \n],\n\n                            \nstorage\n:\n \n[\n\n                                \n10.0.3.102\n\n                            \n]\n\n                        \n},\n\n                        \nzone\n:\n \n2\n\n                    \n},\n\n                    \ndevices\n:\n \n[\n\n                        \n/dev/xvdc\n\n                    \n]\n\n                \n},\n\n                \n{\n\n                    \nnode\n:\n \n{\n\n                        \nhostnames\n:\n \n{\n\n                            \nmanage\n:\n \n[\n\n                                \nnode-3.lab\n\n                            \n],\n\n                            \nstorage\n:\n \n[\n\n                                \n10.0.4.103\n\n                            \n]\n\n                        \n},\n\n                        \nzone\n:\n \n3\n\n                    \n},\n\n                    \ndevices\n:\n \n[\n\n                        \n/dev/xvdc\n\n                    \n]\n\n                \n},\n\n                \n{\n\n                    \nnode\n:\n \n{\n\n                        \nhostnames\n:\n \n{\n\n                            \nmanage\n:\n \n[\n\n                                \nnode-4.lab\n\n                            \n],\n\n                            \nstorage\n:\n \n[\n\n                                \n10.0.2.104\n\n                            \n]\n\n                        \n},\n\n                        \nzone\n:\n \n1\n\n                    \n},\n\n                    \ndevices\n:\n \n[\n\n                        \n/dev/xvdc\n\n                    \n]\n\n                \n},\n\n                \n{\n\n                    \nnode\n:\n \n{\n\n                        \nhostnames\n:\n \n{\n\n                            \nmanage\n:\n \n[\n\n                                \nnode-5.lab\n\n                            \n],\n\n                            \nstorage\n:\n \n[\n\n                                \n10.0.3.105\n\n                            \n]\n\n                        \n},\n\n                        \nzone\n:\n \n2\n\n                    \n},\n\n                    \ndevices\n:\n \n[\n\n                        \n/dev/xvdc\n\n                    \n]\n\n                \n},\n\n                \n{\n\n                    \nnode\n:\n \n{\n\n                        \nhostnames\n:\n \n{\n\n                            \nmanage\n:\n \n[\n\n                                \nnode-6.lab\n\n                            \n],\n\n                            \nstorage\n:\n \n[\n\n                                \n10.0.4.106\n\n                            \n]\n\n                        \n},\n\n                        \nzone\n:\n \n3\n\n                    \n},\n\n                    \ndevices\n:\n \n[\n\n                        \n/dev/xvdc\n\n                    \n]\n\n                \n}\n\n            \n]\n\n        \n}\n\n    \n]\n\n\n}\n\n\n\n\n\n\n Again, apply the expected labels to the remaining 3 OpenShift Nodes:\n\n\noc label node/node-4.lab storagenode=glusterfs\noc label node/node-5.lab storagenode=glusterfs\noc label node/node-6.lab storagenode=glusterfs\n\n\n\n\n\n Wait for all pods to show \n1/1\n in the \nREADY\n column:\n\n\n oc get pods -o wide -n container-native-storage\n\n\n\n\n\n\n\nNote\n\n\nIt may take up to 3 minutes for the GlusterFS pods to transition into \nREADY\n state.\n\n\n\n\nThis confirms all GlusterFS pods are ready to receive remote commands:\n\n\nNAME              READY     STATUS    RESTARTS   AGE       IP           NODE\nglusterfs-0lr75   1/1       Running   0          4m        10.0.4.106   node-6.lab\nglusterfs-1dxz3   1/1       Running   0          4m        10.0.3.105   node-5.lab\nglusterfs-5rc2g   1/1       Running   0          5h        10.0.2.101   node-1.lab\nglusterfs-8nrn0   1/1       Running   0          4m        10.0.2.104   node-4.lab\nglusterfs-jbvdk   1/1       Running   0          5h        10.0.3.102   node-2.lab\nglusterfs-rchtr   1/1       Running   0          5h        10.0.4.103   node-3.lab\nheketi-1-tn0s9    1/1       Running   0          5h        10.130.2.3   node-6.lab\n\n\n\n\n\n If you logged out of the session meanwhile re-instate the heketi-cli configuration with the environment variables;\n\n\nexport HEKETI_CLI_SERVER=http://heketi-container-native-storage.cloudapps.34.252.58.209.nip.io\nexport HEKETI_CLI_USER=admin\nexport HEKETI_CLI_KEY=myS3cr3tpassw0rd\n\n\n\n\n\n Now load the new topology:\n\n\nheketi-cli topology load --json=expanded-topology.json\n\n\n\n\n\nThe output indicated that the existing cluster was expanded, rather than creating a new one:\n\n\nFound node node-1.lab on cluster fb67f97166c58f161b85201e1fd9b8ed\n    Found device /dev/xvdc\nFound node node-2.lab on cluster fb67f97166c58f161b85201e1fd9b8ed\n    Found device /dev/xvdc\nFound node node-3.lab on cluster fb67f97166c58f161b85201e1fd9b8ed\n    Found device /dev/xvdc\nCreating node node-4.lab ... ID: 544158e53934a3d351b874b7d915e8d4\n    Adding device /dev/xvdc ... OK\nCreating node node-5.lab ... ID: 645b6edd4044cb1dd828f728d1c3eb81\n    Adding device /dev/xvdc ... OK\nCreating node node-6.lab ... ID: 3f39ebf3c8c82531a7ba447135742776\n    Adding device /dev/xvdc ... OK\n\n\n\n\n\n Log on to one of the GlusterFS pods and confirm their new peers:\n\n\noc rsh glusterfs-0lr75\n\n\n\n\n\n Run the peer status command inside the container remote session:\n\n\ngluster peer status\n\n\n\n\n\nYou should now have a GlusterFS consisting of 6 nodes:\n\n\nNumber of Peers: 5\n\nHostname: 10.0.3.102\nUuid: c6a6d571-fd9b-4bd8-aade-e480ec2f8eed\nState: Peer in Cluster (Connected)\n\nHostname: 10.0.4.103\nUuid: 46044d06-a928-49c6-8427-a7ab37268fed\nState: Peer in Cluster (Connected)\n\nHostname: 10.0.2.104\nUuid: 62abb8b9-7a68-4658-ac84-8098a1460703\nState: Peer in Cluster (Connected)\n\nHostname: 10.0.3.105\nUuid: 5b44b6ea-6fb5-4ea9-a6f7-328179dc6dda\nState: Peer in Cluster (Connected)\n\nHostname: 10.0.4.106\nUuid: ed39ecf7-1f5c-4934-a89d-ee1dda9a8f98\nState: Peer in Cluster (Connected)\n\n\n\n\n\nWith this you have expanded the existing pool. New PVCs will start to use capacity from the additional nodes.\n\n\n\n\nImportant\n\n\nYou now have a GlusterFS pool with mixed media types (both size and speed). It is recommended to have the same media type per pool.\n\nIf you like to offer multiple media types for CNS in OpenShift, use separate pools and separate \nStorageClass\n objects as described in the \nprevious section\n.\n\n\n\n\n\n\nAdding a device to a node\n#\n\n\nInstead of adding entirely new nodes you can also add new storage devices for CNS to use on existing nodes.\n\n\nIt is again possible to do this by loading an updated topology file. Alternatively to bulk-loading via JSON you are also able to do this directly with the \nheketi-cli\n utility. This also applies to the previous sections in this module.\n\n\nFor this purpose \nnode-6.lab\n has an additional, so far unused block device \n/dev/xvdd\n.\n\n\n To use the heketi-cli make sure the environment variables are still set:\n\n\nexport HEKETI_CLI_SERVER=http://heketi-container-native-storage.cloudapps.34.252.58.209.nip.io\nexport HEKETI_CLI_USER=admin\nexport HEKETI_CLI_KEY=myS3cr3tpassw0rd\n\n\n\n\n\n Determine the UUUI heketi uses to identify \nnode-6.lab\n in it\ns database:\n\n\nheketi-cli topology info | grep -B4 node-6.lab\n\n\n\n\n\nNote the UUID\n:\n\n\nNode Id: ae03d2c5de5cdbd44ba27ff5320d3438\n\nState: online\nCluster Id: eb909a08c8e8fd0bf80499fbbb8a8545\nZone: 3\nManagement Hostname: node-6.lab\n\n\n\n\n\n Query the node\ns available devices:\n\n\nheketi-cli node info ae03d2c5de5cdbd44ba27ff5320d343\n\n\n\n\n\nThe node has one device available:\n\n\nNode Id: ae03d2c5de5cdbd44ba27ff5320d3438\nState: online\nCluster Id: eb909a08c8e8fd0bf80499fbbb8a8545\nZone: 3\nManagement Hostname: node-6.lab\nStorage Hostname: 10.0.4.106\nDevices:\nId:cc594d7f5ce59ab2a991c70572a0852f   Name:/dev/xvdc           State:online    Size (GiB):499     Used (GiB):0       Free (GiB):499\n\n\n\n\n\n Add the device \n/dev/xvdd\n to the node using the UUID noted earlier.\n\n\nheketi-cli device add --node=ae03d2c5de5cdbd44ba27ff5320d3438 --name=/dev/xvdd\n\n\n\n\n\nThe device is registered in heketi\ns database.\n\n\nDevice added successfully\n\n\n\n\n\n Query the node\ns available devices again and you\nll see a second device.\n\n\nheketi-cli node info ae03d2c5de5cdbd44ba27ff5320d343\n\nNode Id: ae03d2c5de5cdbd44ba27ff5320d3438\nState: online\nCluster Id: eb909a08c8e8fd0bf80499fbbb8a8545\nZone: 3\nManagement Hostname: node-6.lab\nStorage Hostname: 10.0.4.106\nDevices:\nId:62cbae7a3f6faac38a551a614419cca3   Name:/dev/xvdd           State:online    Size (GiB):499     Used (GiB):0       Free (GiB):499\nId:cc594d7f5ce59ab2a991c70572a0852f   Name:/dev/xvdc           State:online    Size (GiB):499     Used (GiB):0       Free (GiB):499\n\n\n\n\n\n\n\nReplacing a failed device\n#\n\n\nOne of heketi\ns advantages is the automation of otherwise tedious manual tasks, like replacing a faulty brick in GlusterFS to repair degraded volumes.\n\nWe will simulate this use case now.\n\n\n For convenience you can remain operator for now:\n\n\n[ec2-user@master ~]$ oc whoami\noperator\n\n\n\n\n\n Use any available project to submit some PVCs.\n\n\noc project playground\n\n\n\n\n\n Create the file \ncns-large-pvc.yml\n with content below:\n\n\ncns-large-pvc.yml:\n\n\nkind\n:\n \nPersistentVolumeClaim\n\n\napiVersion\n:\n \nv1\n\n\nmetadata\n:\n\n  \nname\n:\n \nmy-large-container-store\n\n  \nannotations\n:\n\n    \nvolume.beta.kubernetes.io/storage-class\n:\n \ncontainer-native-storage\n\n\nspec\n:\n\n  \naccessModes\n:\n\n  \n-\n \nReadWriteOnce\n\n  \nresources\n:\n\n    \nrequests\n:\n\n      \nstorage\n:\n \n200Gi\n\n\n\n\n\n\n Create this request for a large volume:\n\n\noc create -f cns-large-pvc.yml\n\n\n\n\n\nThe requested capacity in this PVC is larger than any single brick on nodes \nnode-1.lab\n, \nnode-2.lab\n and \nnode-3.lab\n so it will be created from the bricks of the other 3 nodes which have larger bricks (500 GiB).\n\n\nWhere are now going to determine a PVCs physical backing device on CNS. This is done with the following relationships between the various entities of GlusterFS, heketi and OpenShift in mind:\n\n\nPVC -\n PV -\n heketi volume -\n GlusterFS volume -\n GlusterFS brick -\n Physical Device\n\n\n Get the PV\n\n\noc describe pvc/my-large-container-store\n\n\n\n\n\nNote the PVs name:\n\n\n    Name:       my-large-container-store\n    Namespace:  container-native-storage\n    StorageClass:   container-native-storage\n    Status:     Bound\n\n    Volume:     pvc-078a1698-4f5b-11e7-ac96-1221f6b873f8\n\n    Labels:     \nnone\n\n    Capacity:   200Gi\n    Access Modes:   RWO\n    No events.\n\n\n\n\n\n Get the GlusterFS volume name of this PV\n\n\noc describe pv/pvc-078a1698-4f5b-11e7-ac96-1221f6b873f8\n\n\n\n\n\nThe GlusterFS volume name as it used by GlusterFS:\n\n\n    Name:       pvc-078a1698-4f5b-11e7-ac96-1221f6b873f8\n    Labels:     \nnone\n\n    StorageClass:   container-native-storage\n    Status:     Bound\n    Claim:      container-native-storage/my-large-container-store\n    Reclaim Policy: Delete\n    Access Modes:   RWO\n    Capacity:   200Gi\n    Message:\n    Source:\n        Type:       Glusterfs (a Glusterfs mount on the host that shares a pod\ns lifetime)\n        EndpointsName:  glusterfs-dynamic-my-large-container-store\n\n        Path:       vol_3ff9946ddafaabe9745f184e4235d4e1\n\n        ReadOnly:       false\n    No events.\n\n\n\n\n\n Get the volumes topology directly from GlusterFS\n\n\noc rsh glusterfs-52hkc\n\ngluster volume info vol_3ff9946ddafaabe9745f184e4235d4e1\n\n\n\n\n\nThe output indicates this volume is indeed backed by, among others, \nnode-6.lab\n\n\nVolume Name: vol_3ff9946ddafaabe9745f184e4235d4e1\nType: Replicate\nVolume ID: 774ae26f-bd3f-4c06-990b-57012cc5974b\nStatus: Started\nSnapshot Count: 0\nNumber of Bricks: 1 x 3 = 3\nTransport-type: tcp\nBricks:\nBrick1: 10.0.3.105:/var/lib/heketi/mounts/vg_e1b93823a2906c6758aeec13930a0919/brick_b3d5867d2f86ac93fce6967128643f85/brick\nBrick2: 10.0.2.104:/var/lib/heketi/mounts/vg_3c3489a5779c1c840a82a26e0117a415/brick_6323bd816f17c8347b3a68e432501e96/brick\n\nBrick3: 10.0.4.106:/var/lib/heketi/mounts/vg_62cbae7a3f6faac38a551a614419cca3/brick_a6c92b6a07983e9b8386871f5b82497f/brick\n\nOptions Reconfigured:\ntransport.address-family: inet\nperformance.readdir-ahead: on\nnfs.disable: on\n\n\n\n\n\n Using the full path of \nBrick3\n we cross-check with heketi\ns topology on which device it is based on:\n\n\nheketi-cli topology info | grep -B2 \n/var/lib/heketi/mounts/vg_62cbae7a3f6faac38a551a614419cca3/brick_a6c92b6a07983e9b8386871f5b82497f/brick\n\n\n\n\n\n\nAmong other results grep will show the physical backing device of this brick\ns mount path:\n\n\nId:62cbae7a3f6faac38a551a614419cca3   Name:/dev/xvdd           State:online    Size (GiB):499     Used (GiB):201     Free (GiB):298\n\n            Bricks:\n                Id:a6c92b6a07983e9b8386871f5b82497f   Size (GiB):200     Path: /var/lib/heketi/mounts/vg_62cbae7a3f6faac38a551a614419cca3/brick_a6c92b6a07983e9b8386871f5b82497f/brick\n\n\n\n\n\nIn this case it\ns \n/dev/vdd\n of \nnode-6.lab\n.\n\n\n\n\nNote\n\n\nThe device might be different for you. This is subject to heketi\ns dynamic scheduling.\n\n\n\n\nLet\ns assume \n/dev/vdd\n on \nnode-6.lab\n has failed and needs to be replaced.\n\n\nIn such a case you\nll take the device\ns ID and go through the following steps:\n\n\n First, disable the device in heketi\n\n\nheketi-cli device disable 62cbae7a3f6faac38a551a614419cca3\n\n\n\n\n\nThis will take the device offline and exclude it from future volume creation requests.\n\n\n Now delete the device in heketi\n\n\nheketi-cli device delete 62cbae7a3f6faac38a551a614419cca3\n\n\n\n\n\nThis will trigger a brick-replacement in GlusterFS. heketi will transparently create new bricks for each brick on the device to be deleted. The replacement operation will be conducted with the new bricks replacing all bricks on the device to be deleted.\n\nThe new bricks, if possible, will automatically be created in zones different from the remaining bricks to maintain equal balancing and cross-zone availability.\n\n\n Check again the volumes topology directly from GlusterFS\n\n\noc rsh glusterfs-52hkc\n\ngluster volume info vol_3ff9946ddafaabe9745f184e4235d4e1\n\n\n\n\n\nYou will notice that \nBrick3\n is now a different mount path, but on the same node.\n\n\nIf you cross-check again the new bricks mount path with the heketi topology you will see it\ns indeed coming from a different device. The remaining device in \nnode-6.lab\n, in this case \n/dev/vdc\n\n\n\n\nNote\n\n\nDevice removal while maintaining volume health is possible in heketi as well. Simply delete all devices of the node in question as discussed above. Then the device can be deleted from heketi with \nheketi-cli device delete \ndevice-uuid", 
            "title": "Module 4 - Cluster Operations"
        }, 
        {
            "location": "/module-4-cluster-ops/#running-multiple-glusterfs-pools", 
            "text": "In the previous modules a single GlusterFS clusters was used to supply  PersistentVolume  to applications. CNS allows for multiple clusters to run in a single OpenShift deployment.  There are several use cases for this:    Provide data isolation between clusters of different tenants    Provide multiple performance tiers of CNS, i.e. HDD-based vs. SSD-based    Overcome the maximum amount of 300 PVs per cluster, currently imposed in version 3.5    To deploy a second GlusterFS pool follow these steps:   Log in as  operator  to namespace  container-native-storage  oc login -u operator -n container-native-storage  Your deployment has 6 OpenShift Application Nodes in total,  node-1 ,  node-2  and  node-3  currently setup running CNS. We will now set up a second cluster using  node-4 ,  node-5  and  node-6 .   Apply additional labels (user-defined key-value pairs) to the remaining 3 OpenShift Nodes:  oc label node/node-4.lab storagenode=glusterfs\noc label node/node-5.lab storagenode=glusterfs\noc label node/node-6.lab storagenode=glusterfs  The label will be used to control GlusterFS pod placement and availability.   Wait for all pods to show  1/1  in the  READY  column:   oc get pods -o wide -n container-native-storage   Note  It may take up to 3 minutes for the GlusterFS pods to transition into  READY  state.   For bulk import of new nodes into a new cluster, the topology JSON file can be updated to include a second cluster with a separate set of nodes.   Create a new file named updated-topology.json with the content below:  updated-topology.json:  { \n     clusters :   [ \n         { \n             nodes :   [ \n                 { \n                     node :   { \n                         hostnames :   { \n                             manage :   [ \n                                 node-1.lab \n                             ], \n                             storage :   [ \n                                 10.0.2.101 \n                             ] \n                         }, \n                         zone :   1 \n                     }, \n                     devices :   [ \n                         /dev/xvdc \n                     ] \n                 }, \n                 { \n                     node :   { \n                         hostnames :   { \n                             manage :   [ \n                                 node-2.lab \n                             ], \n                             storage :   [ \n                                 10.0.3.102 \n                             ] \n                         }, \n                         zone :   2 \n                     }, \n                     devices :   [ \n                         /dev/xvdc \n                     ] \n                 }, \n                 { \n                     node :   { \n                         hostnames :   { \n                             manage :   [ \n                                 node-3.lab \n                             ], \n                             storage :   [ \n                                 10.0.4.103 \n                             ] \n                         }, \n                         zone :   3 \n                     }, \n                     devices :   [ \n                         /dev/xvdc \n                     ] \n                 } \n             ]           },           { \n             nodes :   [ \n                 { \n                     node :   { \n                         hostnames :   { \n                             manage :   [ \n                                 node-4.lab \n                             ], \n                             storage :   [ \n                                 10.0.2.104 \n                             ] \n                         }, \n                         zone :   1 \n                     }, \n                     devices :   [ \n                         /dev/xvdc \n                     ] \n                 }, \n                 { \n                     node :   { \n                         hostnames :   { \n                             manage :   [ \n                                 node-5.lab \n                             ], \n                             storage :   [ \n                                 10.0.3.105 \n                             ] \n                         }, \n                         zone :   2 \n                     }, \n                     devices :   [ \n                         /dev/xvdc \n                     ] \n                 }, \n                 { \n                     node :   { \n                         hostnames :   { \n                             manage :   [ \n                                 node-6.lab \n                             ], \n                             storage :   [ \n                                 10.0.4.106 \n                             ] \n                         }, \n                         zone :   3 \n                     }, \n                     devices :   [ \n                         /dev/xvdc \n                     ] \n                 } \n             ] \n         } \n     ]  }   The file contains the same content as  topology.json  with a second cluster specification (beginning at the highlighted line).  When loading this topology to heketi, it will recognize the existing cluster (leaving it unchanged) and start creating the new one, with the same bootstrapping process as seen with  cns-deploy .   Prepare the heketi CLI tool like previously in  Module 2 .  export HEKETI_CLI_SERVER=http://heketi-container-native-storage.cloudapps.34.252.58.209.nip.io\nexport HEKETI_CLI_USER=admin\nexport HEKETI_CLI_KEY=myS3cr3tpassw0rd   Verify there is currently only a single cluster known to heketi  heketi-cli cluster list  Example output:  Clusters:\nfb67f97166c58f161b85201e1fd9b8ed  Note this cluster s id  for later reference.   Load the new topology with the heketi client  heketi-cli topology load --json=updated-topology.jso  You should see output similar to the following:  Found node node-1.lab on cluster fb67f97166c58f161b85201e1fd9b8ed\nFound device /dev/xvdc\nFound node node-2.lab on cluster fb67f97166c58f161b85201e1fd9b8ed\nFound device /dev/xvdc\nFound node node-3.lab on cluster fb67f97166c58f161b85201e1fd9b8ed\nFound device /dev/xvdc\nCreating cluster ... ID: 46b205a4298c625c4bca2206b7a82dd3\nCreating node node-4.lab ... ID: 604d2eb15a5ca510ff3fc5ecf912d3c0\nAdding device /dev/xvdc ... OK\nCreating node node-5.lab ... ID: 538b860406870288af23af0fbc2cd27f\nAdding device /dev/xvdc ... OK\nCreating node node-6.lab ... ID: 7736bd0cb6a84540860303a6479cacb2\nAdding device /dev/xvdc ... OK  As indicated from above output a new cluster got created.   List all clusters:  heketi-cli cluster list  You should see a second cluster in the list:  Clusters:\n46b205a4298c625c4bca2206b7a82dd3\nfb67f97166c58f161b85201e1fd9b8ed  The second cluster with the ID  46b205a4298c625c4bca2206b7a82dd3  is an entirely independent GlusterFS deployment.  Note this second cluster s ID  as well for later reference.  Now we have two independent GlusterFS clusters managed by the same heketi instance:      Nodes  Cluster UUID      First Cluster  node-1, node-2, node-3  fb67f97166c58f161b85201e1fd9b8ed    Second Cluster  node-4, node-5, node-6  46b205a4298c625c4bca2206b7a82dd3      Query the updated topology:  heketi-cli topology info  Abbreviated output:  Cluster Id: 46b205a4298c625c4bca2206b7a82dd3\n\n    Volumes:\n\n    Nodes:\n\n      Node Id: 538b860406870288af23af0fbc2cd27f\n      State: online\n      Cluster Id: 46b205a4298c625c4bca2206b7a82dd3\n      Zone: 2\n      Management Hostname: node-5.lab\n      Storage Hostname: 10.0.3.105\n      Devices:\n        Id:e481d022cea9bfb11e8a86c0dd8d3499   Name:/dev/xvdc           State:online    Size (GiB):499     Used (GiB):0       Free (GiB):499\n            Bricks:\n\n      Node Id: 604d2eb15a5ca510ff3fc5ecf912d3c0\n      State: online\n      Cluster Id: 46b205a4298c625c4bca2206b7a82dd3\n      Zone: 1\n      Management Hostname: node-4.lab\n      Storage Hostname: 10.0.2.104\n      Devices:\n        Id:09a25a114c53d7669235b368efd2f8d1   Name:/dev/xvdc           State:online    Size (GiB):499     Used (GiB):0       Free (GiB):499\n            Bricks:\n\n      Node Id: 7736bd0cb6a84540860303a6479cacb2\n      State: online\n      Cluster Id: 46b205a4298c625c4bca2206b7a82dd3\n      Zone: 3\n      Management Hostname: node-6.lab\n      Storage Hostname: 10.0.4.106\n      Devices:\n        Id:cccadb2b54dccd99f698d2ae137a22ff   Name:/dev/xvdc           State:online    Size (GiB):499     Used (GiB):0       Free (GiB):499\n            Bricks:\n\nCluster Id: fb67f97166c58f161b85201e1fd9b8ed\n\n[...output omitted for brevity...]  heketi formed an new, independent 3-node GlusterFS cluster on those nodes.   Check running GlusterFS pods  oc get pods -o wide  From the output you can spot the pod names running on the new cluster s nodes:  NAME              READY     STATUS    RESTARTS   AGE       IP           NODE glusterfs-1nvtj   1/1       Running   0          23m       10.0.4.106   node-6.lab glusterfs-5gvw8   1/1       Running   0          24m       10.0.2.104   node-4.lab glusterfs-5rc2g   1/1       Running   0          4h        10.0.2.101   node-1.lab glusterfs-b4wg1   1/1       Running   0          24m       10.0.3.105   node-5.lab glusterfs-jbvdk   1/1       Running   0          4h        10.0.3.102   node-2.lab\nglusterfs-rchtr   1/1       Running   0          4h        10.0.4.103   node-3.lab\nheketi-1-tn0s9    1/1       Running   0          4h        10.130.2.3   node-6.lab   Note  Again note that the pod names are dynamically generated and will be different. Use the FQDN of your hosts to determine one of new cluster s pods.    Log on to one of the new cluster s pods:  oc rsh glusterfs-1nvtj   Query this GlusterFS node for it s peers:  gluster peer status  As expected this node only has 2 peers, evidence that it s running in it s own GlusterFS pool separate from the first cluster in deployed in Module 2.  Number of Peers: 2\n\nHostname: node-5.lab\nUuid: 0db9b5d0-7fa8-4d2f-8b9e-6664faf34606\nState: Peer in Cluster (Connected)\nOther names:\n10.0.3.105\n\nHostname: node-4.lab\nUuid: 695b661d-2a55-4f94-b22e-40a9db79c01a\nState: Peer in Cluster (Connected)  Before you can use the second cluster two tasks have to be accomplished:    The  StorageClass  for the first cluster has to be updated to point the first cluster s UUID,    A second  StorageClass  for the second cluster has to be created, pointing to the same heketi     Why do we need to update the first  StorageClass ?  By default, when no cluster UUID is specified, heketi serves volume creation requests from any cluster currently registered to it. \nIn order to request a volume from specific cluster you have to supply the cluster s UUID to heketi. The first  StorageClass  has no UUID specified so far.    Edit the existing  StorageClass  definition by updating the  cns-storageclass.yml  file from  Module 3  like below:  cns-storageclass.yml:  apiVersion :   storage.k8s.io/v1beta1  kind :   StorageClass  metadata : \n   name :   container-native-storage \n   annotations : \n     storageclass.beta.kubernetes.io/is-default-class :   true  provisioner :   kubernetes.io/glusterfs  parameters : \n   resturl :   http://heketi-container-native-storage.cloudapps.34.252.58.209.nip.io \n   restauthenabled :   true \n   restuser :   admin \n   volumetype :   replicate:3 \n   secretNamespace :   default \n   secretName :   cns-secret     clusterid :   fb67f97166c58f161b85201e1fd9b8ed   Note the additional  clusterid  parameter highlighted. It s the cluster s UUID as known by heketi. Don t change anything else.   Delete the existing  StorageClass  definition in OpenShift  oc delete storageclass/container-native-storage   Add the  StorageClass  again:  oc create -f cns-storageclass.yml   Create a new file called cns-storageclass-slow.yml with the following contents:  cns-storageclass-slow.yml:  apiVersion :   storage.k8s.io/v1beta1  kind :   StorageClass  metadata : \n   name :   container-native-storage-slow  provisioner :   kubernetes.io/glusterfs  parameters : \n   resturl :   http://heketi-container-native-storage.cloudapps.34.252.58.209.nip.io \n   restauthenabled :   true \n   restuser :   admin \n   volumetype :   replicate:3 \n   secretNamespace :   default \n   secretName :   cns-secret     clusterid :   46b205a4298c625c4bca2206b7a82dd3   Again note the  clusterid  in the  parameters  section referencing the second cluster s UUID   Add the new  StorageClass :  oc create -f cns-storageclass-slow.yml  Let s verify both  StorageClasses  are working as expected:   Create the following two files containing PVCs issued against either of both GlusterFS pools via their respective  StorageClass :  cns-pvc-fast.yml:  kind :   PersistentVolumeClaim  apiVersion :   v1  metadata : \n   name :   my-container-storage-fast \n   annotations : \n     volume.beta.kubernetes.io/storage-class :   container-native-storage  spec : \n   accessModes : \n   -   ReadWriteOnce \n   resources : \n     requests : \n       storage :   5Gi   cns-pvc-slow.yml:  kind :   PersistentVolumeClaim  apiVersion :   v1  metadata : \n   name :   my-container-storage-slow \n   annotations : \n     volume.beta.kubernetes.io/storage-class :   container-native-storage-slow  spec : \n   accessModes : \n   -   ReadWriteOnce \n   resources : \n     requests : \n       storage :   7Gi    Create both PVCs:  oc create -f cns-pvc-fast.yml\noc create -f cns-pvc-slow.yml  They should both be in bound state after a couple of seconds:  NAME                        STATUS    VOLUME                                     CAPACITY   ACCESSMODES   AGE\nmy-container-storage        Bound     pvc-3e249fdd-4ec0-11e7-970e-0a9938370404   5Gi        RWO           32s\nmy-container-storage-slow   Bound     pvc-405083e6-4ec0-11e7-970e-0a9938370404   7Gi        RWO           29s   If you check again one of the pods of the second cluster  oc rsh glusterfs-1nvtj    you will see a new volume has been created  sh-4.2# gluster volume list\nvol_0d976f357a7979060a4c32284ca8e136  This is how you use multiple, parallel GlusterFS pools/clusters on a single OpenShift cluster with a single heketi instance. Whereas the first pool is created with  cns-deploy  subsequent pools/cluster are created with the  heketi-cli  client.  Clean up the PVCs and the second  StorageClass  in preparation for the next section.   Delete both PVCs (and therefore their volume)  oc delete pvc/my-container-storage-fast\noc delete pvc/my-container-storage-slow   Delete the second  StorageClass  oc delete storageclass/container-native-storage-slow", 
            "title": "Running multiple GlusterFS pools"
        }, 
        {
            "location": "/module-4-cluster-ops/#deleting-a-glusterfs-pool", 
            "text": "Since we want to re-use  node-4 ,  node-5  and  node-6  for the next section we need to delete it the GlusterFS pools on top of them first.  This is a process that involves multiple steps of manipulating the heketi topology with the  heketi-cli  client.   Make sure the client is still properly configured via environment variables:  export HEKETI_CLI_SERVER=http://heketi-container-native-storage.cloudapps.34.252.58.209.nip.io\nexport HEKETI_CLI_USER=admin\nexport HEKETI_CLI_KEY=myS3cr3tpassw0rd   First list the topology and identify the cluster we want to delete:  heketi-cli topology info  The portions of interest are highlighted:  Cluster Id: 46b205a4298c625c4bca2206b7a82dd3 \n    Volumes:\n\n    Nodes:         Node Id: 538b860406870288af23af0fbc2cd27f         State: online\n        Cluster Id: 46b205a4298c625c4bca2206b7a82dd3\n        Zone: 2\n        Management Hostname: node-5.lab\n        Storage Hostname: 10.0.3.105\n        Devices:             Id:e481d022cea9bfb11e8a86c0dd8d3499   Name:/dev/xvdc           State:online    Size (GiB):499     Used (GiB):0       Free (GiB):499                 Bricks:         Node Id: 604d2eb15a5ca510ff3fc5ecf912d3c0         State: online\n        Cluster Id: 46b205a4298c625c4bca2206b7a82dd3\n        Zone: 1\n        Management Hostname: node-4.lab\n        Storage Hostname: 10.0.2.104\n        Devices:             Id:09a25a114c53d7669235b368efd2f8d1   Name:/dev/xvdc           State:online    Size (GiB):499     Used (GiB):0       Free (GiB):499                 Bricks:         Node Id: 7736bd0cb6a84540860303a6479cacb2         State: online\n        Cluster Id: 46b205a4298c625c4bca2206b7a82dd3\n        Zone: 3\n        Management Hostname: node-6.lab\n        Storage Hostname: 10.0.4.106\n        Devices:             Id:cccadb2b54dccd99f698d2ae137a22ff   Name:/dev/xvdc           State:online    Size (GiB):499     Used (GiB):0       Free (GiB):499             Bricks:  The hierachical dependencies in this topology works as follows: Clusters   Nodes   Devices. \nAssuming there are no volumes present these need to be deleted in reverse order.  Note that specific values highlighted above in your environment and carry out the following steps in strict order:   Delete all devices of all nodes:  heketi-cli device delete e481d022cea9bfb11e8a86c0dd8d349\nheketi-cli device delete 09a25a114c53d7669235b368efd2f8d1\nheketi-cli device delete cccadb2b54dccd99f698d2ae137a22ff   Delete all nodes of the cluster in question:  heketi-cli node delete 538b860406870288af23af0fbc2cd27f\nheketi-cli node delete 604d2eb15a5ca510ff3fc5ecf912d3c0\nheketi-cli node delete 7736bd0cb6a84540860303a6479cacb2   Finally delete the cluster:  heketi-cli cluster delete 46b205a4298c625c4bca2206b7a82dd3   Confirm the cluster is gone:  heketi-cli cluster list   Verify the clusters new topology back to it s state from Module 3.  This deleted all heketi database entries about the cluster. However the GlusterFS pods are still running, since they are controlled directly by OpenShift instead of heketi:   List all the pods in the  container-native-storage  namespace:  oc get pods -o wide -n container-native-storage  The pods formerly running the second GlusterFS pool are still there:  NAME              READY     STATUS    RESTARTS   AGE       IP           NODE\nglusterfs-1nvtj   1/1       Running   0          1h       10.0.4.106   node-6.lab\nglusterfs-5gvw8   1/1       Running   0          1h       10.0.2.104   node-4.lab\nglusterfs-5rc2g   1/1       Running   0          4h       10.0.2.101   node-1.lab\nglusterfs-b4wg1   1/1       Running   0          1h       10.0.3.105   node-5.lab\nglusterfs-jbvdk   1/1       Running   0          4h       10.0.3.102   node-2.lab\nglusterfs-rchtr   1/1       Running   0          4h       10.0.4.103   node-3.lab\nheketi-1-tn0s9    1/1       Running   0          4h       10.130.2.3   node-6.lab  They can be stopped by removing the labels OpenShift uses to determine GlusterFS pod placement for CNS.   Remove the labels from the last 3 OpenShift nodes like so:  oc label node/node-4.lab storagenode-\noc label node/node-5.lab storagenode-\noc label node/node-6.lab storagenode-  Contrary to the output of these commands the label  storagenode  is actually removed.   Verify that all GlusterFS pods running on  node-4 ,  node-5  and  node-6  are indeed terminated:  oc get pods -o wide -n container-native-storage   Note  It can take up to 2 minutes for the pods to terminate.   You should be back down to 3 GlusterFS pods.  NAME              READY     STATUS    RESTARTS   AGE       IP           NODE\nglusterfs-5rc2g   1/1       Running   0          5h        10.0.2.101   node-1.lab\nglusterfs-jbvdk   1/1       Running   0          5h        10.0.3.102   node-2.lab\nglusterfs-rchtr   1/1       Running   0          5h        10.0.4.103   node-3.lab\nheketi-1-tn0s9    1/1       Running   0          5h        10.130.2.3   node-6.lab", 
            "title": "Deleting a GlusterFS pool"
        }, 
        {
            "location": "/module-4-cluster-ops/#expanding-a-glusterfs-pool", 
            "text": "Instead of creating additional GlusterFS pools in CNS on OpenShift it is also possible to expand existing pools.  This works similar to creating additional pools, with bulk-import via the topology file. Only this time with nodes added to the existing cluster structure in JSON.  Since manipulating JSON can be error-prone create a new file called  expanded-topology.json  with contents as below:  expanded-topology.json:  { \n     clusters :   [ \n         { \n             nodes :   [ \n                 { \n                     node :   { \n                         hostnames :   { \n                             manage :   [ \n                                 node-1.lab \n                             ], \n                             storage :   [ \n                                 10.0.2.101 \n                             ] \n                         }, \n                         zone :   1 \n                     }, \n                     devices :   [ \n                         /dev/xvdc \n                     ] \n                 }, \n                 { \n                     node :   { \n                         hostnames :   { \n                             manage :   [ \n                                 node-2.lab \n                             ], \n                             storage :   [ \n                                 10.0.3.102 \n                             ] \n                         }, \n                         zone :   2 \n                     }, \n                     devices :   [ \n                         /dev/xvdc \n                     ] \n                 }, \n                 { \n                     node :   { \n                         hostnames :   { \n                             manage :   [ \n                                 node-3.lab \n                             ], \n                             storage :   [ \n                                 10.0.4.103 \n                             ] \n                         }, \n                         zone :   3 \n                     }, \n                     devices :   [ \n                         /dev/xvdc \n                     ] \n                 }, \n                 { \n                     node :   { \n                         hostnames :   { \n                             manage :   [ \n                                 node-4.lab \n                             ], \n                             storage :   [ \n                                 10.0.2.104 \n                             ] \n                         }, \n                         zone :   1 \n                     }, \n                     devices :   [ \n                         /dev/xvdc \n                     ] \n                 }, \n                 { \n                     node :   { \n                         hostnames :   { \n                             manage :   [ \n                                 node-5.lab \n                             ], \n                             storage :   [ \n                                 10.0.3.105 \n                             ] \n                         }, \n                         zone :   2 \n                     }, \n                     devices :   [ \n                         /dev/xvdc \n                     ] \n                 }, \n                 { \n                     node :   { \n                         hostnames :   { \n                             manage :   [ \n                                 node-6.lab \n                             ], \n                             storage :   [ \n                                 10.0.4.106 \n                             ] \n                         }, \n                         zone :   3 \n                     }, \n                     devices :   [ \n                         /dev/xvdc \n                     ] \n                 } \n             ] \n         } \n     ]  }    Again, apply the expected labels to the remaining 3 OpenShift Nodes:  oc label node/node-4.lab storagenode=glusterfs\noc label node/node-5.lab storagenode=glusterfs\noc label node/node-6.lab storagenode=glusterfs   Wait for all pods to show  1/1  in the  READY  column:   oc get pods -o wide -n container-native-storage   Note  It may take up to 3 minutes for the GlusterFS pods to transition into  READY  state.   This confirms all GlusterFS pods are ready to receive remote commands:  NAME              READY     STATUS    RESTARTS   AGE       IP           NODE\nglusterfs-0lr75   1/1       Running   0          4m        10.0.4.106   node-6.lab\nglusterfs-1dxz3   1/1       Running   0          4m        10.0.3.105   node-5.lab\nglusterfs-5rc2g   1/1       Running   0          5h        10.0.2.101   node-1.lab\nglusterfs-8nrn0   1/1       Running   0          4m        10.0.2.104   node-4.lab\nglusterfs-jbvdk   1/1       Running   0          5h        10.0.3.102   node-2.lab\nglusterfs-rchtr   1/1       Running   0          5h        10.0.4.103   node-3.lab\nheketi-1-tn0s9    1/1       Running   0          5h        10.130.2.3   node-6.lab   If you logged out of the session meanwhile re-instate the heketi-cli configuration with the environment variables;  export HEKETI_CLI_SERVER=http://heketi-container-native-storage.cloudapps.34.252.58.209.nip.io\nexport HEKETI_CLI_USER=admin\nexport HEKETI_CLI_KEY=myS3cr3tpassw0rd   Now load the new topology:  heketi-cli topology load --json=expanded-topology.json  The output indicated that the existing cluster was expanded, rather than creating a new one:  Found node node-1.lab on cluster fb67f97166c58f161b85201e1fd9b8ed\n    Found device /dev/xvdc\nFound node node-2.lab on cluster fb67f97166c58f161b85201e1fd9b8ed\n    Found device /dev/xvdc\nFound node node-3.lab on cluster fb67f97166c58f161b85201e1fd9b8ed\n    Found device /dev/xvdc\nCreating node node-4.lab ... ID: 544158e53934a3d351b874b7d915e8d4\n    Adding device /dev/xvdc ... OK\nCreating node node-5.lab ... ID: 645b6edd4044cb1dd828f728d1c3eb81\n    Adding device /dev/xvdc ... OK\nCreating node node-6.lab ... ID: 3f39ebf3c8c82531a7ba447135742776\n    Adding device /dev/xvdc ... OK   Log on to one of the GlusterFS pods and confirm their new peers:  oc rsh glusterfs-0lr75   Run the peer status command inside the container remote session:  gluster peer status  You should now have a GlusterFS consisting of 6 nodes:  Number of Peers: 5\n\nHostname: 10.0.3.102\nUuid: c6a6d571-fd9b-4bd8-aade-e480ec2f8eed\nState: Peer in Cluster (Connected)\n\nHostname: 10.0.4.103\nUuid: 46044d06-a928-49c6-8427-a7ab37268fed\nState: Peer in Cluster (Connected)\n\nHostname: 10.0.2.104\nUuid: 62abb8b9-7a68-4658-ac84-8098a1460703\nState: Peer in Cluster (Connected)\n\nHostname: 10.0.3.105\nUuid: 5b44b6ea-6fb5-4ea9-a6f7-328179dc6dda\nState: Peer in Cluster (Connected)\n\nHostname: 10.0.4.106\nUuid: ed39ecf7-1f5c-4934-a89d-ee1dda9a8f98\nState: Peer in Cluster (Connected)  With this you have expanded the existing pool. New PVCs will start to use capacity from the additional nodes.   Important  You now have a GlusterFS pool with mixed media types (both size and speed). It is recommended to have the same media type per pool. \nIf you like to offer multiple media types for CNS in OpenShift, use separate pools and separate  StorageClass  objects as described in the  previous section .", 
            "title": "Expanding a GlusterFS pool"
        }, 
        {
            "location": "/module-4-cluster-ops/#adding-a-device-to-a-node", 
            "text": "Instead of adding entirely new nodes you can also add new storage devices for CNS to use on existing nodes.  It is again possible to do this by loading an updated topology file. Alternatively to bulk-loading via JSON you are also able to do this directly with the  heketi-cli  utility. This also applies to the previous sections in this module.  For this purpose  node-6.lab  has an additional, so far unused block device  /dev/xvdd .   To use the heketi-cli make sure the environment variables are still set:  export HEKETI_CLI_SERVER=http://heketi-container-native-storage.cloudapps.34.252.58.209.nip.io\nexport HEKETI_CLI_USER=admin\nexport HEKETI_CLI_KEY=myS3cr3tpassw0rd   Determine the UUUI heketi uses to identify  node-6.lab  in it s database:  heketi-cli topology info | grep -B4 node-6.lab  Note the UUID :  Node Id: ae03d2c5de5cdbd44ba27ff5320d3438 State: online\nCluster Id: eb909a08c8e8fd0bf80499fbbb8a8545\nZone: 3\nManagement Hostname: node-6.lab   Query the node s available devices:  heketi-cli node info ae03d2c5de5cdbd44ba27ff5320d343  The node has one device available:  Node Id: ae03d2c5de5cdbd44ba27ff5320d3438\nState: online\nCluster Id: eb909a08c8e8fd0bf80499fbbb8a8545\nZone: 3\nManagement Hostname: node-6.lab\nStorage Hostname: 10.0.4.106\nDevices:\nId:cc594d7f5ce59ab2a991c70572a0852f   Name:/dev/xvdc           State:online    Size (GiB):499     Used (GiB):0       Free (GiB):499   Add the device  /dev/xvdd  to the node using the UUID noted earlier.  heketi-cli device add --node=ae03d2c5de5cdbd44ba27ff5320d3438 --name=/dev/xvdd  The device is registered in heketi s database.  Device added successfully   Query the node s available devices again and you ll see a second device.  heketi-cli node info ae03d2c5de5cdbd44ba27ff5320d343\n\nNode Id: ae03d2c5de5cdbd44ba27ff5320d3438\nState: online\nCluster Id: eb909a08c8e8fd0bf80499fbbb8a8545\nZone: 3\nManagement Hostname: node-6.lab\nStorage Hostname: 10.0.4.106\nDevices:\nId:62cbae7a3f6faac38a551a614419cca3   Name:/dev/xvdd           State:online    Size (GiB):499     Used (GiB):0       Free (GiB):499\nId:cc594d7f5ce59ab2a991c70572a0852f   Name:/dev/xvdc           State:online    Size (GiB):499     Used (GiB):0       Free (GiB):499", 
            "title": "Adding a device to a node"
        }, 
        {
            "location": "/module-4-cluster-ops/#replacing-a-failed-device", 
            "text": "One of heketi s advantages is the automation of otherwise tedious manual tasks, like replacing a faulty brick in GlusterFS to repair degraded volumes. \nWe will simulate this use case now.   For convenience you can remain operator for now:  [ec2-user@master ~]$ oc whoami\noperator   Use any available project to submit some PVCs.  oc project playground   Create the file  cns-large-pvc.yml  with content below:  cns-large-pvc.yml:  kind :   PersistentVolumeClaim  apiVersion :   v1  metadata : \n   name :   my-large-container-store \n   annotations : \n     volume.beta.kubernetes.io/storage-class :   container-native-storage  spec : \n   accessModes : \n   -   ReadWriteOnce \n   resources : \n     requests : \n       storage :   200Gi    Create this request for a large volume:  oc create -f cns-large-pvc.yml  The requested capacity in this PVC is larger than any single brick on nodes  node-1.lab ,  node-2.lab  and  node-3.lab  so it will be created from the bricks of the other 3 nodes which have larger bricks (500 GiB).  Where are now going to determine a PVCs physical backing device on CNS. This is done with the following relationships between the various entities of GlusterFS, heketi and OpenShift in mind:  PVC -  PV -  heketi volume -  GlusterFS volume -  GlusterFS brick -  Physical Device   Get the PV  oc describe pvc/my-large-container-store  Note the PVs name:      Name:       my-large-container-store\n    Namespace:  container-native-storage\n    StorageClass:   container-native-storage\n    Status:     Bound     Volume:     pvc-078a1698-4f5b-11e7-ac96-1221f6b873f8     Labels:      none \n    Capacity:   200Gi\n    Access Modes:   RWO\n    No events.   Get the GlusterFS volume name of this PV  oc describe pv/pvc-078a1698-4f5b-11e7-ac96-1221f6b873f8  The GlusterFS volume name as it used by GlusterFS:      Name:       pvc-078a1698-4f5b-11e7-ac96-1221f6b873f8\n    Labels:      none \n    StorageClass:   container-native-storage\n    Status:     Bound\n    Claim:      container-native-storage/my-large-container-store\n    Reclaim Policy: Delete\n    Access Modes:   RWO\n    Capacity:   200Gi\n    Message:\n    Source:\n        Type:       Glusterfs (a Glusterfs mount on the host that shares a pod s lifetime)\n        EndpointsName:  glusterfs-dynamic-my-large-container-store         Path:       vol_3ff9946ddafaabe9745f184e4235d4e1         ReadOnly:       false\n    No events.   Get the volumes topology directly from GlusterFS  oc rsh glusterfs-52hkc\n\ngluster volume info vol_3ff9946ddafaabe9745f184e4235d4e1  The output indicates this volume is indeed backed by, among others,  node-6.lab  Volume Name: vol_3ff9946ddafaabe9745f184e4235d4e1\nType: Replicate\nVolume ID: 774ae26f-bd3f-4c06-990b-57012cc5974b\nStatus: Started\nSnapshot Count: 0\nNumber of Bricks: 1 x 3 = 3\nTransport-type: tcp\nBricks:\nBrick1: 10.0.3.105:/var/lib/heketi/mounts/vg_e1b93823a2906c6758aeec13930a0919/brick_b3d5867d2f86ac93fce6967128643f85/brick\nBrick2: 10.0.2.104:/var/lib/heketi/mounts/vg_3c3489a5779c1c840a82a26e0117a415/brick_6323bd816f17c8347b3a68e432501e96/brick Brick3: 10.0.4.106:/var/lib/heketi/mounts/vg_62cbae7a3f6faac38a551a614419cca3/brick_a6c92b6a07983e9b8386871f5b82497f/brick Options Reconfigured:\ntransport.address-family: inet\nperformance.readdir-ahead: on\nnfs.disable: on   Using the full path of  Brick3  we cross-check with heketi s topology on which device it is based on:  heketi-cli topology info | grep -B2  /var/lib/heketi/mounts/vg_62cbae7a3f6faac38a551a614419cca3/brick_a6c92b6a07983e9b8386871f5b82497f/brick   Among other results grep will show the physical backing device of this brick s mount path:  Id:62cbae7a3f6faac38a551a614419cca3   Name:/dev/xvdd           State:online    Size (GiB):499     Used (GiB):201     Free (GiB):298             Bricks:\n                Id:a6c92b6a07983e9b8386871f5b82497f   Size (GiB):200     Path: /var/lib/heketi/mounts/vg_62cbae7a3f6faac38a551a614419cca3/brick_a6c92b6a07983e9b8386871f5b82497f/brick  In this case it s  /dev/vdd  of  node-6.lab .   Note  The device might be different for you. This is subject to heketi s dynamic scheduling.   Let s assume  /dev/vdd  on  node-6.lab  has failed and needs to be replaced.  In such a case you ll take the device s ID and go through the following steps:   First, disable the device in heketi  heketi-cli device disable 62cbae7a3f6faac38a551a614419cca3  This will take the device offline and exclude it from future volume creation requests.   Now delete the device in heketi  heketi-cli device delete 62cbae7a3f6faac38a551a614419cca3  This will trigger a brick-replacement in GlusterFS. heketi will transparently create new bricks for each brick on the device to be deleted. The replacement operation will be conducted with the new bricks replacing all bricks on the device to be deleted. \nThe new bricks, if possible, will automatically be created in zones different from the remaining bricks to maintain equal balancing and cross-zone availability.   Check again the volumes topology directly from GlusterFS  oc rsh glusterfs-52hkc\n\ngluster volume info vol_3ff9946ddafaabe9745f184e4235d4e1  You will notice that  Brick3  is now a different mount path, but on the same node.  If you cross-check again the new bricks mount path with the heketi topology you will see it s indeed coming from a different device. The remaining device in  node-6.lab , in this case  /dev/vdc   Note  Device removal while maintaining volume health is possible in heketi as well. Simply delete all devices of the node in question as discussed above. Then the device can be deleted from heketi with  heketi-cli device delete  device-uuid", 
            "title": "Replacing a failed device"
        }, 
        {
            "location": "/module-5-advanced/", 
            "text": "Overview\n\n\nIn this module a couple of advanced topics are discussed. Some of them are in \nTechnology Preview\n state at the moment and not supported in production.\n\n\n\n\nCNS with other volumes types\n#\n\n\nThe only volume type supported with CNS in production is 3-way replicated.\n\nFor demonstration purposes you however can also created distributed volumes (no replication) and dispersed volume (4:2 erasure-coding).\n\n\nCurrently volume type is implemented as a parameter in the \nStorageClass\n, not the PVC. All volumes of this \nStorageClass\n will be of the specified type.\n\n\n Create a file called \ncns-storageclass-dispersed.yml\n with the following content:\n\n\ncns-storageclass-dispersed.yml:\n\n\napiVersion\n:\n \nstorage.k8s.io/v1beta1\n\n\nkind\n:\n \nStorageClass\n\n\nmetadata\n:\n\n  \nname\n:\n \ncontainer-native-storage-dispersed\n\n\nprovisioner\n:\n \nkubernetes.io/glusterfs\n\n\nparameters\n:\n\n  \nresturl\n:\n \nhttp://heketi-container-native-storage.cloudapps.34.252.58.209.nip.io\n\n  \nrestauthenabled\n:\n \ntrue\n\n  \nrestuser\n:\n \nadmin\n\n  \nvolumetype\n:\n \ndispersed:4:2\n\n  \nsecretNamespace\n:\n \ndefault\n\n  \nsecretName\n:\n \ncns-secret\n\n\n\n\n\n\nThis \nStorageClass\n called \ncontainer-native-storage-dispersed\n uses the same GlusterFS pool and the same heketi credentials to create volumes, with the difference of the \nvolumetype\n parameter. It will cause the GlusterFS Dynamic Provisioner (the driver in OpenShift calling heketi\ns API) to request dispersed volumes.\n\n\n\n\nImportant\n\n\nFor a 4:2 erasure-coding volume to be healthy at minimum 6 nodes are needed.\n\n\n\n\n\n\nOpenShift Registry on CNS\n#\n\n\nThe Registry in OpenShift is used to store all images that result of Source-to-Image deployments as well as general purpose container image storage.\n\nIt runs as one or more containers in specific Infrastructure Nodes or Master Nodes in OpenShift.\n\n\nBy default the registry uses a hosts local storage which makes it prone to outages. Also, multiple registry pods need shared storage.\n\n\nThis can be achieved with CNS simply by making the registry pods refer to a PVC in access mode \nRWX\n based on CNS.\n\n\n\n\nImportant\n\n\nThis method will be disruptive. All data stored in the registry so far will become unavailable.\n\nMigration scenarios exist but are beyond the scope of this lab.\n\n\n\n\n Make sure you are logged in as \noperator\n in the \ndefault\n namespace:\n\n\noc login -u operator -n default\n\n\n\n\n\n Create a PVC for shared storage like the file \ncns-registry-pvc.yml\n below:\n\n\ncns-registry-pvc.yml:\n\n\nkind\n:\n \nPersistentVolumeClaim\n\n\napiVersion\n:\n \nv1\n\n\nmetadata\n:\n\n  \nname\n:\n \nregistry-storage\n\n  \nannotations\n:\n\n    \nvolume.beta.kubernetes.io/storage-class\n:\n \ncontainer-native-storage\n\n\nspec\n:\n\n  \naccessModes\n:\n\n  \n-\n \nReadWriteMany\n\n  \nresources\n:\n\n    \nrequests\n:\n\n      \nstorage\n:\n \n20Gi\n\n\n\n\n\n\nCreate the PVC and ensure it\ns \nBOUND\n\n\noc create -f cns-registry-pvc.yml\n\n\n\n\n\nIn your environment a registry is already running. This will be the case for most environments. So the existing registry configuration needs to be adjusted to include a PVC and make the pods mount it\ns volume.\n\nThis is done by modifying the \nDeploymentConfig\n of the registry.\n\n\n Update the registry\ns \nDeploymentConfig\n to refer to the PVC created before:\n\n\nThe registry will now redeploy.\n\n\n Observe the registry deployment get updated:\n\n\nWith this the OpenShift Registry is based on persistent storage provided by CNS. Since this is shared storage this also allows to scale out the registry pods. \n\n\n\n\nRunning heketi on the Master\n#\n\n\nIn this exercise, and by default, the heketi pod runs on OpenShift Application Nodes, co-located with the GlusterFS pods and application pods.\n\n\nIn production deployments facilities like the OpenShift Registry or the OpenShift Router are running in pods on special nodes called \nInfrastructure Nodes\n. It is a good practice to do the same with heketi.\n\n\n Make sure you are logged in as \noperator\n in the \ndefault\n namespace:\n\n\noc login -u operator -n container-native-storage\n\n\n\n\n\n Display all pods in this namespace\n\n\noc get pods -o wide\n\n\n\n\n\nYou will notice the heketi pod running among the GlusterFS pods:\n\n\n Modify the registry\ns \nDeploymentConfig\n as follows:\n\n\nThe \nnodeSelector\n is basically a search predicate applied when making scheduling decision for pods among nodes.\n\nIn this environment the master node also acts as an infra node and therefore carries the label \nregion=infra\n.\n\n\nBy modifying the \nDeploymentConfig\n the heketi pod will be recreated and scheduled on the Master. It\ns data is persisted in a BoltDB instance hosted outside of the pod on a GlusterFS volume from the first cluster in CNS.", 
            "title": "Module 5 - Advanced Topics"
        }, 
        {
            "location": "/module-5-advanced/#cns-with-other-volumes-types", 
            "text": "The only volume type supported with CNS in production is 3-way replicated. \nFor demonstration purposes you however can also created distributed volumes (no replication) and dispersed volume (4:2 erasure-coding).  Currently volume type is implemented as a parameter in the  StorageClass , not the PVC. All volumes of this  StorageClass  will be of the specified type.   Create a file called  cns-storageclass-dispersed.yml  with the following content:  cns-storageclass-dispersed.yml:  apiVersion :   storage.k8s.io/v1beta1  kind :   StorageClass  metadata : \n   name :   container-native-storage-dispersed  provisioner :   kubernetes.io/glusterfs  parameters : \n   resturl :   http://heketi-container-native-storage.cloudapps.34.252.58.209.nip.io \n   restauthenabled :   true \n   restuser :   admin \n   volumetype :   dispersed:4:2 \n   secretNamespace :   default \n   secretName :   cns-secret   This  StorageClass  called  container-native-storage-dispersed  uses the same GlusterFS pool and the same heketi credentials to create volumes, with the difference of the  volumetype  parameter. It will cause the GlusterFS Dynamic Provisioner (the driver in OpenShift calling heketi s API) to request dispersed volumes.   Important  For a 4:2 erasure-coding volume to be healthy at minimum 6 nodes are needed.", 
            "title": "CNS with other volumes types"
        }, 
        {
            "location": "/module-5-advanced/#openshift-registry-on-cns", 
            "text": "The Registry in OpenShift is used to store all images that result of Source-to-Image deployments as well as general purpose container image storage. \nIt runs as one or more containers in specific Infrastructure Nodes or Master Nodes in OpenShift.  By default the registry uses a hosts local storage which makes it prone to outages. Also, multiple registry pods need shared storage.  This can be achieved with CNS simply by making the registry pods refer to a PVC in access mode  RWX  based on CNS.   Important  This method will be disruptive. All data stored in the registry so far will become unavailable. \nMigration scenarios exist but are beyond the scope of this lab.    Make sure you are logged in as  operator  in the  default  namespace:  oc login -u operator -n default   Create a PVC for shared storage like the file  cns-registry-pvc.yml  below:  cns-registry-pvc.yml:  kind :   PersistentVolumeClaim  apiVersion :   v1  metadata : \n   name :   registry-storage \n   annotations : \n     volume.beta.kubernetes.io/storage-class :   container-native-storage  spec : \n   accessModes : \n   -   ReadWriteMany \n   resources : \n     requests : \n       storage :   20Gi   Create the PVC and ensure it s  BOUND  oc create -f cns-registry-pvc.yml  In your environment a registry is already running. This will be the case for most environments. So the existing registry configuration needs to be adjusted to include a PVC and make the pods mount it s volume. \nThis is done by modifying the  DeploymentConfig  of the registry.   Update the registry s  DeploymentConfig  to refer to the PVC created before:  The registry will now redeploy.   Observe the registry deployment get updated:  With this the OpenShift Registry is based on persistent storage provided by CNS. Since this is shared storage this also allows to scale out the registry pods.", 
            "title": "OpenShift Registry on CNS"
        }, 
        {
            "location": "/module-5-advanced/#running-heketi-on-the-master", 
            "text": "In this exercise, and by default, the heketi pod runs on OpenShift Application Nodes, co-located with the GlusterFS pods and application pods.  In production deployments facilities like the OpenShift Registry or the OpenShift Router are running in pods on special nodes called  Infrastructure Nodes . It is a good practice to do the same with heketi.   Make sure you are logged in as  operator  in the  default  namespace:  oc login -u operator -n container-native-storage   Display all pods in this namespace  oc get pods -o wide  You will notice the heketi pod running among the GlusterFS pods:   Modify the registry s  DeploymentConfig  as follows:  The  nodeSelector  is basically a search predicate applied when making scheduling decision for pods among nodes. \nIn this environment the master node also acts as an infra node and therefore carries the label  region=infra .  By modifying the  DeploymentConfig  the heketi pod will be recreated and scheduled on the Master. It s data is persisted in a BoltDB instance hosted outside of the pod on a GlusterFS volume from the first cluster in CNS.", 
            "title": "Running heketi on the Master"
        }
    ]
}