{
    "docs": [
        {
            "location": "/", 
            "text": "Welcome to Container-Native Storage Hands-on Lab\n#\n\n\nFor full documentation visit \nmkdocs.org\n.\n\n\nCommands\n#\n\n\n\n\nmkdocs new [dir-name]\n - Create a new project.\n\n\nmkdocs serve\n - Start the live-reloading docs server.\n\n\nmkdocs build\n - Build the documentation site.\n\n\nmkdocs help\n - Print this help message.\n\n\n\n\nCode\n#\n\n\n# This line is emphasized\n\n\n# This line isn\nt\n\n\n# This line is emphasized\n\n\n\n\n\n\n Bubble sort \n\n\ndef\n \nbubble_sort\n(\nitems\n):\n\n    \nfor\n \ni\n \nin\n \nrange\n(\nlen\n(\nitems\n)):\n\n        \nfor\n \nj\n \nin\n \nrange\n(\nlen\n(\nitems\n)\n \n-\n \n1\n \n-\n \ni\n):\n\n            \nif\n \nitems\n[\nj\n]\n \n \nitems\n[\nj\n \n+\n \n1\n]:\n\n                \nitems\n[\nj\n],\n \nitems\n[\nj\n \n+\n \n1\n]\n \n=\n \nitems\n[\nj\n \n+\n \n1\n],\n \nitems\n[\nj\n]\n\n\n\n\n\n\nProject layout\n#\n\n\nmkdocs.yml    # The configuration file.\ndocs/\n    index.md  # The documentation homepage.\n    ...       # Other markdown pages, images and other files.", 
            "title": "Overview"
        }, 
        {
            "location": "/#welcome-to-container-native-storage-hands-on-lab", 
            "text": "For full documentation visit  mkdocs.org .", 
            "title": "Welcome to Container-Native Storage Hands-on Lab"
        }, 
        {
            "location": "/#commands", 
            "text": "mkdocs new [dir-name]  - Create a new project.  mkdocs serve  - Start the live-reloading docs server.  mkdocs build  - Build the documentation site.  mkdocs help  - Print this help message.", 
            "title": "Commands"
        }, 
        {
            "location": "/#code", 
            "text": "# This line is emphasized  # This line isn t  # This line is emphasized    Bubble sort   def   bubble_sort ( items ): \n     for   i   in   range ( len ( items )): \n         for   j   in   range ( len ( items )   -   1   -   i ): \n             if   items [ j ]     items [ j   +   1 ]: \n                 items [ j ],   items [ j   +   1 ]   =   items [ j   +   1 ],   items [ j ]", 
            "title": "Code"
        }, 
        {
            "location": "/#project-layout", 
            "text": "mkdocs.yml    # The configuration file.\ndocs/\n    index.md  # The documentation homepage.\n    ...       # Other markdown pages, images and other files.", 
            "title": "Project layout"
        }, 
        {
            "location": "/module-1/", 
            "text": "", 
            "title": "Module 1 - OpenShift in 5 Minutes"
        }, 
        {
            "location": "/module-2-deploy-cns/", 
            "text": "Overview\n\n\nIn this module you will set up container-native storage (CNS) in your OpenShift environment. You will use this to dynamically provision storage to be available to workloads in OpenShift. It is provided by GlusterFS running in containers. GlusterFS in turn is backed by local storage available to the OpenShift nodes.\n\n\nAt the end of this module you will have 3 GlusterFS pods running together with the heketi API frontend properly integrated into OpenShift.\n\n\n\n\nMake sure you are logged on as the \nec2-user\n to the master node:\n\n\n[ec2-user@master ~]$ hostname -f\nmaster.lab\n\n\n\n\n\n\n\nCaution\n\n\nAll of the following tasks are carried out as the ec2-user from the master node. For Copy \n Paste convenience we will omit the shell prompt unless necessary.\n\n\nAll files created can be stored in root\u2019s home directory unless a particular path is specified.\n\n\n\n\nFirst install the CNS deployment tool:\n\n\nsudo yum -y install cns-deploy\n\n\n\n\n\nConfigure OpenShift Node firewall with Ansible\n#\n\n\n\n\nHint\n\n\nIn the following we will use Ansible\ns configuration management capabilities in order to make sure all the OpenShift nodes have the right firewall settings. This is for your convenience.\n\nAnsible is already present because the OpenShift installer makes heavy use of it.\n\n\nAlternatively you can manually configure each nodes iptables configuration to open ports 2222/tcp, 24007-24008/tcp and 49152-49664/tcp.\n\n\n\n\n\n\nYou should be able to ping all hosts using Ansible:\n\n\nansible nodes -m ping\n\n\n\n\n\nAll 6 OpenShift application nodes should respond\n\n\nnode3.example.com | SUCCESS =\n {\n    \nchanged\n: false,\n    \nping\n: \npong\n\n}\nnode2.example.com | SUCCESS =\n {\n    \nchanged\n: false,\n    \nping\n: \npong\n\n}\nnode1.example.com | SUCCESS =\n {\n    \nchanged\n: false,\n    \nping\n: \npong\n\n}\n\n\n\n\n\nNext, create a file called \nconfigure-firewall.yml\n and copy\npaste the following contents:\n\n\nconfigure-firewall.yml:\n\n\n---\n\n\n\n-\n \nhosts\n:\n \nnodes\n\n\n  \ntasks\n:\n\n\n    \n-\n \nname\n:\n \ninsert iptables rules required for GlusterFS\n\n      \nblockinfile\n:\n\n        \ndest\n:\n \n/etc/sysconfig/iptables\n\n        \nblock\n:\n \n|\n\n          \n-A OS_FIREWALL_ALLOW -p tcp -m state --state NEW -m tcp --dport 24007 -j ACCEPT\n\n          \n-A OS_FIREWALL_ALLOW -p tcp -m state --state NEW -m tcp --dport 24008 -j ACCEPT\n\n          \n-A OS_FIREWALL_ALLOW -p tcp -m state --state NEW -m tcp --dport 2222 -j ACCEPT\n\n          \n-A OS_FIREWALL_ALLOW -p tcp -m state --state NEW -m multiport --dports 49152:49664 -j ACCEPT\n\n        \ninsertbefore\n:\n \n^COMMIT\n\n\n    \n-\n \nname\n:\n \nreload iptables\n\n      \nsystemd\n:\n\n        \nname\n:\n \niptables\n\n        \nstate\n:\n \nreloaded\n\n\n\n...\n\n\n\n\n\n\nDone. This small playbook will save us some work in configuring the firewall top open required ports for CNS on each individual node.\n\n\nRun it with the following command:\n\n\nansible-playbook configure-firewall.yml\n\n\n\n\n\nYour output should look like this.\n\n\nPLAY [nodes] *******************************************************************\n\nTASK [setup] *******************************************************************\nok: [node2.example.com]\nok: [node1.example.com]\nok: [node3.example.com]\n\nTASK [insert iptables rules required for GlusterFS] ****************************\nchanged: [node3.example.com]\nchanged: [node2.example.com]\nchanged: [node1.example.com]\n\nTASK [reload iptables] *********************************************************\nchanged: [node2.example.com]\nchanged: [node1.example.com]\nchanged: [node3.example.com]\n\nPLAY RECAP *********************************************************************\nnode1.example.com          : ok=3    changed=2    unreachable=0    failed=0\nnode2.example.com          : ok=3    changed=2    unreachable=0    failed=0\nnode3.example.com          : ok=3    changed=2    unreachable=0    failed=0\n\n\n\n\n\nWith this we checked the requirement for additional firewall ports to be opened on the OpenShift app nodes.\n\n\n\n\nPrepare OpenShift for CNS\n#\n\n\nNext we will create a namespace (also referred to as a \nProject\n) in OpenShift. It will be used to group the GlusterFS pods.\n\n\nFor this you need to be logged as an admin user in OpenShift.\n\n\n[ec2-user@master ~]# oc whoami\nsystem:admin\n\n\n\n\n\nIf you are for some reason not an admin, login as system admin like this:\n\n\noc login -u system:admin -n default\n\n\n\n\n\nCreate a namespace with a designation of your choice. In this example we will use \ncontainer-native-storage\n.\n\n\noc new-project container-native-storage\n\n\n\n\n\nGlusterFS pods need access to the physical block devices on the host. Hence they need elevated permissions.\n\n\nEnable containers to run in privileged mode:\n\n\noadm policy add-scc-to-user privileged -z default\n\n\n\n\n\nBuild Container-native Storage Topology\n#\n\n\nCNS will virtualize locally attached block storage on the OpenShift App nodes. In order to deploy you will need to supply the installer with information about where to find these nodes and what network and which block devices to use.\n\nThis is done using JSON file describing the topology of your OpenShift deployment.\n\n\nFor this purpose, create the file topology.json like the example below. The hostnames, device paths and zone IDs are already correct. Replace the IP addresses with the ones from your environment.\n\n\n\n\nWarning\n\n\nThe IP addresses will be different in your environment as they are dynamically allocated. Please refer to \n/etc/hosts\n for the correct mapping.\n\n\n\n\ntopology.json:\n\n\n{\n\n    \nclusters\n:\n \n[\n\n        \n{\n\n            \nnodes\n:\n \n[\n\n                \n{\n\n                    \nnode\n:\n \n{\n\n                        \nhostnames\n:\n \n{\n\n                            \nmanage\n:\n \n[\n\n                                \nnode-1.lab\n\n                            \n],\n\n                            \nstorage\n:\n \n[\n\n                                \n192.168.0.102\n\n                            \n]\n\n                        \n},\n\n                        \nzone\n:\n \n1\n\n                    \n},\n\n                    \ndevices\n:\n \n[\n\n                        \n/dev/vdc\n\n                    \n]\n\n                \n},\n\n                \n{\n\n                    \nnode\n:\n \n{\n\n                        \nhostnames\n:\n \n{\n\n                            \nmanage\n:\n \n[\n\n                                \nnode-2.lab\n\n                            \n],\n\n                            \nstorage\n:\n \n[\n\n                                \n192.168.0.103\n\n                            \n]\n\n                        \n},\n\n                        \nzone\n:\n \n2\n\n                    \n},\n\n                    \ndevices\n:\n \n[\n\n                        \n/dev/vdc\n\n                    \n]\n\n                \n},\n\n                \n{\n\n                    \nnode\n:\n \n{\n\n                        \nhostnames\n:\n \n{\n\n                            \nmanage\n:\n \n[\n\n                                \nnode-3.lab\n\n                            \n],\n\n                            \nstorage\n:\n \n[\n\n                                \n192.168.0.104\n\n                            \n]\n\n                        \n},\n\n                        \nzone\n:\n \n3\n\n                    \n},\n\n                    \ndevices\n:\n \n[\n\n                        \n/dev/vdc\n\n                    \n]\n\n                \n}\n\n            \n]\n\n        \n}\n\n    \n]\n\n\n}\n\n\n\n\n\n\n\n\nWhat is the zone ID for?\n\n\nNext to the obvious information like fully-qualified hostnames, IP address and device names required for Gluster the topology contains an additional property called \nzone\n per node.\n\n\nA zone identifies a failure domain. In CNS data is always replicated 3 times. Reflecting these by zone IDs as arbitrary but distinct numerical values allows CNS to ensure that two copies are never stored on nodes in the same failure domain.\n\n\nThis information is considered when building new volumes, expanding existing volumes or replacing bricks in degraded volumes.\n\nAn example for failure domains are AWS Availability Zones or physical servers sharing the same PDU.\n\n\n\n\n\n\nDeploy Container-native Storage\n#\n\n\nYou are now ready to deploy CNS. Alongside GlusterFS pods the API front-end known as \nheketi\n is deployed. By default it runs without any authentication layer.\n\nTo protect the API from unauthorized access we will define passwords for the \nadmin\n and \nuser\n role in heketi like below. We will refer to these later again.\n\n\n\n\n\n\n\n\nHeketi Role\n\n\nPassword\n\n\n\n\n\n\n\n\n\n\nadmin\n\n\nmyS3cr3tpassw0rd\n\n\n\n\n\n\nuser\n\n\nmys3rs3cr3tpassw0rd\n\n\n\n\n\n\n\n\nNext start the deployment routine with the following command:\n\n\ncns-deploy -n container-native-storage -g topology.json --admin-key \nmyS3cr3tpassw0rd\n --user-key \nmys3rs3cr3tpassw0rd\n\n\n\n\n\n\nAnswer the interactive prompt with \nY\n.\n\n\n\n\nNote\n\n\nThe deployment will take several minutes to complete. You may want to monitor the progress in parallel also in the OpenShift UI in the \ncontainer-native-storage\n project.\n\n\n\n\nOn the command line the output should look like this:\n\n\nWelcome to the deployment tool for GlusterFS on Kubernetes and OpenShift.\n\nBefore getting started, this script has some requirements of the execution\nenvironment and of the container platform that you should verify.\n\nThe client machine that will run this script must have:\n * Administrative access to an existing Kubernetes or OpenShift cluster\n * Access to a python interpreter \npython\n\n * Access to the heketi client \nheketi-cli\n\n\nEach of the nodes that will host GlusterFS must also have appropriate firewall\nrules for the required GlusterFS ports:\n * 2222  - sshd (if running GlusterFS in a pod)\n * 24007 - GlusterFS Daemon\n * 24008 - GlusterFS Management\n * 49152 to 49251 - Each brick for every volume on the host requires its own\n   port. For every new brick, one new port will be used starting at 49152. We\n   recommend a default range of 49152-49251 on each host, though you can adjust\n   this to fit your needs.\n\nIn addition, for an OpenShift deployment you must:\n * Have \ncluster_admin\n role on the administrative account doing the deployment\n * Add the \ndefault\n and \nrouter\n Service Accounts to the \nprivileged\n SCC\n * Have a router deployed that is configured to allow apps to access services\n   running in the cluster\n\n\nDo you wish to proceed with deployment? Y\n\n[Y]es, [N]o? [Default: Y]:\n\nUsing OpenShift CLI.\nNAME                       STATUS    AGE\ncontainer-native-storage   Active    28m\nUsing namespace \ncontainer-native-storage\n.\nChecking that heketi pod is not running ... OK\ntemplate \ndeploy-heketi\n created\nserviceaccount \nheketi-service-account\n created\ntemplate \nheketi\n created\ntemplate \nglusterfs\n created\nrole \nedit\n added: \nsystem:serviceaccount:container-native-storage:heketi-service-account\n\n\nnode \nnode1.example.com\n labeled\n\nnode \nnode2.example.com\n labeled\n\nnode \nnode3.example.com\n labeled\n\ndaemonset \nglusterfs\n created\n\nWaiting for GlusterFS pods to start ... OK\n\nservice \ndeploy-heketi\n created\nroute \ndeploy-heketi\n created\ndeploymentconfig \ndeploy-heketi\n created\nWaiting for deploy-heketi pod to start ... OK\nCreating cluster ... ID: 307f708621f4e0c9eda962b713272e81\n\nCreating node node1.example.com ... ID: f60a225a16e8678d5ef69afb4815e417\n\nAdding device /dev/vdc ... OK\n\nCreating node node2.example.com ... ID: 13b7c17c541069862d7e66d142ab789e\n\nAdding device /dev/vdc ... OK\n\nCreating node node3.example.com ... ID: 5a6fbe5eb1864e711f8bd9b0cb5946ea\n\nAdding device /dev/vdc ... OK\nheketi topology loaded.\nSaving heketi-storage.json\nsecret \nheketi-storage-secret\n created\nendpoints \nheketi-storage-endpoints\n created\nservice \nheketi-storage-endpoints\n created\njob \nheketi-storage-copy-job\n created\ndeploymentconfig \ndeploy-heketi\n deleted\nroute \ndeploy-heketi\n deleted\nservice \ndeploy-heketi\n deleted\njob \nheketi-storage-copy-job\n deleted\npod \ndeploy-heketi-1-599rc\n deleted\nsecret \nheketi-storage-secret\n deleted\nservice \nheketi\n created\n\nroute \nheketi\n created\n\ndeploymentconfig \nheketi\n created\n\nWaiting for heketi pod to start ... OK\n\nheketi is now running.\nReady to create and provide GlusterFS volumes.\n\n\n\n\n\nIn order of the appearance of the highlighted lines above in a nutshell what happens here is the following:\n\n\n\n\n\n\nEnter \nY\n and press Enter.\n\n\n\n\n\n\nOpenShift nodes are labeled. Label is referred to in a DaemonSet.\n\n\n\n\n\n\nGlusterFS daemonset is started. DaemonSet means: start exactly \none\n pod per node.\n\n\n\n\n\n\nAll nodes will be referenced in heketi\u2019s database by a UUID. Node block devices are formatted for mounting by GlusterFS.\n\n\n\n\n\n\nA public route is created for the heketi pod to expose it\ns API.\n\n\n\n\n\n\nheketi is deployed in a pod as well.\n\n\n\n\n\n\n\n\nVerifying the deployment\n#\n\n\nYou now have deployed CNS. Let\u2019s verify all components are in place.\n\n\nIf not already there on the CLI change back to the \ncontainer-native-storage\n namespace:\n\n\noc project container-native-storage\n\n\n\n\n\nList all running pods:\n\n\noc get pods -o wide\n\n\n\n\n\nYou should see all pods up and running. Highlighted containerized gluster daemons on each pods carry the IP of the OpenShift node they are running on.\n\n\nNAME              READY     STATUS    RESTARTS   AGE       IP              NODE\n\nglusterfs-37vn8   1/1       Running   0          3m       192.168.0.102   node1.example.com\n\nglusterfs-cq68l   1/1       Running   0          3m       192.168.0.103   node2.example.com\n\nglusterfs-m9fvl   1/1       Running   0          3m       192.168.0.104   node3.example.com\n\nheketi-1-cd032    1/1       Running   0          1m       10.130.0.4      node3.example.com\n\n\n\n\n\n\n\nNote\n\n\nThe exact pod names will be different in your environment, since they are auto-generated.\n\n\n\n\nThe GlusterFS pods use the hosts network and disk devices to run the software-defined storage system. Hence they attached to the host\u2019s network. See schematic below for a visualization.\n\n\n\n\nheketi\n is a component that will expose an API for GlusterFS to OpenShift. This allows OpenShift to dynamically allocate storage from CNS in a programmatic fashion. See below for a visualization. Note that for simplicity, in our example heketi runs on the OpenShift App nodes, not on the Infra node.\n\n\n\n\nTo expose heketi\u2019s API a \nservice\n named \nheketi\n has been generated in OpenShift.\n\n\nCheck the service with:\n\n\noc get service/heketi\n\n\n\n\n\nThe output should look similar to the below:\n\n\nNAME      CLUSTER-IP     EXTERNAL-IP   PORT(S)    AGE\nheketi    172.30.5.231   \nnone\n        8080/TCP   31m\n\n\n\n\n\nTo also use heketi outside of OpenShift in addition to the service a route has been deployed.\n\n\nDisplay the route with:\n\n\noc get route/heketi\n\n\n\n\n\nThe output should look similar to the below:\n\n\n[ec2-user@master ~]# oc get route/heketi\nNAME      HOST/PORT                                               PATH      SERVICES   PORT      TERMINATION   WILDCARD\nheketi    heketi-container-native-storage.cloudapps.example.com             heketi     \nall\n                   None\n\n\n\n\n\nBased on this \nheketi\n will be available on this URL:\n\n\nhttp://heketi-container-native-storage.cloudapps.example.com\n\n\nYou may verify this with a trivial health check:\n\n\ncurl http://heketi-container-native-storage.cloudapps.example.com/hello\n\n\n\n\n\nThis should say:\n\n\nHello from Heketi\n\n\n\n\n\nWith this you have successfully verified the deployed components.", 
            "title": "Module 2 - Deploying Container-Native Storage"
        }, 
        {
            "location": "/module-2-deploy-cns/#configure-openshift-node-firewall-with-ansible", 
            "text": "Hint  In the following we will use Ansible s configuration management capabilities in order to make sure all the OpenShift nodes have the right firewall settings. This is for your convenience. \nAnsible is already present because the OpenShift installer makes heavy use of it.  Alternatively you can manually configure each nodes iptables configuration to open ports 2222/tcp, 24007-24008/tcp and 49152-49664/tcp.    You should be able to ping all hosts using Ansible:  ansible nodes -m ping  All 6 OpenShift application nodes should respond  node3.example.com | SUCCESS =  {\n     changed : false,\n     ping :  pong \n}\nnode2.example.com | SUCCESS =  {\n     changed : false,\n     ping :  pong \n}\nnode1.example.com | SUCCESS =  {\n     changed : false,\n     ping :  pong \n}  Next, create a file called  configure-firewall.yml  and copy paste the following contents:  configure-firewall.yml:  ---  -   hosts :   nodes \n\n   tasks : \n\n     -   name :   insert iptables rules required for GlusterFS \n       blockinfile : \n         dest :   /etc/sysconfig/iptables \n         block :   | \n           -A OS_FIREWALL_ALLOW -p tcp -m state --state NEW -m tcp --dport 24007 -j ACCEPT \n           -A OS_FIREWALL_ALLOW -p tcp -m state --state NEW -m tcp --dport 24008 -j ACCEPT \n           -A OS_FIREWALL_ALLOW -p tcp -m state --state NEW -m tcp --dport 2222 -j ACCEPT \n           -A OS_FIREWALL_ALLOW -p tcp -m state --state NEW -m multiport --dports 49152:49664 -j ACCEPT \n         insertbefore :   ^COMMIT \n\n     -   name :   reload iptables \n       systemd : \n         name :   iptables \n         state :   reloaded  ...   Done. This small playbook will save us some work in configuring the firewall top open required ports for CNS on each individual node.  Run it with the following command:  ansible-playbook configure-firewall.yml  Your output should look like this.  PLAY [nodes] *******************************************************************\n\nTASK [setup] *******************************************************************\nok: [node2.example.com]\nok: [node1.example.com]\nok: [node3.example.com]\n\nTASK [insert iptables rules required for GlusterFS] ****************************\nchanged: [node3.example.com]\nchanged: [node2.example.com]\nchanged: [node1.example.com]\n\nTASK [reload iptables] *********************************************************\nchanged: [node2.example.com]\nchanged: [node1.example.com]\nchanged: [node3.example.com]\n\nPLAY RECAP *********************************************************************\nnode1.example.com          : ok=3    changed=2    unreachable=0    failed=0\nnode2.example.com          : ok=3    changed=2    unreachable=0    failed=0\nnode3.example.com          : ok=3    changed=2    unreachable=0    failed=0  With this we checked the requirement for additional firewall ports to be opened on the OpenShift app nodes.", 
            "title": "Configure OpenShift Node firewall with Ansible"
        }, 
        {
            "location": "/module-2-deploy-cns/#prepare-openshift-for-cns", 
            "text": "Next we will create a namespace (also referred to as a  Project ) in OpenShift. It will be used to group the GlusterFS pods.  For this you need to be logged as an admin user in OpenShift.  [ec2-user@master ~]# oc whoami\nsystem:admin  If you are for some reason not an admin, login as system admin like this:  oc login -u system:admin -n default  Create a namespace with a designation of your choice. In this example we will use  container-native-storage .  oc new-project container-native-storage  GlusterFS pods need access to the physical block devices on the host. Hence they need elevated permissions.  Enable containers to run in privileged mode:  oadm policy add-scc-to-user privileged -z default", 
            "title": "Prepare OpenShift for CNS"
        }, 
        {
            "location": "/module-2-deploy-cns/#build-container-native-storage-topology", 
            "text": "CNS will virtualize locally attached block storage on the OpenShift App nodes. In order to deploy you will need to supply the installer with information about where to find these nodes and what network and which block devices to use. \nThis is done using JSON file describing the topology of your OpenShift deployment.  For this purpose, create the file topology.json like the example below. The hostnames, device paths and zone IDs are already correct. Replace the IP addresses with the ones from your environment.   Warning  The IP addresses will be different in your environment as they are dynamically allocated. Please refer to  /etc/hosts  for the correct mapping.   topology.json:  { \n     clusters :   [ \n         { \n             nodes :   [ \n                 { \n                     node :   { \n                         hostnames :   { \n                             manage :   [ \n                                 node-1.lab \n                             ], \n                             storage :   [ \n                                 192.168.0.102 \n                             ] \n                         }, \n                         zone :   1 \n                     }, \n                     devices :   [ \n                         /dev/vdc \n                     ] \n                 }, \n                 { \n                     node :   { \n                         hostnames :   { \n                             manage :   [ \n                                 node-2.lab \n                             ], \n                             storage :   [ \n                                 192.168.0.103 \n                             ] \n                         }, \n                         zone :   2 \n                     }, \n                     devices :   [ \n                         /dev/vdc \n                     ] \n                 }, \n                 { \n                     node :   { \n                         hostnames :   { \n                             manage :   [ \n                                 node-3.lab \n                             ], \n                             storage :   [ \n                                 192.168.0.104 \n                             ] \n                         }, \n                         zone :   3 \n                     }, \n                     devices :   [ \n                         /dev/vdc \n                     ] \n                 } \n             ] \n         } \n     ]  }    What is the zone ID for?  Next to the obvious information like fully-qualified hostnames, IP address and device names required for Gluster the topology contains an additional property called  zone  per node.  A zone identifies a failure domain. In CNS data is always replicated 3 times. Reflecting these by zone IDs as arbitrary but distinct numerical values allows CNS to ensure that two copies are never stored on nodes in the same failure domain.  This information is considered when building new volumes, expanding existing volumes or replacing bricks in degraded volumes. \nAn example for failure domains are AWS Availability Zones or physical servers sharing the same PDU.", 
            "title": "Build Container-native Storage Topology"
        }, 
        {
            "location": "/module-2-deploy-cns/#deploy-container-native-storage", 
            "text": "You are now ready to deploy CNS. Alongside GlusterFS pods the API front-end known as  heketi  is deployed. By default it runs without any authentication layer. \nTo protect the API from unauthorized access we will define passwords for the  admin  and  user  role in heketi like below. We will refer to these later again.     Heketi Role  Password      admin  myS3cr3tpassw0rd    user  mys3rs3cr3tpassw0rd     Next start the deployment routine with the following command:  cns-deploy -n container-native-storage -g topology.json --admin-key  myS3cr3tpassw0rd  --user-key  mys3rs3cr3tpassw0rd   Answer the interactive prompt with  Y .   Note  The deployment will take several minutes to complete. You may want to monitor the progress in parallel also in the OpenShift UI in the  container-native-storage  project.   On the command line the output should look like this:  Welcome to the deployment tool for GlusterFS on Kubernetes and OpenShift.\n\nBefore getting started, this script has some requirements of the execution\nenvironment and of the container platform that you should verify.\n\nThe client machine that will run this script must have:\n * Administrative access to an existing Kubernetes or OpenShift cluster\n * Access to a python interpreter  python \n * Access to the heketi client  heketi-cli \n\nEach of the nodes that will host GlusterFS must also have appropriate firewall\nrules for the required GlusterFS ports:\n * 2222  - sshd (if running GlusterFS in a pod)\n * 24007 - GlusterFS Daemon\n * 24008 - GlusterFS Management\n * 49152 to 49251 - Each brick for every volume on the host requires its own\n   port. For every new brick, one new port will be used starting at 49152. We\n   recommend a default range of 49152-49251 on each host, though you can adjust\n   this to fit your needs.\n\nIn addition, for an OpenShift deployment you must:\n * Have  cluster_admin  role on the administrative account doing the deployment\n * Add the  default  and  router  Service Accounts to the  privileged  SCC\n * Have a router deployed that is configured to allow apps to access services\n   running in the cluster Do you wish to proceed with deployment? Y [Y]es, [N]o? [Default: Y]:\n\nUsing OpenShift CLI.\nNAME                       STATUS    AGE\ncontainer-native-storage   Active    28m\nUsing namespace  container-native-storage .\nChecking that heketi pod is not running ... OK\ntemplate  deploy-heketi  created\nserviceaccount  heketi-service-account  created\ntemplate  heketi  created\ntemplate  glusterfs  created\nrole  edit  added:  system:serviceaccount:container-native-storage:heketi-service-account  node  node1.example.com  labeled node  node2.example.com  labeled node  node3.example.com  labeled daemonset  glusterfs  created Waiting for GlusterFS pods to start ... OK service  deploy-heketi  created\nroute  deploy-heketi  created\ndeploymentconfig  deploy-heketi  created\nWaiting for deploy-heketi pod to start ... OK\nCreating cluster ... ID: 307f708621f4e0c9eda962b713272e81 Creating node node1.example.com ... ID: f60a225a16e8678d5ef69afb4815e417 Adding device /dev/vdc ... OK Creating node node2.example.com ... ID: 13b7c17c541069862d7e66d142ab789e Adding device /dev/vdc ... OK Creating node node3.example.com ... ID: 5a6fbe5eb1864e711f8bd9b0cb5946ea Adding device /dev/vdc ... OK\nheketi topology loaded.\nSaving heketi-storage.json\nsecret  heketi-storage-secret  created\nendpoints  heketi-storage-endpoints  created\nservice  heketi-storage-endpoints  created\njob  heketi-storage-copy-job  created\ndeploymentconfig  deploy-heketi  deleted\nroute  deploy-heketi  deleted\nservice  deploy-heketi  deleted\njob  heketi-storage-copy-job  deleted\npod  deploy-heketi-1-599rc  deleted\nsecret  heketi-storage-secret  deleted\nservice  heketi  created route  heketi  created deploymentconfig  heketi  created Waiting for heketi pod to start ... OK heketi is now running.\nReady to create and provide GlusterFS volumes.  In order of the appearance of the highlighted lines above in a nutshell what happens here is the following:    Enter  Y  and press Enter.    OpenShift nodes are labeled. Label is referred to in a DaemonSet.    GlusterFS daemonset is started. DaemonSet means: start exactly  one  pod per node.    All nodes will be referenced in heketi\u2019s database by a UUID. Node block devices are formatted for mounting by GlusterFS.    A public route is created for the heketi pod to expose it s API.    heketi is deployed in a pod as well.", 
            "title": "Deploy Container-native Storage"
        }, 
        {
            "location": "/module-2-deploy-cns/#verifying-the-deployment", 
            "text": "You now have deployed CNS. Let\u2019s verify all components are in place.  If not already there on the CLI change back to the  container-native-storage  namespace:  oc project container-native-storage  List all running pods:  oc get pods -o wide  You should see all pods up and running. Highlighted containerized gluster daemons on each pods carry the IP of the OpenShift node they are running on.  NAME              READY     STATUS    RESTARTS   AGE       IP              NODE glusterfs-37vn8   1/1       Running   0          3m       192.168.0.102   node1.example.com glusterfs-cq68l   1/1       Running   0          3m       192.168.0.103   node2.example.com glusterfs-m9fvl   1/1       Running   0          3m       192.168.0.104   node3.example.com heketi-1-cd032    1/1       Running   0          1m       10.130.0.4      node3.example.com   Note  The exact pod names will be different in your environment, since they are auto-generated.   The GlusterFS pods use the hosts network and disk devices to run the software-defined storage system. Hence they attached to the host\u2019s network. See schematic below for a visualization.   heketi  is a component that will expose an API for GlusterFS to OpenShift. This allows OpenShift to dynamically allocate storage from CNS in a programmatic fashion. See below for a visualization. Note that for simplicity, in our example heketi runs on the OpenShift App nodes, not on the Infra node.   To expose heketi\u2019s API a  service  named  heketi  has been generated in OpenShift.  Check the service with:  oc get service/heketi  The output should look similar to the below:  NAME      CLUSTER-IP     EXTERNAL-IP   PORT(S)    AGE\nheketi    172.30.5.231    none         8080/TCP   31m  To also use heketi outside of OpenShift in addition to the service a route has been deployed.  Display the route with:  oc get route/heketi  The output should look similar to the below:  [ec2-user@master ~]# oc get route/heketi\nNAME      HOST/PORT                                               PATH      SERVICES   PORT      TERMINATION   WILDCARD\nheketi    heketi-container-native-storage.cloudapps.example.com             heketi      all                    None  Based on this  heketi  will be available on this URL:  http://heketi-container-native-storage.cloudapps.example.com  You may verify this with a trivial health check:  curl http://heketi-container-native-storage.cloudapps.example.com/hello  This should say:  Hello from Heketi  With this you have successfully verified the deployed components.", 
            "title": "Verifying the deployment"
        }, 
        {
            "location": "/module-3-using-cns/", 
            "text": "Overview\n\n\nIn this module you will use CNS as a developer would do in OpenShift. For that purpose you will dynamically provision storage both in standalone fashion and in context of an application deployment.\n\n\nAt the end of this module you will have two application stacks running that consume persistent storage provided by CNS.\n\n\n\n\nCreating a StorageClass\n#\n\n\nOpenShift uses Kubernetes\n PersistentStorage facility to dynamically allocate storage of any kind for applications. This is a fairly simple framework in which only 3 components exists: the storage provider, the storage volume and the request for a storage volume.\n\n\n\n\nOpenShift knows non-ephemeral storage as \npersistent\n volumes. This is storage that is decoupled from pod lifecycles. Users can request such storage by submitting a \nPersistentVolumeClaim\n to the system, which carries aspects like desired capacity or access mode (shared, single, read-only).\n\n\nA storage provider in the system is represented by a \nStorageClass\n and is referenced in the claim. Upon receiving the claim it talks to the API of the actual storage system to provision the storage.  \n\n\nThe storage is represented in OpenShift as a \nPersistentVolume\n which can directly be used by pods to mount it.\n\n\nWith these basics defined we can configure our system for CNS. First we will set up the credentials for CNS in OpenShift.\n\n\nCreate an encoded value for the CNS admin user like below:\n\n\necho -n \nmyS3cr3tpassw0rd\n | base64\n\n\n\n\n\nThe encoded string looks like this:\n\n\nbXlTM2NyM3RwYXNzdzByZA==\n\n\n\n\n\nWe will store this encoded value in an OpenShift secret.\n\n\nCreate a file called \ncns-secret.yml\n with the as per below (highlight shows where to put encoded password):\n\n\ncns-secret.yml:\n\n\napiVersion\n:\n \nv1\n\n\nkind\n:\n \nSecret\n\n\nmetadata\n:\n\n  \nname\n:\n \ncns-secret\n\n  \nnamespace\n:\n \ndefault\n\n\ndata\n:\n\n\n  \nkey\n:\n \nbXlTM2NyM3RwYXNzdzByZA==\n\n\ntype\n:\n \nkubernetes.io/glusterfs\n\n\n\n\n\n\nCreate the secret in OpenShift with the following command:\n\n\noc create -f cns-secret.yml\n\n\nTo represent CNS as a storage provider in the system you first have to create a StorageClass. Define by creating a file called \ncns-storageclass.yml\n which references the secret and the heketi URL shown earlier with the contents as below:\n\n\ncns-storageclass.yml:\n\n\napiVersion\n:\n \nstorage.k8s.io/v1beta1\n\n\nkind\n:\n \nStorageClass\n\n\nmetadata\n:\n\n  \nname\n:\n \ncontainer-native-storage\n\n  \nannotations\n:\n\n    \nstorageclass.beta.kubernetes.io/is-default-class\n:\n \ntrue\n\n\nprovisioner\n:\n \nkubernetes.io/glusterfs\n\n\nparameters\n:\n\n  \nresturl\n:\n \nhttp://heketi-container-native-storage.cloudapps.example.com\n\n  \nrestauthenabled\n:\n \ntrue\n\n  \nrestuser\n:\n \nadmin\n\n  \nvolumetype\n:\n \nreplicate:3\n\n  \nsecretNamespace\n:\n \ndefault\n\n  \nsecretName\n:\n \ncns-secret\n\n\n\n\n\n\nCreate the StorageClass in OpenShift with the following command:\n\n\noc create -f cns-storageclass.yml\n\n\n\n\n\nWith these components in place the system is ready to dynamically provision storage capacity from Container-native Storage.\n\n\n\n\nRequesting Storage\n#\n\n\nTo get storage provisioned as a user you have to \nclaim\n storage. The \nPersistentVolumeClaim\n (PVC) basically acts a request to the system to provision storage with certain properties, like a specific capacity.\n\nAlso the access mode is set here, where \nReadWriteOnce\n allows one container at a time to mount this storage.\n\n\nCreate a claim by specifying a file called \ncns-pvc.yml\n with the following contents:\n\n\ncns-pvc.yml:\n\n\nkind\n:\n \nPersistentVolumeClaim\n\n\napiVersion\n:\n \nv1\n\n\nmetadata\n:\n\n  \nname\n:\n \nmy-container-storage\n\n  \nannotations\n:\n\n    \nvolume.beta.kubernetes.io/storage-class\n:\n \ncontainer-native-storage\n\n\nspec\n:\n\n  \naccessModes\n:\n\n  \n-\n \nReadWriteOnce\n\n  \nresources\n:\n\n    \nrequests\n:\n\n      \nstorage\n:\n \n10Gi\n\n\n\n\n\n\nWith above PVC we are requesting 10 GiB of non-shared storage. Instead of \nReadWriteOnce\n you could also have specified \nReadWriteOnly\n (for read-only) and \nReadWriteMany\n (for shared storage).\n\n\nSubmit the PVC to the system like so:\n\n\noc create -f cns-pvc.yml\n\n\n\n\n\nLook at the requests state with the following command:\n\n\noc get pvc\n\n\n\n\n\nYou should see the PVC listed and in \nBound\n state.\n\n\nNAME                   STATUS    VOLUME                                     CAPACITY   ACCESSMODES   AGE\nmy-container-storage   Bound     pvc-382ac13d-4a9f-11e7-b56f-2cc2602a6dc8   10Gi       RWO           16s\n\n\n\n\n\n\n\nNote\n\n\nIt may take a couple seconds for the claim to be in \nbound\n.\n\n\n\n\n\n\nCaution\n\n\nIf the PVC is stuck in \nPENDING\n state you will need to investigate. Run \noc describe pvc/my-container-storage\n to see a more detailed explanation. Typically there are two root causes - the StorageClass is not properly setup (wrong name, wrong credentials, incorrect secret name, wrong heketi URL, heketi service not up, heketi pod not up\u2026) or the PVC is malformed (wrong StorageClass, name already taken \u2026)\n\n\n\n\n\n\nTip\n\n\nYou can also do this step with the UI. If you like you can switch to an arbitrary project you have access to and go to the \nStorage\n tab. Select \nCreate\n storage and make selections accordingly to the PVC described before.\n\n\n\n\nWhen the claim was fulfilled successfully it is in the \nBound\n state. That means the system has successfully (via the StorageClass) reached out to the storage backend (in our case GlusterFS). The backend in turn provisioned the storage and provided a handle back OpenShift. In OpenShift the provisioned storage is then represented by a \nPersistentVolume\n (PV) which is \nbound\n to the PVC.\n\n\nLook at the PVC for these details:\n\n\noc describe pvc/my-container-storage\n\n\n\n\n\nThe details of the PVC show against which \nStorageClass\n it has been submitted and the name of the \nPersistentVolume\n which was generated to fulfill the claim.\n\n\nName:           my-container-storage\n\nNamespace:      container-native-storage\nStorageClass:   container-native-storage\nStatus:         Bound\n\nVolume:         pvc-382ac13d-4a9f-11e7-b56f-2cc2602a6dc8\n\nLabels:         \nnone\n\nCapacity:       10Gi\nAccess Modes:   RWO\nNo events.\n\n\n\n\n\n\n\nNote\n\n\nThe PV name will be different in your environment since it\u2019s automatically generated.\n\n\n\n\nLook at the corresponding PV by it\u2019s name:\n\n\noc describe pv/pvc-382ac13d-4a9f-11e7-b56f-2cc2602a6dc8\n\n\n\n\n\nThe output shows several interesting things, like the access mode (RWO = ReadWriteOnce), the reclaim policy (what happens when the PV object gets deleted), the capacity and the type of storage backing this PV (in our case GlusterFS as part of CNS):\n\n\nName:           pvc-382ac13d-4a9f-11e7-b56f-2cc2602a6dc8\nLabels:         \nnone\n\nStorageClass:   container-native-storage\n\nStatus:         Bound\n\nClaim:          container-native-storage/my-container-storage\n\nReclaim Policy: Delete\n\nAccess Modes:   RWO\n\nCapacity:       10Gi\n\nMessage:\nSource:\n\n    Type:               Glusterfs (a Glusterfs mount on the host that shares a pod\ns lifetime)\n\n    EndpointsName:      glusterfs-dynamic-my-container-storage\n    Path:               vol_304670f0d50bf5aa4717a69652bd48ff\n    ReadOnly:           false\nNo events.\n\n\n\n\n\n\n\nWhy is it called \nBound\n?\n\n\nOriginally PVs weren\nt automatically created. Hence in earlier documentation you may also find references about administrators actually \npre-provisioning\n PVs. Later PVCs would \npick up\n a suitable PV by looking at it\u2019s capacity. When successful they are \nbound\n to this PV.\n\nThis was needed for storage like NFS that does not have an API and therefore does not support \ndynamic provisioning\n. Hence it\ns called \nstatic provisioning\n.\n\nThis kind of storage should not be used anymore as it requires manual intervention, risky capacity planning and incurs inefficient storage utilization.\n\n\n\n\nLet\u2019s release this storage capacity again, since it\u2019s in the wrong namespace anyway.\n\nStorage is freed up by deleting the \nPVC\n. The PVC controls the lifecycle of the storage, not the PV.\n\n\n\n\nImportant\n\n\nNever delete PVs that are dynamically provided. They are only handles for pods mounting the storage. Storage lifecycle is entirely controlled via PVCs.\n\n\n\n\nDelete the storage by deleting the PVC like this:\n\n\noc delete pvc/my-container-storage\n\n\n\n\n\n\n\nUsing non-shared storage for databases\n#\n\n\nNormally a user doesn\u2019t request storage with a PVC directly. Rather the PVC is integrated in a larger template that describe the entire application. Such examples ship with OpenShift out of the box.\n\n\n\n\nTip\n\n\nThe steps described in this section can again also be done with the UI. For this purpose follow these steps similar to the one in Module 1:\n\n\n\n\n\n\n\n\n\n\nLog on to the OpenShift UI at the URL provided in \nOverview section\n\n\n\n\n\n\nCreate a project called \nmy-test-project\n, label and description is optional\n\n\n\n\n\n\nIn the Overview, next to the project\u2019s name select \nAdd to project\n\n\n\n\n\n\nIn the \nBrowse Catalog\n view select \nRuby\n from the list of programming languages\n\n\n\n\n\n\nSelect the example app entitled \nRails + PostgreSQL (Persistent)\n\n\n\n\n\n\n(optional) Change the \nVolume Capacity\n parameter to something greater than 1GiB, e.g. 15 GiB\n\n\n\n\n\n\nSelect \nCreate\n to start deploying the app\n\n\n\n\n\n\nSelect \nContinue to Overview\n in the confirmation screen\n\n\n\n\n\n\nWait for the application deployment to finish and continue below at\n\n\n\n\n\n\n\n\n\n\nTo create an application from the OpenShift Example templates on the CLI follow these steps.\n\n\nCreate a new project with a name of your choice:\n\n\noc new-project my-test-project\n\n\n\n\n\nTo use the example applications that ship with OpenShift we can export and modify the template for a sample Ruby on Rails with PostgreSQL application. All these templates ship in pre-defined namespace called \nopenshift\n.\n\n\nExport the template from the \nopenshift\n namespace in YAML format:\n\n\noc export template/rails-pgsql-persistent -n openshift -o yaml \n rails-app-template.yml\n\n\n\n\n\nIn the file \nrails-app-template.yml\n you can now review the template for this entire application stack in all it\u2019s glory.\n\n\n\n\nWhat does the template file contain?\n\n\nIn essence it creates Ruby on Rails instance in a pod which functionality mimics a very basic blogging application. The blog articles are saved in a PostgreSQL database which runs in a separate pod.\n\nThe template describes all OpenShift resources necessary to stand up the rails pod and the postgres pod and make them accessible via services and routes.\n\nIn addition a PVC is issued (line 194) to supply this pod with persistent storage below the mount point \n/var/lib/pgsql/data\n (line 275).\n\n\n\n\nThe template contains a couple of parameters which default values we can override.\n\n\n\n\nTip\n\n\nTo list all available parameters from this template run \noc process -f rails-app-template.yml --parameters\n\nThe \noc process\n command parses the template and replaces any parameters with their default values if not supplied explicitly like in the next step.\n\n\n\n\nThere is a parameter in the template is called \nVOLUME_CAPACITY\n. It is used to customize the capacity in the PVC. We will process the template with the CLI client and override this parameter with a value of \n15Gi\n as follows:\n\n\nRender the template with the custom parameter value as follows:\n\n\noc process -f rails-app-template.yml -o yaml -p VOLUME_CAPACITY=15Gi \n my-rails-app.yml\n\n\n\n\n\nThe result \nmy-rails-app.yml\n file contains all resources including our custom PVC for this application.\n\n\nDeploy these resources like so:\n\n\noc create -f my-rails-app.yml\n\n\n\n\n\nAmong various OpenShift resource also our PVC will be created:\n\n\nsecret \nrails-pgsql-persistent\n created\nservice \nrails-pgsql-persistent\n created\nroute \nrails-pgsql-persistent\n created\nimagestream \nrails-pgsql-persistent\n created\nbuildconfig \nrails-pgsql-persistent\n created\ndeploymentconfig \nrails-pgsql-persistent\n created\n\npersistentvolumeclaim \npostgresql\n created\n\nservice \npostgresql\n created\ndeploymentconfig \npostgresql\n created\n\n\n\n\n\nYou can now use the OpenShift UI (while being logged in the newly created project) to follow the deployment process.\n\n\nAlternatively watch the containers deploy like this:\n\n\noc get pods -w\n\n\n\n\n\nThe complete output should look like this:\n\n\nNAME                             READY     STATUS              RESTARTS   AGE\npostgresql-1-deploy              0/1       ContainerCreating   0          11s\nrails-pgsql-persistent-1-build   0/1       ContainerCreating   0          11s\nNAME                  READY     STATUS    RESTARTS   AGE\npostgresql-1-deploy   1/1       Running   0          14s\npostgresql-1-81gnm   0/1       Pending   0         0s\npostgresql-1-81gnm   0/1       Pending   0         0s\nrails-pgsql-persistent-1-build   1/1       Running   0         19s\npostgresql-1-81gnm   0/1       Pending   0         15s\npostgresql-1-81gnm   0/1       ContainerCreating   0         16s\npostgresql-1-81gnm   0/1       Running   0         47s\npostgresql-1-81gnm   1/1       Running   0         4m\npostgresql-1-deploy   0/1       Completed   0         4m\npostgresql-1-deploy   0/1       Terminating   0         4m\npostgresql-1-deploy   0/1       Terminating   0         4m\nrails-pgsql-persistent-1-deploy   0/1       Pending   0         0s\nrails-pgsql-persistent-1-deploy   0/1       Pending   0         0s\nrails-pgsql-persistent-1-deploy   0/1       ContainerCreating   0         0s\nrails-pgsql-persistent-1-build   0/1       Completed   0         11m\nrails-pgsql-persistent-1-deploy   1/1       Running   0         6s\nrails-pgsql-persistent-1-hook-pre   0/1       Pending   0         0s\nrails-pgsql-persistent-1-hook-pre   0/1       Pending   0         0s\nrails-pgsql-persistent-1-hook-pre   0/1       ContainerCreating   0         0s\nrails-pgsql-persistent-1-hook-pre   1/1       Running   0         6s\nrails-pgsql-persistent-1-hook-pre   0/1       Completed   0         15s\nrails-pgsql-persistent-1-dkj7w   0/1       Pending   0         0s\nrails-pgsql-persistent-1-dkj7w   0/1       Pending   0         0s\nrails-pgsql-persistent-1-dkj7w   0/1       ContainerCreating   0         0s\nrails-pgsql-persistent-1-dkj7w   0/1       Running   0         1m\nrails-pgsql-persistent-1-dkj7w   1/1       Running   0         1m\nrails-pgsql-persistent-1-deploy   0/1       Completed   0         1m\nrails-pgsql-persistent-1-deploy   0/1       Terminating   0         1m\nrails-pgsql-persistent-1-deploy   0/1       Terminating   0         1m\nrails-pgsql-persistent-1-hook-pre   0/1       Terminating   0         1m\nrails-pgsql-persistent-1-hook-pre   0/1       Terminating   0         1m\n\n\n\n\n\nExit out of the watch mode with: \nCtrl\n + \nc\n\n\n\n\nNote\n\n\nIt may take up to 5 minutes for the deployment to complete.\n\n\n\n\nYou should also see a PVC being issued and in the \nBound\n state.\n\n\n[root@master ~]# oc get pvc\nNAME         STATUS    VOLUME                                     CAPACITY   ACCESSMODES   AGE\npostgresql   Bound     pvc-9bb84d88-4ac6-11e7-b56f-2cc2602a6dc8   15Gi       RWO           4m\n\n\n\n\n\n\n\nWhy did this even work?\n\n\nIf you paid close attention you likely noticed that the PVC in the template does not specify a particular \nStorageClass\n. This still yields a PV deployed because our \nStorageClass\n has actually been defined as the system-wide default. PVCs that don\nt specify a \nStorageClass\n will use this.\n\n\n\n\nNow go ahead and try out the application. The overview page in the OpenShift UI will tell you the \nroute\n which has been deployed as well. Use it and append \n/articles\n to the URL to get to the actual app.\n\n\nOtherwise get it on the CLI like this:\n\n\noc get route\n\n\n\n\n\nOutput:\n\n\nNAME                     HOST/PORT                                                      PATH      SERVICES                 PORT      TERMINATION   WILDCARD\nrails-pgsql-persistent   rails-pgsql-persistent-my-test-project.cloudapps.example.com             rails-pgsql-persistent   \nall\n                   None\n\n\n\n\n\nFollowing this output, point your browser to \nhttp://rails-pgsql-persistent-my-test-project.cloudapps.example.com/articles\n.\n\nYou should be able to successfully create articles and comments. The username/password to create articles and comments is by default \nopenshift\n/\nsecret\n.\n\nWhen they are saved they are actually saved in the PostgreSQL database which stores it\u2019s table spaces on a GlusterFS volume provided by CNS.\n\n\nNow let\u2019s take a look at how this was actually achieved.\n\n\nLook at the PVC to determine the PV:\n\n\noc get pvc\n\n\n\n\n\nOutput:\n\n\nNAME         STATUS    VOLUME                                     CAPACITY   ACCESSMODES   AGE\npostgresql   Bound     pvc-9bb84d88-4ac6-11e7-b56f-2cc2602a6dc8   15Gi       RWO           17m\n\n\n\n\n\n\n\nNote\n\n\nYour volume (PV) name will be different as it\u2019s dynamically generated.\n\n\n\n\nLook at the details of this PV:\n\n\noc describe pv/pvc-9bb84d88-4ac6-11e7-b56f-2cc2602a6dc8\n\n\n\n\n\nOutput shows (in highlight) the name of the volume, the backend type (GlusterFS) and the volume name GlusterFS uses internally.\n\n\nName:           pvc-9bb84d88-4ac6-11e7-b56f-2cc2602a6dc8\n\nLabels:         \nnone\n\nStorageClass:   container-native-storage\nStatus:         Bound\nClaim:          my-test-project/postgresql\nReclaim Policy: Delete\nAccess Modes:   RWO\nCapacity:       15Gi\nMessage:\nSource:\n\n    Type:               Glusterfs (a Glusterfs mount on the host that shares a pod\ns lifetime)\n\n    EndpointsName:      glusterfs-dynamic-postgresql\n\n    Path:               vol_e8fe7f46fedf7af7628feda0dcbf2f60\n\n    ReadOnly:           false\nNo events.\n\n\n\n\n\nNote the GlusterFS volume name, in this case \nvol_e8fe7f46fedf7af7628feda0dcbf2f60\n.\n\n\nNow let\u2019s switch to the namespace we used for CNS deployment:\n\n\noc project container-native-storage\n\n\n\n\n\nLook at the GlusterFS pods running\n\n\noc get pods -o wide\n\n\n\n\n\nPick one of the GlusterFS pods by name (which one is not important):\n\n\nNAME              READY     STATUS    RESTARTS   AGE       IP              NODE\nglusterfs-37vn8   1/1       Running   1          15m       192.168.0.102   node1.example.com\nglusterfs-cq68l   1/1       Running   1          15m       192.168.0.103   node2.example.com\nglusterfs-m9fvl   1/1       Running   1          15m       192.168.0.104   node3.example.com\nheketi-1-cd032    1/1       Running   1          13m       10.130.0.5      node3.example.com\n\n\n\n\n\nRemember the IP address\n of the pod you select. In this case \n192.168.0.102\n.\n\n\nLog on to GlusterFS pod with a remote terminal session like so:\n\n\noc rsh glusterfs-37vn8\n\n\n\n\n\nYou will end up in shell session in the container with root privileges.\n\n\nsh-4.2#\n\n\n\n\n\nYou have now access to this container\u2019s process and filesystem namespace which has the GlusterFS CLI utilities installed.\n\n\nLet\u2019s list all known volumes:\n\n\nsh-4.2# gluster volume list\n\n\n\n\n\nYou will see two volumes:\n\n\nheketidbstorage\nvol_e8fe7f46fedf7af7628feda0dcbf2f60\n\n\n\n\n\n\n\n\n\nheketidbstorage\n is a internal-only volume dedicated to heketi\u2019s internal database.\n\n\n\n\n\n\nvol_e8fe7f46fedf7af7628feda0dcbf2f60\n is the volume backing the PV of the PostgreSQL database deployed earlier.\n\n\n\n\n\n\nInterrogate GlusterFS about the topology of this volume:\n\n\nsh-4.2# gluster volume info vol_e8fe7f46fedf7af7628feda0dcbf2f60\n\n\n\n\n\nThe output will show you how the volume has been created. You will also see that the pod you are currently logged on serves one the bricks (in highlight).\n\n\nVolume Name: vol_e8fe7f46fedf7af7628feda0dcbf2f60\nType: Replicate\nVolume ID: c2bedd16-8b0d-432c-b9eb-4ab1274826dd\nStatus: Started\nSnapshot Count: 0\nNumber of Bricks: 1 x 3 = 3\nTransport-type: tcp\nBricks:\nBrick1: 192.168.0.103:/var/lib/heketi/mounts/vg_63b05bee6695ee5a63ad95bfbce43bf7/brick_aa28de668c8c21192df55956a822bd3c/brick\n\nBrick2: 192.168.0.102:/var/lib/heketi/mounts/vg_0246fd563709384a3cbc3f3bbeeb87a9/brick_684a01f8993f241a92db02b117e0b912/brick\n\nBrick3: 192.168.0.104:/var/lib/heketi/mounts/vg_5a8c767e65feef7455b58d01c6936b83/brick_25972cf5ed7ea81c947c62443ccb308c/brick\nOptions Reconfigured:\ntransport.address-family: inet\nperformance.readdir-ahead: on\nnfs.disable: on\n\n\n\n\n\n\n\nNote\n\n\nIdentify the right brick by looking at the host IP of the pod you have just logged on to. \noc get pods -o wide\n will give you this information.\n\n\n\n\nGlusterFS created this volume as a 3-way replica set across all GlusterFS pods, therefore across all your OpenShift App nodes running CNS. This is currently the only supported volume type in production. Later you will see how can schedule (unsupported) volume types like dispersed or distributed.\n\n\nYou can even look at the local brick:\n\n\nsh-4.2# ls -ahl /var/lib/heketi/mounts/vg_0246fd563709384a3cbc3f3bbeeb87a9/brick_684a01f8993f241a92db02b117e0b912/brick\ntotal 16K\ndrwxrwsr-x.   5 root       2001   57 Jun  6 14:44 .\ndrwxr-xr-x.   3 root       root   19 Jun  6 14:44 ..\ndrw---S---. 263 root       2001 8.0K Jun  6 14:46 .glusterfs\ndrwxr-sr-x.   3 root       2001   25 Jun  6 14:44 .trashcan\ndrwx------.  20 1000080000 2001 8.0K Jun  6 14:46 userdata\n\nsh-4.2# ls -ahl /var/lib/heketi/mounts/vg_0246fd563709384a3cbc3f3bbeeb87a9/brick_684a01f8993f241a92db02b117e0b912/brick/userdata\n\ntotal 68K\ndrwx------. 20 1000080000 2001 8.0K Jun  6 14:46 .\ndrwxrwsr-x.  5 root       2001   57 Jun  6 14:44 ..\n-rw-------.  2 1000080000 root    4 Jun  6 14:44 PG_VERSION\ndrwx------.  6 1000080000 root   54 Jun  6 14:46 base\ndrwx------.  2 1000080000 root 8.0K Jun  6 14:47 global\ndrwx------.  2 1000080000 root   18 Jun  6 14:44 pg_clog\ndrwx------.  2 1000080000 root    6 Jun  6 14:44 pg_commit_ts\ndrwx------.  2 1000080000 root    6 Jun  6 14:44 pg_dynshmem\n-rw-------.  2 1000080000 root 4.6K Jun  6 14:46 pg_hba.conf\n-rw-------.  2 1000080000 root 1.6K Jun  6 14:44 pg_ident.conf\ndrwx------.  2 1000080000 root   32 Jun  6 14:46 pg_log\ndrwx------.  4 1000080000 root   39 Jun  6 14:44 pg_logical\ndrwx------.  4 1000080000 root   36 Jun  6 14:44 pg_multixact\ndrwx------.  2 1000080000 root   18 Jun  6 14:46 pg_notify\ndrwx------.  2 1000080000 root    6 Jun  6 14:44 pg_replslot\ndrwx------.  2 1000080000 root    6 Jun  6 14:44 pg_serial\ndrwx------.  2 1000080000 root    6 Jun  6 14:44 pg_snapshots\ndrwx------.  2 1000080000 root    6 Jun  6 14:46 pg_stat\ndrwx------.  2 1000080000 root   84 Jun  6 15:16 pg_stat_tmp\ndrwx------.  2 1000080000 root   18 Jun  6 14:44 pg_subtrans\ndrwx------.  2 1000080000 root    6 Jun  6 14:44 pg_tblspc\ndrwx------.  2 1000080000 root    6 Jun  6 14:44 pg_twophase\ndrwx------.  3 1000080000 root   60 Jun  6 14:44 pg_xlog\n-rw-------.  2 1000080000 root   88 Jun  6 14:44 postgresql.auto.conf\n-rw-------.  2 1000080000 root  21K Jun  6 14:46 postgresql.conf\n-rw-------.  2 1000080000 root   46 Jun  6 14:46 postmaster.opts\n-rw-------.  2 1000080000 root   89 Jun  6 14:46 postmaster.pid\n\n\n\n\n\n\n\nNote\n\n\nThe exact path name will be different in your environment as it has been automatically generated.\n\n\n\n\nYou are looking at the PostgreSQL internal data file structure from the perspective of the GlusterFS server side. Evidence that the database uses CNS.\n\n\nClients, like the OpenShift nodes and their application pods talk to this storage with the GlusterFS protocol as it where an ordinary GlusterFS deployment.\n\nWhen a pod starts that mounts storage from a PV backed by CNS the GlusterFS mount plugin in OpenShift will mount the GlusterFS volume on the right App Node and then \nbind-mount\n this directory to the right pod.\n\nThis is happen transparently to the application and looks like a normal local filesystem inside the pod.\n\n\nYou may exit your remote session to the GlusterFS pod.\n\n\nsh-4.2# exit\n\n\n\n\n\n\n\nProviding shared storage to multiple application instances\n#\n\n\nIn the previous example we provisioned an RWO PV - the volume is only usable with one pod at a time. RWO is what most of the OpenShift storage backends support.\n\nSo far only very few options, like the basic NFS support existed, to provide a PersistentVolume to more than one container at once. The access mode used for this is \nReadWriteMany\n.\n\n\nWith CNS this capabilities is now available to all OpenShift deployments, no matter where they are deployed. To demonstrate this capability with an application we will deploy a PHP-based file uploader that has multiple front-end instances sharing a common storage repository.\n\n\nFirst make sure you are still in the example project created earlier\n\n\noc project my-test-project\n\n\n\n\n\nNext deploy the example application:\n\n\noc new-app openshift/php:7.0~https://github.com/christianh814/openshift-php-upload-demo --name=file-uploader\n\n\n\n\n\n\n\nNote\n\n\nThis is yet another way to build and launch an application from source code in OpenShift. The content before the ~ is the name of a Source-to-Image builder (a container that knows how to build applications of a certain type from source, in this case PHP) and the URL following is a GitHub repository hosting the source code.\n\nFeel free to check it out.\n\n\n\n\nOutput:\n\n\n--\n Found image a1ebebb (6 weeks old) in image stream \nopenshift/php\n under tag \n7.0\n for \nopenshift/php:7.0\n\n\n    Apache 2.4 with PHP 7.0\n    -----------------------\n    Platform for building and running PHP 7.0 applications\n\n    Tags: builder, php, php70, rh-php70\n\n    * A source build using source code from https://github.com/christianh814/openshift-php-upload-demo will be created\n      * The resulting image will be pushed to image stream \nfile-uploader:latest\n\n      * Use \nstart-build\n to trigger a new build\n    * This image will be deployed in deployment config \nfile-uploader\n\n    * Port 8080/tcp will be load balanced by service \nfile-uploader\n\n      * Other containers can access this service through the hostname \nfile-uploader\n\n\n--\n Creating resources ...\n    imagestream \nfile-uploader\n created\n    buildconfig \nfile-uploader\n created\n    deploymentconfig \nfile-uploader\n created\n    service \nfile-uploader\n created\n--\n Success\n    Build scheduled, use \noc logs -f bc/file-uploader\n to track its progress.\n    Run \noc status\n to view your app.\n\n\n\n\n\nWait for the application to be deployed with the suggest command:\n\n\noc logs -f bc/file-uploader\n\n\n\n\n\nThe follow-mode of the above command ends automatically when the build is successful and you return to your shell.\n\n\n...\nCloning \nhttps://github.com/christianh814/openshift-php-upload-demo\n ...\n        Commit: 7508da63d78b4abc8d03eac480ae930beec5d29d (Update index.html)\n        Author: Christian Hernandez \nchristianh814@users.noreply.github.com\n\n        Date:   Thu Mar 23 09:59:38 2017 -0700\n---\n Installing application source...\nPushing image 172.30.120.134:5000/my-test-project/file-uploader:latest ...\nPushed 0/5 layers, 2% complete\nPushed 1/5 layers, 20% complete\nPushed 2/5 layers, 40% complete\nPush successful\n\n\n\n\n\nWhen the build is completed ensure the pods are running:\n\n\noc get pods\n\n\n\n\n\nAmong your existing pods you should see two new pods running.\n\n\nNAME                             READY     STATUS      RESTARTS   AGE\nfile-uploader-1-build            0/1       Completed   0          2m\nfile-uploader-1-k2v0d            1/1       Running     0          1m\n...\n\n\n\n\n\nNote the name of the single pod currently running the app: \nfile-uploader-1-k2v0d\n. The container called \nfile-uploader-1-build\n is the builder container that deployed the application and it has already terminated. A service has been created for our app but not exposed yet.\n\n\nLet\u2019s fix this:\n\n\noc expose svc/file-uploader\n\n\n\n\n\nCheck the route that has been created:\n\n\noc get route/file-uploader\n\n\n\n\n\nThe route forwards all traffic to port 8080 of the container running the app.\n\n\nNAME                     HOST/PORT                                                      PATH      SERVICES                 PORT       TERMINATION   WILDCARD\nfile-uploader            file-uploader-my-test-project.cloudapps.example.com                      file-uploader            8080-tcp                 None\n\n\n\n\n\nPoint your browser the the URL advertised by the route (\nhttp://file-uploader-my-test-project.cloudapps.example.com\n)\n\n\nThe application simply lists all file previously uploaded and offers the ability to upload new ones as well as download the existing data. Right now there is nothing.\n\n\nSelect an arbitrary from your local system and upload it to the app.\n\n\n\n\nAfter uploading a file validate it has been stored locally in the container by following the link \nList Uploaded Files\n in the browser\n\n\nOr logging into it via a remote session (using the name noted earlier):\n\n\noc rsh file-uploader-1-k2v0d\n\n\n\n\n\nIn the container explore the directory in which the uploaded files will be stored.\n\n\nsh-4.2$ cd uploaded\nsh-4.2$ pwd\n/opt/app-root/src/uploaded\nsh-4.2$ ls -lh\ntotal 16K\n-rw-r--r--. 1 1000080000 root 16K May 26 09:32 cns-deploy-4.0.0-15.el7rhgs.x86_64.rpm.gz\n\n\n\n\n\n\n\nNote\n\n\nThe exact name of the pod will be different in your environment.\n\n\n\n\nThe app should also list the file in the overview:\n\n\n\n\nThis pod currently does not use any persistent storage. It stores the file locally.\n\n\n\n\nCaution\n\n\nNever store data in a pod. It\u2019s ephemeral by definition and will be lost as soon as the pod terminates.\n\n\n\n\nLet\u2019s see when this become a problem. Exit out of the container shell:\n\n\nsh-4.2$ exit\n\n\n\n\n\nLet\u2019s scale the deployment to 3 instances of the app:\n\n\n[root@master ~]# oc scale dc/file-uploader --replicas=3\n\n\n\n\n\nWatch the additional pods getting spawned:\n\n\n[root@master ~]# oc get pods\nNAME                             READY     STATUS      RESTARTS   AGE\nfile-uploader-1-3cgh1            1/1       Running     0          20s\nfile-uploader-1-3hckj            1/1       Running     0          20s\nfile-uploader-1-build            0/1       Completed   0          4m\nfile-uploader-1-k2v0d            1/1       Running     0          3m\n...\n\n\n\n\n\n\n\nNote\n\n\nThe pod names will be different in your environment since they are automatically generated.\n\n\n\n\nWhen you log on to one of the new instances you will see they have no data.\n\n\n[root@master ~]# oc rsh file-uploader-1-3cgh1\nsh-4.2$ cd uploaded\nsh-4.2$ pwd\n/opt/app-root/src/uploaded\nsh-4.2$ ls -hl\ntotal 0\n\n\n\n\n\nSimilarly, other users of the app will sometimes see your uploaded files and sometimes not - whenever the load balancing service in OpenShift points to the pod that has the file stored locally. You can simulate this with another instance of your browser in \nIncognito mode\n pointing to your app.\n\n\nThe app is of course not usable like this. We can fix this by providing shared storage to this app.\n\n\nFirst create a PVC with the appropriate setting in a file called \ncns-rwx-pvc.yml\n with below contents:\n\n\ncns-rwx-pvc.yml.\n\n\nkind: PersistentVolumeClaim\napiVersion: v1\nmetadata:\n  name: my-shared-storage\n  annotations:\n    volume.beta.kubernetes.io/storage-class: container-native-storage\nspec:\n  accessModes:\n  - ReadWriteMany\n  resources:\n    requests:\n      storage: 10Gi\n\n\n\n\n\nSubmit the request to the system:\n\n\n[root@master ~]# oc create -f cns-rwx-pvc.yml\n\n\n\n\n\nLet\u2019s look at the result:\n\n\n[root@master ~]# oc get pvc\nNAME                STATUS    VOLUME                                     CAPACITY   ACCESSMODES   AGE\nmy-shared-storage   Bound     pvc-62aa4dfe-4ad2-11e7-b56f-2cc2602a6dc8   10Gi       RWX           22s\n...\n\n\n\n\n\nNotice the ACCESSMODE being set to \nRWX\n (short for \nReadWriteMany\n, synonym for \nshared storage\n).\n\n\nWe can now update the \nDeploymentConfig\n of our application to use this PVC to provide the application with persistent, shared storage for uploads.\n\n\n[root@master ~]# oc volume dc/file-uploader --add --name=shared-storage --type=persistentVolumeClaim --claim-name=my-shared-storage --mount-path=/opt/app-root/src/uploaded\n\n\n\n\n\nOur app will now re-deploy (in a rolling fashion) with the new settings - all pods will mount the volume identified by the PVC under /opt/app-root/src/upload (the path is predictable so we can hard-code it here).\n\n\nYou can watch it like this:\n\n\n[root@master ~]# oc logs dc/file-uploader -f\n--\n Scaling up file-uploader-2 from 0 to 3, scaling down file-uploader-1 from 3 to 0 (keep 3 pods available, don\nt exceed 4 pods)\n    Scaling file-uploader-2 up to 1\n    Scaling file-uploader-1 down to 2\n    Scaling file-uploader-2 up to 2\n    Scaling file-uploader-1 down to 1\n    Scaling file-uploader-2 up to 3\n    Scaling file-uploader-1 down to 0\n--\n Success\n\n\n\n\n\nThe new config \nfile-uploader-2\n will have 3 pods all sharing the same storage.\n\n\n[root@master ~]# oc get pods\nNAME                             READY     STATUS      RESTARTS   AGE\nfile-uploader-1-build            0/1       Completed   0          18m\nfile-uploader-2-jd22b            1/1       Running     0          1m\nfile-uploader-2-kw9lq            1/1       Running     0          2m\nfile-uploader-2-xbz24            1/1       Running     0          1m\n...\n\n\n\n\n\nTry it out in your application: upload new files and watch them being visible from within all application pods. In the browser the application behaves fluently as it circles through the pods between browser requests.\n\n\n[root@master ~]# oc rsh file-uploader-2-jd22b\nsh-4.2$ ls -lh uploaded\ntotal 16K\n-rw-r--r--. 1 1000080000 root 16K May 26 10:21 cns-deploy-4.0.0-15.el7rhgs.x86_64.rpm.gz\nsh-4.2$ exit\nexit\n[root@master ~]# oc rsh file-uploader-2-kw9lq\nsh-4.2$ ls -lh uploaded\n-rw-r--r--. 1 1000080000 root 16K May 26 10:21 cns-deploy-4.0.0-15.el7rhgs.x86_64.rpm.gz\nsh-4.2$ exit\nexit\n[root@master ~]# oc rsh file-uploader-2-xbz24\nsh-4.2$ ls -lh uploaded\n-rw-r--r--. 1 1000080000 root 16K May 26 10:21 cns-deploy-4.0.0-15.el7rhgs.x86_64.rpm.gz\nsh-4.2$ exit\n\n\n\n\n\nThat\u2019s it. You have successfully provided shared storage to pods throughout the entire system, therefore avoiding the need for data to be replicated at the application level to each pod.\n\n\nWith CNS this is available wherever OpenShift is deployed with no external dependency.", 
            "title": "Module 3 - Using Persistent Storage"
        }, 
        {
            "location": "/module-3-using-cns/#creating-a-storageclass", 
            "text": "OpenShift uses Kubernetes  PersistentStorage facility to dynamically allocate storage of any kind for applications. This is a fairly simple framework in which only 3 components exists: the storage provider, the storage volume and the request for a storage volume.   OpenShift knows non-ephemeral storage as  persistent  volumes. This is storage that is decoupled from pod lifecycles. Users can request such storage by submitting a  PersistentVolumeClaim  to the system, which carries aspects like desired capacity or access mode (shared, single, read-only).  A storage provider in the system is represented by a  StorageClass  and is referenced in the claim. Upon receiving the claim it talks to the API of the actual storage system to provision the storage.    The storage is represented in OpenShift as a  PersistentVolume  which can directly be used by pods to mount it.  With these basics defined we can configure our system for CNS. First we will set up the credentials for CNS in OpenShift.  Create an encoded value for the CNS admin user like below:  echo -n  myS3cr3tpassw0rd  | base64  The encoded string looks like this:  bXlTM2NyM3RwYXNzdzByZA==  We will store this encoded value in an OpenShift secret.  Create a file called  cns-secret.yml  with the as per below (highlight shows where to put encoded password):  cns-secret.yml:  apiVersion :   v1  kind :   Secret  metadata : \n   name :   cns-secret \n   namespace :   default  data :     key :   bXlTM2NyM3RwYXNzdzByZA==  type :   kubernetes.io/glusterfs   Create the secret in OpenShift with the following command:  oc create -f cns-secret.yml  To represent CNS as a storage provider in the system you first have to create a StorageClass. Define by creating a file called  cns-storageclass.yml  which references the secret and the heketi URL shown earlier with the contents as below:  cns-storageclass.yml:  apiVersion :   storage.k8s.io/v1beta1  kind :   StorageClass  metadata : \n   name :   container-native-storage \n   annotations : \n     storageclass.beta.kubernetes.io/is-default-class :   true  provisioner :   kubernetes.io/glusterfs  parameters : \n   resturl :   http://heketi-container-native-storage.cloudapps.example.com \n   restauthenabled :   true \n   restuser :   admin \n   volumetype :   replicate:3 \n   secretNamespace :   default \n   secretName :   cns-secret   Create the StorageClass in OpenShift with the following command:  oc create -f cns-storageclass.yml  With these components in place the system is ready to dynamically provision storage capacity from Container-native Storage.", 
            "title": "Creating a StorageClass"
        }, 
        {
            "location": "/module-3-using-cns/#requesting-storage", 
            "text": "To get storage provisioned as a user you have to  claim  storage. The  PersistentVolumeClaim  (PVC) basically acts a request to the system to provision storage with certain properties, like a specific capacity. \nAlso the access mode is set here, where  ReadWriteOnce  allows one container at a time to mount this storage.  Create a claim by specifying a file called  cns-pvc.yml  with the following contents:  cns-pvc.yml:  kind :   PersistentVolumeClaim  apiVersion :   v1  metadata : \n   name :   my-container-storage \n   annotations : \n     volume.beta.kubernetes.io/storage-class :   container-native-storage  spec : \n   accessModes : \n   -   ReadWriteOnce \n   resources : \n     requests : \n       storage :   10Gi   With above PVC we are requesting 10 GiB of non-shared storage. Instead of  ReadWriteOnce  you could also have specified  ReadWriteOnly  (for read-only) and  ReadWriteMany  (for shared storage).  Submit the PVC to the system like so:  oc create -f cns-pvc.yml  Look at the requests state with the following command:  oc get pvc  You should see the PVC listed and in  Bound  state.  NAME                   STATUS    VOLUME                                     CAPACITY   ACCESSMODES   AGE\nmy-container-storage   Bound     pvc-382ac13d-4a9f-11e7-b56f-2cc2602a6dc8   10Gi       RWO           16s   Note  It may take a couple seconds for the claim to be in  bound .    Caution  If the PVC is stuck in  PENDING  state you will need to investigate. Run  oc describe pvc/my-container-storage  to see a more detailed explanation. Typically there are two root causes - the StorageClass is not properly setup (wrong name, wrong credentials, incorrect secret name, wrong heketi URL, heketi service not up, heketi pod not up\u2026) or the PVC is malformed (wrong StorageClass, name already taken \u2026)    Tip  You can also do this step with the UI. If you like you can switch to an arbitrary project you have access to and go to the  Storage  tab. Select  Create  storage and make selections accordingly to the PVC described before.   When the claim was fulfilled successfully it is in the  Bound  state. That means the system has successfully (via the StorageClass) reached out to the storage backend (in our case GlusterFS). The backend in turn provisioned the storage and provided a handle back OpenShift. In OpenShift the provisioned storage is then represented by a  PersistentVolume  (PV) which is  bound  to the PVC.  Look at the PVC for these details:  oc describe pvc/my-container-storage  The details of the PVC show against which  StorageClass  it has been submitted and the name of the  PersistentVolume  which was generated to fulfill the claim.  Name:           my-container-storage Namespace:      container-native-storage\nStorageClass:   container-native-storage\nStatus:         Bound Volume:         pvc-382ac13d-4a9f-11e7-b56f-2cc2602a6dc8 Labels:          none \nCapacity:       10Gi\nAccess Modes:   RWO\nNo events.   Note  The PV name will be different in your environment since it\u2019s automatically generated.   Look at the corresponding PV by it\u2019s name:  oc describe pv/pvc-382ac13d-4a9f-11e7-b56f-2cc2602a6dc8  The output shows several interesting things, like the access mode (RWO = ReadWriteOnce), the reclaim policy (what happens when the PV object gets deleted), the capacity and the type of storage backing this PV (in our case GlusterFS as part of CNS):  Name:           pvc-382ac13d-4a9f-11e7-b56f-2cc2602a6dc8\nLabels:          none \nStorageClass:   container-native-storage Status:         Bound Claim:          container-native-storage/my-container-storage Reclaim Policy: Delete Access Modes:   RWO Capacity:       10Gi Message:\nSource:     Type:               Glusterfs (a Glusterfs mount on the host that shares a pod s lifetime)     EndpointsName:      glusterfs-dynamic-my-container-storage\n    Path:               vol_304670f0d50bf5aa4717a69652bd48ff\n    ReadOnly:           false\nNo events.   Why is it called  Bound ?  Originally PVs weren t automatically created. Hence in earlier documentation you may also find references about administrators actually  pre-provisioning  PVs. Later PVCs would  pick up  a suitable PV by looking at it\u2019s capacity. When successful they are  bound  to this PV. \nThis was needed for storage like NFS that does not have an API and therefore does not support  dynamic provisioning . Hence it s called  static provisioning . \nThis kind of storage should not be used anymore as it requires manual intervention, risky capacity planning and incurs inefficient storage utilization.   Let\u2019s release this storage capacity again, since it\u2019s in the wrong namespace anyway. \nStorage is freed up by deleting the  PVC . The PVC controls the lifecycle of the storage, not the PV.   Important  Never delete PVs that are dynamically provided. They are only handles for pods mounting the storage. Storage lifecycle is entirely controlled via PVCs.   Delete the storage by deleting the PVC like this:  oc delete pvc/my-container-storage", 
            "title": "Requesting Storage"
        }, 
        {
            "location": "/module-3-using-cns/#using-non-shared-storage-for-databases", 
            "text": "Normally a user doesn\u2019t request storage with a PVC directly. Rather the PVC is integrated in a larger template that describe the entire application. Such examples ship with OpenShift out of the box.   Tip  The steps described in this section can again also be done with the UI. For this purpose follow these steps similar to the one in Module 1:      Log on to the OpenShift UI at the URL provided in  Overview section    Create a project called  my-test-project , label and description is optional    In the Overview, next to the project\u2019s name select  Add to project    In the  Browse Catalog  view select  Ruby  from the list of programming languages    Select the example app entitled  Rails + PostgreSQL (Persistent)    (optional) Change the  Volume Capacity  parameter to something greater than 1GiB, e.g. 15 GiB    Select  Create  to start deploying the app    Select  Continue to Overview  in the confirmation screen    Wait for the application deployment to finish and continue below at      To create an application from the OpenShift Example templates on the CLI follow these steps.  Create a new project with a name of your choice:  oc new-project my-test-project  To use the example applications that ship with OpenShift we can export and modify the template for a sample Ruby on Rails with PostgreSQL application. All these templates ship in pre-defined namespace called  openshift .  Export the template from the  openshift  namespace in YAML format:  oc export template/rails-pgsql-persistent -n openshift -o yaml   rails-app-template.yml  In the file  rails-app-template.yml  you can now review the template for this entire application stack in all it\u2019s glory.   What does the template file contain?  In essence it creates Ruby on Rails instance in a pod which functionality mimics a very basic blogging application. The blog articles are saved in a PostgreSQL database which runs in a separate pod. \nThe template describes all OpenShift resources necessary to stand up the rails pod and the postgres pod and make them accessible via services and routes. \nIn addition a PVC is issued (line 194) to supply this pod with persistent storage below the mount point  /var/lib/pgsql/data  (line 275).   The template contains a couple of parameters which default values we can override.   Tip  To list all available parameters from this template run  oc process -f rails-app-template.yml --parameters \nThe  oc process  command parses the template and replaces any parameters with their default values if not supplied explicitly like in the next step.   There is a parameter in the template is called  VOLUME_CAPACITY . It is used to customize the capacity in the PVC. We will process the template with the CLI client and override this parameter with a value of  15Gi  as follows:  Render the template with the custom parameter value as follows:  oc process -f rails-app-template.yml -o yaml -p VOLUME_CAPACITY=15Gi   my-rails-app.yml  The result  my-rails-app.yml  file contains all resources including our custom PVC for this application.  Deploy these resources like so:  oc create -f my-rails-app.yml  Among various OpenShift resource also our PVC will be created:  secret  rails-pgsql-persistent  created\nservice  rails-pgsql-persistent  created\nroute  rails-pgsql-persistent  created\nimagestream  rails-pgsql-persistent  created\nbuildconfig  rails-pgsql-persistent  created\ndeploymentconfig  rails-pgsql-persistent  created persistentvolumeclaim  postgresql  created service  postgresql  created\ndeploymentconfig  postgresql  created  You can now use the OpenShift UI (while being logged in the newly created project) to follow the deployment process.  Alternatively watch the containers deploy like this:  oc get pods -w  The complete output should look like this:  NAME                             READY     STATUS              RESTARTS   AGE\npostgresql-1-deploy              0/1       ContainerCreating   0          11s\nrails-pgsql-persistent-1-build   0/1       ContainerCreating   0          11s\nNAME                  READY     STATUS    RESTARTS   AGE\npostgresql-1-deploy   1/1       Running   0          14s\npostgresql-1-81gnm   0/1       Pending   0         0s\npostgresql-1-81gnm   0/1       Pending   0         0s\nrails-pgsql-persistent-1-build   1/1       Running   0         19s\npostgresql-1-81gnm   0/1       Pending   0         15s\npostgresql-1-81gnm   0/1       ContainerCreating   0         16s\npostgresql-1-81gnm   0/1       Running   0         47s\npostgresql-1-81gnm   1/1       Running   0         4m\npostgresql-1-deploy   0/1       Completed   0         4m\npostgresql-1-deploy   0/1       Terminating   0         4m\npostgresql-1-deploy   0/1       Terminating   0         4m\nrails-pgsql-persistent-1-deploy   0/1       Pending   0         0s\nrails-pgsql-persistent-1-deploy   0/1       Pending   0         0s\nrails-pgsql-persistent-1-deploy   0/1       ContainerCreating   0         0s\nrails-pgsql-persistent-1-build   0/1       Completed   0         11m\nrails-pgsql-persistent-1-deploy   1/1       Running   0         6s\nrails-pgsql-persistent-1-hook-pre   0/1       Pending   0         0s\nrails-pgsql-persistent-1-hook-pre   0/1       Pending   0         0s\nrails-pgsql-persistent-1-hook-pre   0/1       ContainerCreating   0         0s\nrails-pgsql-persistent-1-hook-pre   1/1       Running   0         6s\nrails-pgsql-persistent-1-hook-pre   0/1       Completed   0         15s\nrails-pgsql-persistent-1-dkj7w   0/1       Pending   0         0s\nrails-pgsql-persistent-1-dkj7w   0/1       Pending   0         0s\nrails-pgsql-persistent-1-dkj7w   0/1       ContainerCreating   0         0s\nrails-pgsql-persistent-1-dkj7w   0/1       Running   0         1m\nrails-pgsql-persistent-1-dkj7w   1/1       Running   0         1m\nrails-pgsql-persistent-1-deploy   0/1       Completed   0         1m\nrails-pgsql-persistent-1-deploy   0/1       Terminating   0         1m\nrails-pgsql-persistent-1-deploy   0/1       Terminating   0         1m\nrails-pgsql-persistent-1-hook-pre   0/1       Terminating   0         1m\nrails-pgsql-persistent-1-hook-pre   0/1       Terminating   0         1m  Exit out of the watch mode with:  Ctrl  +  c   Note  It may take up to 5 minutes for the deployment to complete.   You should also see a PVC being issued and in the  Bound  state.  [root@master ~]# oc get pvc\nNAME         STATUS    VOLUME                                     CAPACITY   ACCESSMODES   AGE\npostgresql   Bound     pvc-9bb84d88-4ac6-11e7-b56f-2cc2602a6dc8   15Gi       RWO           4m   Why did this even work?  If you paid close attention you likely noticed that the PVC in the template does not specify a particular  StorageClass . This still yields a PV deployed because our  StorageClass  has actually been defined as the system-wide default. PVCs that don t specify a  StorageClass  will use this.   Now go ahead and try out the application. The overview page in the OpenShift UI will tell you the  route  which has been deployed as well. Use it and append  /articles  to the URL to get to the actual app.  Otherwise get it on the CLI like this:  oc get route  Output:  NAME                     HOST/PORT                                                      PATH      SERVICES                 PORT      TERMINATION   WILDCARD\nrails-pgsql-persistent   rails-pgsql-persistent-my-test-project.cloudapps.example.com             rails-pgsql-persistent    all                    None  Following this output, point your browser to  http://rails-pgsql-persistent-my-test-project.cloudapps.example.com/articles . \nYou should be able to successfully create articles and comments. The username/password to create articles and comments is by default  openshift / secret . \nWhen they are saved they are actually saved in the PostgreSQL database which stores it\u2019s table spaces on a GlusterFS volume provided by CNS.  Now let\u2019s take a look at how this was actually achieved.  Look at the PVC to determine the PV:  oc get pvc  Output:  NAME         STATUS    VOLUME                                     CAPACITY   ACCESSMODES   AGE\npostgresql   Bound     pvc-9bb84d88-4ac6-11e7-b56f-2cc2602a6dc8   15Gi       RWO           17m   Note  Your volume (PV) name will be different as it\u2019s dynamically generated.   Look at the details of this PV:  oc describe pv/pvc-9bb84d88-4ac6-11e7-b56f-2cc2602a6dc8  Output shows (in highlight) the name of the volume, the backend type (GlusterFS) and the volume name GlusterFS uses internally.  Name:           pvc-9bb84d88-4ac6-11e7-b56f-2cc2602a6dc8 Labels:          none \nStorageClass:   container-native-storage\nStatus:         Bound\nClaim:          my-test-project/postgresql\nReclaim Policy: Delete\nAccess Modes:   RWO\nCapacity:       15Gi\nMessage:\nSource:     Type:               Glusterfs (a Glusterfs mount on the host that shares a pod s lifetime)     EndpointsName:      glusterfs-dynamic-postgresql     Path:               vol_e8fe7f46fedf7af7628feda0dcbf2f60     ReadOnly:           false\nNo events.  Note the GlusterFS volume name, in this case  vol_e8fe7f46fedf7af7628feda0dcbf2f60 .  Now let\u2019s switch to the namespace we used for CNS deployment:  oc project container-native-storage  Look at the GlusterFS pods running  oc get pods -o wide  Pick one of the GlusterFS pods by name (which one is not important):  NAME              READY     STATUS    RESTARTS   AGE       IP              NODE\nglusterfs-37vn8   1/1       Running   1          15m       192.168.0.102   node1.example.com\nglusterfs-cq68l   1/1       Running   1          15m       192.168.0.103   node2.example.com\nglusterfs-m9fvl   1/1       Running   1          15m       192.168.0.104   node3.example.com\nheketi-1-cd032    1/1       Running   1          13m       10.130.0.5      node3.example.com  Remember the IP address  of the pod you select. In this case  192.168.0.102 .  Log on to GlusterFS pod with a remote terminal session like so:  oc rsh glusterfs-37vn8  You will end up in shell session in the container with root privileges.  sh-4.2#  You have now access to this container\u2019s process and filesystem namespace which has the GlusterFS CLI utilities installed.  Let\u2019s list all known volumes:  sh-4.2# gluster volume list  You will see two volumes:  heketidbstorage\nvol_e8fe7f46fedf7af7628feda0dcbf2f60    heketidbstorage  is a internal-only volume dedicated to heketi\u2019s internal database.    vol_e8fe7f46fedf7af7628feda0dcbf2f60  is the volume backing the PV of the PostgreSQL database deployed earlier.    Interrogate GlusterFS about the topology of this volume:  sh-4.2# gluster volume info vol_e8fe7f46fedf7af7628feda0dcbf2f60  The output will show you how the volume has been created. You will also see that the pod you are currently logged on serves one the bricks (in highlight).  Volume Name: vol_e8fe7f46fedf7af7628feda0dcbf2f60\nType: Replicate\nVolume ID: c2bedd16-8b0d-432c-b9eb-4ab1274826dd\nStatus: Started\nSnapshot Count: 0\nNumber of Bricks: 1 x 3 = 3\nTransport-type: tcp\nBricks:\nBrick1: 192.168.0.103:/var/lib/heketi/mounts/vg_63b05bee6695ee5a63ad95bfbce43bf7/brick_aa28de668c8c21192df55956a822bd3c/brick Brick2: 192.168.0.102:/var/lib/heketi/mounts/vg_0246fd563709384a3cbc3f3bbeeb87a9/brick_684a01f8993f241a92db02b117e0b912/brick Brick3: 192.168.0.104:/var/lib/heketi/mounts/vg_5a8c767e65feef7455b58d01c6936b83/brick_25972cf5ed7ea81c947c62443ccb308c/brick\nOptions Reconfigured:\ntransport.address-family: inet\nperformance.readdir-ahead: on\nnfs.disable: on   Note  Identify the right brick by looking at the host IP of the pod you have just logged on to.  oc get pods -o wide  will give you this information.   GlusterFS created this volume as a 3-way replica set across all GlusterFS pods, therefore across all your OpenShift App nodes running CNS. This is currently the only supported volume type in production. Later you will see how can schedule (unsupported) volume types like dispersed or distributed.  You can even look at the local brick:  sh-4.2# ls -ahl /var/lib/heketi/mounts/vg_0246fd563709384a3cbc3f3bbeeb87a9/brick_684a01f8993f241a92db02b117e0b912/brick\ntotal 16K\ndrwxrwsr-x.   5 root       2001   57 Jun  6 14:44 .\ndrwxr-xr-x.   3 root       root   19 Jun  6 14:44 ..\ndrw---S---. 263 root       2001 8.0K Jun  6 14:46 .glusterfs\ndrwxr-sr-x.   3 root       2001   25 Jun  6 14:44 .trashcan\ndrwx------.  20 1000080000 2001 8.0K Jun  6 14:46 userdata\n\nsh-4.2# ls -ahl /var/lib/heketi/mounts/vg_0246fd563709384a3cbc3f3bbeeb87a9/brick_684a01f8993f241a92db02b117e0b912/brick/userdata\n\ntotal 68K\ndrwx------. 20 1000080000 2001 8.0K Jun  6 14:46 .\ndrwxrwsr-x.  5 root       2001   57 Jun  6 14:44 ..\n-rw-------.  2 1000080000 root    4 Jun  6 14:44 PG_VERSION\ndrwx------.  6 1000080000 root   54 Jun  6 14:46 base\ndrwx------.  2 1000080000 root 8.0K Jun  6 14:47 global\ndrwx------.  2 1000080000 root   18 Jun  6 14:44 pg_clog\ndrwx------.  2 1000080000 root    6 Jun  6 14:44 pg_commit_ts\ndrwx------.  2 1000080000 root    6 Jun  6 14:44 pg_dynshmem\n-rw-------.  2 1000080000 root 4.6K Jun  6 14:46 pg_hba.conf\n-rw-------.  2 1000080000 root 1.6K Jun  6 14:44 pg_ident.conf\ndrwx------.  2 1000080000 root   32 Jun  6 14:46 pg_log\ndrwx------.  4 1000080000 root   39 Jun  6 14:44 pg_logical\ndrwx------.  4 1000080000 root   36 Jun  6 14:44 pg_multixact\ndrwx------.  2 1000080000 root   18 Jun  6 14:46 pg_notify\ndrwx------.  2 1000080000 root    6 Jun  6 14:44 pg_replslot\ndrwx------.  2 1000080000 root    6 Jun  6 14:44 pg_serial\ndrwx------.  2 1000080000 root    6 Jun  6 14:44 pg_snapshots\ndrwx------.  2 1000080000 root    6 Jun  6 14:46 pg_stat\ndrwx------.  2 1000080000 root   84 Jun  6 15:16 pg_stat_tmp\ndrwx------.  2 1000080000 root   18 Jun  6 14:44 pg_subtrans\ndrwx------.  2 1000080000 root    6 Jun  6 14:44 pg_tblspc\ndrwx------.  2 1000080000 root    6 Jun  6 14:44 pg_twophase\ndrwx------.  3 1000080000 root   60 Jun  6 14:44 pg_xlog\n-rw-------.  2 1000080000 root   88 Jun  6 14:44 postgresql.auto.conf\n-rw-------.  2 1000080000 root  21K Jun  6 14:46 postgresql.conf\n-rw-------.  2 1000080000 root   46 Jun  6 14:46 postmaster.opts\n-rw-------.  2 1000080000 root   89 Jun  6 14:46 postmaster.pid   Note  The exact path name will be different in your environment as it has been automatically generated.   You are looking at the PostgreSQL internal data file structure from the perspective of the GlusterFS server side. Evidence that the database uses CNS.  Clients, like the OpenShift nodes and their application pods talk to this storage with the GlusterFS protocol as it where an ordinary GlusterFS deployment. \nWhen a pod starts that mounts storage from a PV backed by CNS the GlusterFS mount plugin in OpenShift will mount the GlusterFS volume on the right App Node and then  bind-mount  this directory to the right pod. \nThis is happen transparently to the application and looks like a normal local filesystem inside the pod.  You may exit your remote session to the GlusterFS pod.  sh-4.2# exit", 
            "title": "Using non-shared storage for databases"
        }, 
        {
            "location": "/module-3-using-cns/#providing-shared-storage-to-multiple-application-instances", 
            "text": "In the previous example we provisioned an RWO PV - the volume is only usable with one pod at a time. RWO is what most of the OpenShift storage backends support. \nSo far only very few options, like the basic NFS support existed, to provide a PersistentVolume to more than one container at once. The access mode used for this is  ReadWriteMany .  With CNS this capabilities is now available to all OpenShift deployments, no matter where they are deployed. To demonstrate this capability with an application we will deploy a PHP-based file uploader that has multiple front-end instances sharing a common storage repository.  First make sure you are still in the example project created earlier  oc project my-test-project  Next deploy the example application:  oc new-app openshift/php:7.0~https://github.com/christianh814/openshift-php-upload-demo --name=file-uploader   Note  This is yet another way to build and launch an application from source code in OpenShift. The content before the ~ is the name of a Source-to-Image builder (a container that knows how to build applications of a certain type from source, in this case PHP) and the URL following is a GitHub repository hosting the source code. \nFeel free to check it out.   Output:  --  Found image a1ebebb (6 weeks old) in image stream  openshift/php  under tag  7.0  for  openshift/php:7.0 \n\n    Apache 2.4 with PHP 7.0\n    -----------------------\n    Platform for building and running PHP 7.0 applications\n\n    Tags: builder, php, php70, rh-php70\n\n    * A source build using source code from https://github.com/christianh814/openshift-php-upload-demo will be created\n      * The resulting image will be pushed to image stream  file-uploader:latest \n      * Use  start-build  to trigger a new build\n    * This image will be deployed in deployment config  file-uploader \n    * Port 8080/tcp will be load balanced by service  file-uploader \n      * Other containers can access this service through the hostname  file-uploader \n\n--  Creating resources ...\n    imagestream  file-uploader  created\n    buildconfig  file-uploader  created\n    deploymentconfig  file-uploader  created\n    service  file-uploader  created\n--  Success\n    Build scheduled, use  oc logs -f bc/file-uploader  to track its progress.\n    Run  oc status  to view your app.  Wait for the application to be deployed with the suggest command:  oc logs -f bc/file-uploader  The follow-mode of the above command ends automatically when the build is successful and you return to your shell.  ...\nCloning  https://github.com/christianh814/openshift-php-upload-demo  ...\n        Commit: 7508da63d78b4abc8d03eac480ae930beec5d29d (Update index.html)\n        Author: Christian Hernandez  christianh814@users.noreply.github.com \n        Date:   Thu Mar 23 09:59:38 2017 -0700\n---  Installing application source...\nPushing image 172.30.120.134:5000/my-test-project/file-uploader:latest ...\nPushed 0/5 layers, 2% complete\nPushed 1/5 layers, 20% complete\nPushed 2/5 layers, 40% complete\nPush successful  When the build is completed ensure the pods are running:  oc get pods  Among your existing pods you should see two new pods running.  NAME                             READY     STATUS      RESTARTS   AGE\nfile-uploader-1-build            0/1       Completed   0          2m\nfile-uploader-1-k2v0d            1/1       Running     0          1m\n...  Note the name of the single pod currently running the app:  file-uploader-1-k2v0d . The container called  file-uploader-1-build  is the builder container that deployed the application and it has already terminated. A service has been created for our app but not exposed yet.  Let\u2019s fix this:  oc expose svc/file-uploader  Check the route that has been created:  oc get route/file-uploader  The route forwards all traffic to port 8080 of the container running the app.  NAME                     HOST/PORT                                                      PATH      SERVICES                 PORT       TERMINATION   WILDCARD\nfile-uploader            file-uploader-my-test-project.cloudapps.example.com                      file-uploader            8080-tcp                 None  Point your browser the the URL advertised by the route ( http://file-uploader-my-test-project.cloudapps.example.com )  The application simply lists all file previously uploaded and offers the ability to upload new ones as well as download the existing data. Right now there is nothing.  Select an arbitrary from your local system and upload it to the app.   After uploading a file validate it has been stored locally in the container by following the link  List Uploaded Files  in the browser  Or logging into it via a remote session (using the name noted earlier):  oc rsh file-uploader-1-k2v0d  In the container explore the directory in which the uploaded files will be stored.  sh-4.2$ cd uploaded\nsh-4.2$ pwd\n/opt/app-root/src/uploaded\nsh-4.2$ ls -lh\ntotal 16K\n-rw-r--r--. 1 1000080000 root 16K May 26 09:32 cns-deploy-4.0.0-15.el7rhgs.x86_64.rpm.gz   Note  The exact name of the pod will be different in your environment.   The app should also list the file in the overview:   This pod currently does not use any persistent storage. It stores the file locally.   Caution  Never store data in a pod. It\u2019s ephemeral by definition and will be lost as soon as the pod terminates.   Let\u2019s see when this become a problem. Exit out of the container shell:  sh-4.2$ exit  Let\u2019s scale the deployment to 3 instances of the app:  [root@master ~]# oc scale dc/file-uploader --replicas=3  Watch the additional pods getting spawned:  [root@master ~]# oc get pods\nNAME                             READY     STATUS      RESTARTS   AGE\nfile-uploader-1-3cgh1            1/1       Running     0          20s\nfile-uploader-1-3hckj            1/1       Running     0          20s\nfile-uploader-1-build            0/1       Completed   0          4m\nfile-uploader-1-k2v0d            1/1       Running     0          3m\n...   Note  The pod names will be different in your environment since they are automatically generated.   When you log on to one of the new instances you will see they have no data.  [root@master ~]# oc rsh file-uploader-1-3cgh1\nsh-4.2$ cd uploaded\nsh-4.2$ pwd\n/opt/app-root/src/uploaded\nsh-4.2$ ls -hl\ntotal 0  Similarly, other users of the app will sometimes see your uploaded files and sometimes not - whenever the load balancing service in OpenShift points to the pod that has the file stored locally. You can simulate this with another instance of your browser in  Incognito mode  pointing to your app.  The app is of course not usable like this. We can fix this by providing shared storage to this app.  First create a PVC with the appropriate setting in a file called  cns-rwx-pvc.yml  with below contents:  cns-rwx-pvc.yml.  kind: PersistentVolumeClaim\napiVersion: v1\nmetadata:\n  name: my-shared-storage\n  annotations:\n    volume.beta.kubernetes.io/storage-class: container-native-storage\nspec:\n  accessModes:\n  - ReadWriteMany\n  resources:\n    requests:\n      storage: 10Gi  Submit the request to the system:  [root@master ~]# oc create -f cns-rwx-pvc.yml  Let\u2019s look at the result:  [root@master ~]# oc get pvc\nNAME                STATUS    VOLUME                                     CAPACITY   ACCESSMODES   AGE\nmy-shared-storage   Bound     pvc-62aa4dfe-4ad2-11e7-b56f-2cc2602a6dc8   10Gi       RWX           22s\n...  Notice the ACCESSMODE being set to  RWX  (short for  ReadWriteMany , synonym for  shared storage ).  We can now update the  DeploymentConfig  of our application to use this PVC to provide the application with persistent, shared storage for uploads.  [root@master ~]# oc volume dc/file-uploader --add --name=shared-storage --type=persistentVolumeClaim --claim-name=my-shared-storage --mount-path=/opt/app-root/src/uploaded  Our app will now re-deploy (in a rolling fashion) with the new settings - all pods will mount the volume identified by the PVC under /opt/app-root/src/upload (the path is predictable so we can hard-code it here).  You can watch it like this:  [root@master ~]# oc logs dc/file-uploader -f\n--  Scaling up file-uploader-2 from 0 to 3, scaling down file-uploader-1 from 3 to 0 (keep 3 pods available, don t exceed 4 pods)\n    Scaling file-uploader-2 up to 1\n    Scaling file-uploader-1 down to 2\n    Scaling file-uploader-2 up to 2\n    Scaling file-uploader-1 down to 1\n    Scaling file-uploader-2 up to 3\n    Scaling file-uploader-1 down to 0\n--  Success  The new config  file-uploader-2  will have 3 pods all sharing the same storage.  [root@master ~]# oc get pods\nNAME                             READY     STATUS      RESTARTS   AGE\nfile-uploader-1-build            0/1       Completed   0          18m\nfile-uploader-2-jd22b            1/1       Running     0          1m\nfile-uploader-2-kw9lq            1/1       Running     0          2m\nfile-uploader-2-xbz24            1/1       Running     0          1m\n...  Try it out in your application: upload new files and watch them being visible from within all application pods. In the browser the application behaves fluently as it circles through the pods between browser requests.  [root@master ~]# oc rsh file-uploader-2-jd22b\nsh-4.2$ ls -lh uploaded\ntotal 16K\n-rw-r--r--. 1 1000080000 root 16K May 26 10:21 cns-deploy-4.0.0-15.el7rhgs.x86_64.rpm.gz\nsh-4.2$ exit\nexit\n[root@master ~]# oc rsh file-uploader-2-kw9lq\nsh-4.2$ ls -lh uploaded\n-rw-r--r--. 1 1000080000 root 16K May 26 10:21 cns-deploy-4.0.0-15.el7rhgs.x86_64.rpm.gz\nsh-4.2$ exit\nexit\n[root@master ~]# oc rsh file-uploader-2-xbz24\nsh-4.2$ ls -lh uploaded\n-rw-r--r--. 1 1000080000 root 16K May 26 10:21 cns-deploy-4.0.0-15.el7rhgs.x86_64.rpm.gz\nsh-4.2$ exit  That\u2019s it. You have successfully provided shared storage to pods throughout the entire system, therefore avoiding the need for data to be replicated at the application level to each pod.  With CNS this is available wherever OpenShift is deployed with no external dependency.", 
            "title": "Providing shared storage to multiple application instances"
        }, 
        {
            "location": "/module-1/", 
            "text": "", 
            "title": "Module 4 - Cluster Operations"
        }, 
        {
            "location": "/module-1/", 
            "text": "", 
            "title": "Module 5 - Advanced Topics"
        }
    ]
}